{"posts":[{"title":"Python玩转自动化运维","text":"专注于自动化运维领域，帮助任何想将编程与运维相结合的朋友，从零开始引领大家走进运维自动化。 内容受众你是否已经决定走上了运维工程师的道路，或者是深耕运维多年的老手； 升职加薪一直与自己绝缘，还总是锅从天上来； 看着别人自动化运维搞得风生水起，自己巡检两台设备的时间别人已经巡检好了一百台设备，还顺带出了个报表，你迷茫了； 人往往很难跳出自己的舒适圈，对于之前没有涉足过的领域也很难找到合适的切入点，再加上平时的工作已经足够焦头烂额，没有时间去从零开始学一门新技术。 内容定位并不为了单纯的输出枯燥的知识点，区别于其他资料的随意搬运，东一榔头西一棒子； 而是从场景出发，使输出的内容具有实操性，自顶向下将具体场景拆分，并落实到每个知识点，让同学清晰的意识到所学知识能够如何运用； 让运维老手掌握平时经常听到的 Netmiko, SNMP, Netconf等知识，能够学有所得，学能所用； 番外篇会将正篇中涉及到的细节逐一展开，让计算机专业同学能够深入理解，夯实基础，逐步掌握操作系统，数据结构等专业知识； 并且也会涉及到编程思想，学习方法论等技巧，让大家有挑战大厂的底气。 路径的选择对于正在从事运维工作，但工作中遇到了一些自动化的需求的朋友，那么这里作者并不提倡大家去构建整个的自动化运维体系，一个是没有必要，另一个是这也可能会消耗相当大的精力，所以我们就暂时只点与自己实际工作相关的技能即可，全部的技能树如下图所示： 路径的规划一、【自动化运维新手村】那么对于自动化运维来说肯定首推的编程语言就是Python。 但网上对于Python语言的教程一抓一大把，我为什么还要在这里去讲解呢？这其实等同于另一个问题：刚接触编程的人是否能通过看类似的教程快速上手Python? 或者通俗易懂的讲：是不是能看得懂? 大部分从事运维工作的朋友可能都没有编程经验，也没有系统的学习过计算机底层原理，所以对于标准式的教程就会感觉，好像会了，但又没有全会；好像会了，但又不会用，所以我根据自己的学习方法再加上身边运维朋友们的学习经历，通过一些生动的例子和实际的场景来让大家快速的上手Python这门语言，并且能直接运用于实际工作中。 新手村的内容主要是为大家搭建一个简易的CMDB（资产管理），这也符合很多运维朋友的实际需求场景，比如想要做一个小功能，可以提供给其他人使用。除此之外也会对其中涉及到的部分知识点进行详细的深入挖掘，呈现在番外篇中，让对其感兴趣的读者朋友去更多的了解。 Python基本数据结构和操作 将CMDB的增删改查与Python数据结构相结合，将枯燥的基础知识进行深入浅出的讲解 番外：数据结构的详解 番外：字符串的详解 番外：数组的详解 番外：字典的详解 面试番外：算法 Python函数与对象 将如何构建CMDB的函数与对象逐步拆解 番外：深入了解面向对象 番外：设计模式简介 Flask Web框架 将对CMDB的操作与Web框架相结合，通过API的方式对CMDB进行操作 面试番外：计算机网络 MySQL数据库 将CMDB存储到数据库中，对其进行的增删改查从对文件读写转移到对数据库进行操作 番外：MongoDB数据库 面试番外：数据库 Linux部署应用 将应用部署在Linux系统上稳定的运行 面试番外：操作系统 二、【自动化运维初级村】 其实初级村的内容顺序的设计和新手村是类似的，但这一部分所运用的Python知识就会稍微深入，所以建议已经对Python有一定了解的朋友，可以跳过新手村直接看初级村的内容，因为初级村会是更多运维朋友们经常遇到的需求场景，也是做自动化运必须要走的路。 paramiko和netmiko 两者的使用方式及优劣 Python 正则解析 将通过SSH收集回来的各种信息进行正则解析 番外：text-fsm解析 Python snmp 如何使用Python调用SNMP采集指令，并讲解OID使用方式 NetConf 如何通过NetConf对网络设备进行操作 Crontab并回调CMDB 如何设置定时执行SSH任务进行定期巡检，并更新回写CMDB 番外：Python scheduler Flask Web框架 将SSH和NetConf与Web框架相结合，通过API方式或前端调用其执行 三、【自动化运维中级村】 中级村与初级村最大的不同就是，会从简单需求应用的完成，转变为更为复杂架构的设计与实现，以及能够支撑更大型运维的场景。 Celery任务队列 如何使用任务队列异步处理大量的SSH或NetConf执行操作 番外：Redis数据库 番外：RabbitMQ中间件 Rsyslog日志采集 通过Rsyslog收集设备日志并存储进行展示和查询。 Mysql性能优化 支持更大数据量的存储和查询，进行数据库的优化，分库和分表 ELK， Rsyslog - Kafkf - LogStash - ElasticSearch - Kibana 使用ELK套件对日志进行存储及展示 前后端分离 由于系统功能的丰富，需要实现前后端分离的应用。 四、【自动化运维高级村】 如果想要做一个完整的自动化运维系统，就必然需要有相应完善的自动化运维的理念和方法论，所以高级村会设计到更多关于自动化运维的理念介绍，以及更为复杂和适应更大型企业的自动化运维系统。 Celery分时任务队列 使用Celery任务队列，将SNMP大量采集任务或SSH变更任务根据频率发布在不同的队列中进行高效采集 Flink处理 使用Flink对SNMP采集的数据进行清洗并存储 Rsyslog - Kafka - Consumer- MySQL 如何将设备上报的日志进行收集解析，并通过Kafka中间件削峰，并进行存储 番外： Kafka中间件 告警通知，屏蔽，抑制 如何设置告警的通知，屏蔽和抑制机制 任务编排 通过白屏方式将变更步骤注册为执行算子，并进行编排后下发 基于意图的分组配置下发 将设备根据角色分组，并设置基于意图的模板命令，使用SSH或NetConf方式与其结合对网络设备进行变更或查询操作 路径的总结上述的路径梳理是一个目前我能给到大家的较为系统的自动化运维方面的实践，在更新的过程中，我可能会根据读者的需要或者平时的思考总结对内容进行更新迭代，所以说路径并不是固定的，它只是一个学习的方向，并且学习的过程必然是会有阻碍和困难的，希望大家都能有坚定的决心克服一个个的难点，当然也会提供给大家学习交流的群组，供大家互相探讨，互相进步，也欢迎大家有什么疑问都找我咨询。最后期待大家都能拿到属于自己的结果。","link":"/posts/5cb4afad.html"},{"title":"1.2 自动化运维新手村-Python基础-2","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读此篇； 了解过Python基本的数据结构，但又没有经常在实践中运用的读者，建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 上一节我们通过对自动化运维的基石–CMDBv1.0的演示，为大家讲了Python的基本数据类型和相关的操作，那么这一节我们就深入cmdb-v1.0.py的源码，并了解一下Python的语句，函数以及面向对象相关的知识。 一说到阅读源码很多读者就要慌了，觉得Python都没入门就阅读源码了？首先Python的一大好处就是，代码的逻辑像阅读英文一样简洁，并且我们的cmdb-v1.0.py的源码只有一百一十行左右，就实现了对资产数据增删改查的基本功能，话不多说，马上开始. Python脚本的启动1root&gt; # python ./cmdb-v1.0.py [额外参数...] 在命令行中直接通过python加文件名就可以执行该脚本，那么当执行该脚本时，脚本内部做了什么操作呢 1234567891011121314151617181920212223242526272829303132333435if __name__ == &quot;__main__&quot;: operations = [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] args = sys.argv if len(args) &lt; 3: print(&quot;please input operation and args&quot;) else: if args[1] == &quot;init&quot;: init(args[2]) elif args[1] == &quot;add&quot;: add(*args[2:]) elif args[1] == &quot;get&quot;: get(args[2]) elif args[1] == &quot;update&quot;: update(args[2], args[3]) elif args[1] == &quot;delete&quot;: delete(*args[2:]) else: print(&quot;operation must be one of get,update,delete&quot;) 上述代码就是我们整个脚本的启动入口，大家最先看到的就是一行判断语句，那我们就先从判断语句开始讲起 条件判断 不管哪种编程语言，条件判断都是其最基本的逻辑，是让一行一行的代码能够被编排起来的最基本手段，条件判断可以实现在不同的情况下执行不同的代码块，如图所示 下面为Python判断语句的伪代码形式，当判断条件为真时执行语句1，为假时执行语句2，执行语句可以为多行，通过缩进来控制 1234567if 判断条件: 执行语句1else: 执行语句2 我们的源码中的第一行 if __name__ == &quot;__main__&quot;，这就是一个字符串判断的语句，__name__是一个Python的内置变量，它表示当前被执行脚本的名称，所以此处判断语句的含义为是否当前被执行脚本的名称等于&quot;__main__&quot;，这里有两个地方需要大家注意一下： 当使用python 文件名.py的方式执行脚本时，该脚本的__name__值即为__main__ 条件判断语句中等于通过==来表示，而非= 在掌握了判断语句的原理后，再加上我们上节内容所讲，我们就可以理解源码中启动入口的基本逻辑 1234567891011121314151617181920212223242526272829303132333435# 当前被执行脚本的名称是否等于&quot;__main__&quot;，如果等于执行以下语句if __name__ == &quot;__main__&quot;: args = sys.argv # 获取命令行输入的参数，此处sys.argv为python的内置方法 if len(args) &lt; 3: # 如果参数数量小于3个，则执行以下语句 print(&quot;please input operation and args&quot;) # 打印提示内容 else: # 如果参数数量不小于3个，则执行以下语句 if args[1] == &quot;init&quot;: # 是否参数的第二个元素等于&quot;init&quot;(数组下标从0开始) init(args[2]) # 如果等于&quot;init&quot;则执行该函数 elif args[1] == &quot;add&quot;: # 如果不等于&quot;init&quot;，则判断是否等于&quot;add&quot; add(*args[2:]) # 如果等于&quot;add&quot;则执行该函数 elif args[1] == &quot;get&quot;: # 如果也不等于&quot;add&quot;，则判断是否等于&quot;get&quot; get(args[2]) # 如果等于&quot;get&quot;则执行该函数 elif args[1] == &quot;update&quot;: # 如果也不等于&quot;get&quot;，则判断是否等于&quot;update&quot; update(args[2], args[3]) # 如果等于&quot;update&quot;则执行该函数 elif args[1] == &quot;delete&quot;: # 如果也不等于&quot;update&quot;，则判断是否等于&quot;delete&quot; delete(*args[2:]) # 如果等于&quot;delete&quot;则执行该函数 else: # 如果都不等于则执行以下打印语句，输出提示 print(&quot;operation must be one of get,update,delete&quot;) 大家可以发现只通过上述的条件判断语句就可以根据我们执行脚本时的命令行参数，去分别执行不同的增删改查的逻辑，读者可能对这里的*arg有一些疑问，我们会在番外篇中提到。 循环语句 目前我们已经掌握了让脚本启动，并且根据不同的条件判断去依次执行语句的能力，不过这时程序还只是在顺序执行，如果我们想查询多个资产信息，那么就必须多次去执行查询的语句，这时候就需要使用循环语句，循环语句可以让我们执行某一个代码块多次，如图所示 Python中的循环语句的伪代码形式如下所示 123for 判断条件: # 只要判断条件为真就会一直执行语句1 执行语句1 由于循环语句相对比较好理解，我们就先简单介绍以下，后面的源码中遇到时，再深入讲解一些细节 函数 通过上面的学习我们已经能够比较好的编排自己的代码去顺序执行或者循环执行，但对于一些可以重复使用的语句，我们可以把其组织起来，将它们定义为一个函数，这样我们后续就可以直接去使用这个函数，而不必每次都编写大量相同的语句。 Python中的函数伪代码形式如下所示 123def 函数名称(参数...)： 代码块 比如我们源码中定义的查询资产信息的函数的伪代码如下 1234567def get(path): # 函数名称为 get， 接受一个参数 path 打开资产信息的文件 根据参数path去查询资产信息中对应的信息 打印相关信息 当我们定义好这样一个函数之后，我们后续就可以十分方便的去调用它，调用的方法就是 1info = get(&quot;/beijing/switch/10.0.0.1&quot;) 其实函数的本意就是我们将一些可被复用的代码进行提取，将其中可变的变量作为参数传入，而将其相同的逻辑保留，这样我们每次只需要传入不同的参数就可以执行该逻辑，不用在需要使用该逻辑的地方再次编写冗余的代码 比如在脚本的启动入口地方，如下 1234567891011121314151617181920212223242526272829303132333435def init(path): ...def get(path): ...def delete(path): ...if args[1] == &quot;init&quot;: init(args[2])elif args[1] == &quot;get&quot;: get(args[2])else: print(&quot;operation must be one of get,update,delete&quot;) 我们只需要在执行脚本时，通过判断命令行的指令，就可以去执行不同的函数，十分方便，但这里有两个地方需要大家注意以下 定义的函数只是语句的抽象逻辑，如果不调用它，那么它就永远不会执行，比如如果我们定义了一个删除的函数，但始终都没有任何地方去调用它，那么它就永远不会被执行 print()其实也是一个函数，只不过它是Python的内置函数，它的功能相当于接收一个字符串，并将其输出到屏幕上，所以我们其实在尝试编写第一个python程序print(&quot;hello world&quot;)时，就已经无形中使用到了函数 Tips 大家可以设想以下，如果每次我们想将内容输出到屏幕，都需要自己去编写print的内部逻辑细节，那简直就是一场灾难；所以在什么时候将某个代码块抽象为函数，将多少逻辑的代码块抽象为一个函数，这其实是编程的艺术，取决于每个人对于实际场景的把握。但也有一些变成规范可依。 我们通常只将一个功能抽象为一个函数，也就是说每个函数只实现一个单一的功能。 面向对象 很多对于编程稍微有了解的读者都知道，面向对象是很多编程语言都有的一个特性，所谓面向对象其实是一种编程的思路，与之不同的思路还有面向过程； 比如同样要实现相同的功能，可以使用不同的思路，思路没有孰优孰劣之分，只要在当前场景适用即可 虽然Python实现一些简单的功能，只需要面向过程即可，比如将目标场景，拆分为不同的步骤，将每个步骤定义为函数，然后通过编排函数去实现最终的目标，但Python本身从设计之处就是一门面向对象的语言，并且Python中一切皆对象。 那么对象究竟是什么：世界上的任何事物都可以把它看成一个对象，其具有自己的属性和行为，不同的对象之间通过方法来交互。 比如Python中的某个字符串，它就是一个对象，它具有自己属性和方法，如下 12345root&gt; # a = &quot;string&quot;root&gt; # dir(a)['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'] 我们定义了一个字符串变量a，通过dir方法来查看其所具有的所有属性和方法。 面向对象只是解决问题的思路，我们并不是将问题拆解为不同的步骤，而是将问题分解为不同的对象，在我们的CMDBv1.0中，我们要解决的问题就是实现资产信息的增删改查，那么我们分析该问题发现需要两个对象来解决： 资产对象 属性：可以执行的操作，当前的版本，创建时间，上次修改的时间等 方法：增、删、改、查 信息存储对象 属性：存储的位置，数据的大小 方法：存，取 所以经过我们面向对象的拆解，最终将我们的问题从面向过程，即根据命令行参数的传入去执行不同的函数，而修改为了我们与资产对象进行交互，而资产对象与存储对象进行交互。 知识总结 开始初步阅读CMDBv1.0版本的源码 讲解了Python基本的条件语句和循环语句 讲解了Python的函数，以及面向对象的分析 CMDB系列第二节我们就暂且讲到这里，对于判断语句和循环语句还有很多细节没有涉及到，但我们已经掌握了其基本的原理，并且我们了解了面向对象的思路。 后面的章节我们会继续阅读CMDBv1.0的源码，了解更为细节的内容，并且用面向对象的思路将CMDBv1.0改造为CMDBv1.5，敬请期待。 篇后语很多读者在阅读的过程中可能发现，我们很多的知识都是浅尝辄止，看似都是一些皮毛，并没有什么真材实料，包括阅读源码也是，只看了个大概； 其实不然，当我们新上手一门新的技术时，我们并不能揪住一个知识点不放，比如字符串是一个对象，通过dir可以发现它有二十多个属性和二十多种方法，但我们难道要在一开始就都掌握并把他们背会吗？ 答案当然是否定的，这些细节我们初期都不需要去深究，我们的场景是构建一个简易版CMDB，那么我们只需要一步一步将这过程中阻碍我们前进的知识掌握即可，具体的细节可以在后续的深入过程中去慢慢了解。 这也是为什么很多学生不愿意听老师讲课的原因，因为他并不知道我这节课学的知识点有什么用，只是机械的接受老师的灌输，所以好的学习方法一定是自顶向下的，希望读者朋友们能体会到其真正的内涵。","link":"/posts/a45b0691.html"},{"title":"1.3 自动化运维新手村-Python基础-3","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 上一节我们简单地阅读了CMDBv1.0.py的源码，了解到其基本的模块构成，并且结合实际的程序理解了Python的常用数据结构以及基本的条件语句与循环语句的运用。那么这一节我们将详细阅读每一个模块的代码，并将其所涉及的知识点拆解开来，帮助各位读者更快的进入到Python的世界。 为什么要阅读源码填鸭式学习在开始之前我想向大家说明的是，之所以才在第三节就带领大家深入到源码中去，是因为Python作为最易上手的解释型语言，其社区生态和包含的各种第三方库数不胜数，而且任何一门语言其内置的规范和方法对于初学者来说都无法穷尽，所以如果一点儿一点儿的去学习某个内置方法，某个包如何使用，在我看来其实是本末倒置的，任何在没有使用场景的情况下进行填鸭式的灌输内容，都是”耍流氓“。 场景&lt;-&gt;知识点比如大家其他新手教程里十分常见的读写文件功能，说实话读写文件也就是一两行代码可以完成的事情，但哪怕你看过无数次如何读写文件，到了真正需要用它的时候，都是一脸懵”x”的，你只能模糊的记得好像学到过，但就是无法下手，本质上是因为你没有真正的场景去用它，对于刚接触编程的读者来说，一些简单的示例，无法在脑海中构建起一个完整的程序，这也是我刚开始就带领大家由浅入深去接触源码的原因。 编程思维首先这里的源码是对于我们运维人来说都熟知的CMDB的基本功能，大家在了解其功能的情况下，去思考如果这个代码是你写的，你会如何去实现这个CMDB基本的增删改查，然后在我们一起阅读源码的过程中，你会发现真正实现这些功能的过程是怎样的，慢慢去深入到具体的细节又该如何编写代码，这其实就是一个编程的思维，我们学任何一门语言，都不止是单纯的为了去学会它，然后用它，而是要了解面对一个大问题时候用编程去解决他的思维逻辑，这也就是为什么一些计算机专业毕业的同学，可以快速应对很多新技术的变迁。所以希望大家在阅读源码前能够有这样的认知，并且带着思考去一起学习。 CMDBv1.0.py伪代码首先我们先再熟悉一下CMDBv1.0.py的伪代码 123456789101112131415161718192021222324252627282930313233def get(): 查询内容def update(): 修改内容def delete(): 删除内容def init(): 初始化信息 def add(): 添加内容if __name__ == &quot;__main__&quot;: 通过命令行参数执行对应的操作 阅读源码前的思考上面说到希望读者们在阅读源码前要带着思考，这里的思考准确的就说，如果是你，你会如何去设计并且实现所需要的功能，虽然你不知道具体的代码如何写出来，但起码要去思考每一步应该怎么做，对于一些刚接触编程的读者，一开始就由我先来带着大家思考。 很多人一看到我们第一节中演示的功能时就已经懵了，内心os：”这么多复杂的步骤我从哪儿下手呢，根本毫无头绪“。那么其实我们的CMDBv1.0最主要的功能就是增删改查，具体的代码就应该是分别去实现增删改查四个不同的功能，然后在不同的情况下去使用这些功能就好了，这种思考貌似听起来是废话，但其实不然。如果你真的能够像我描述的这样思考，那恭喜你，其实你已经算是迈出了一大步，已经具备了将一个大问题去拆解成小问题的能力；如果你能再将其落地成我上述的伪代码形式，那么再次恭喜你，你已经具备了从一行行的脚本代码跨越到函数式编程的能力。 一开始不具备这种思维方式很正常，现在我已经向你演示了如何去分析拆解一个场景，那么接下来我们就继续深入，看看每一个步骤都是如何实现的。 上源码初始化地域信息一般对于运维资产的维护中，最顶层的是地域，也就是说我的设备是放在哪里的，那么我们CMDB如果需要录入设备信息之前，就必须要先初始化数据，那么初始化地域的代码如下： 1234567import jsondef init(region): data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} print(json.dumps(data, indent=2)) 我们定义了一个函数叫做init，而这个函数需要传入一个region名称作为参数，这里的region其实就相当于我们的地域，我们此处假设一个region只有一个idc机房，那么我们需要初始化的数据格式如下： 12345678910111213data = { &quot;region名称&quot;: { &quot;idc&quot;: &quot;region名称&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {} }} 而根据我们第一节课对于Python基本数据类型的学习来看，我们的数据源data是一个字典，那么给字典赋值的操作如下： 1data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} 我们初始化好数据之后，肯定想将其打印出来看一看，那么可以使用print(data)将数据打印出来，但此处我们的data是一个字典，直接进行打印的结果可能像是如下这样： 1{&quot;region&quot;: {&quot;idc&quot;: &quot;region&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {}}} 这种格式当数据越来越多的话就会很难阅读，所以我们利用到了一个Python的内置库json Tips：json库解读 Json（JavaScript Object Notation 的缩写）是一种数据交换格式，最常用于客户端-服务器通信；当然你也可以将它保存到本地，所以也可以用来作为配置文件；Json 很像 Python 中的字典，但Json本质上是一种字符串，所以在Python中需要利用其内置库 json，来实现Json字符串和字典的转换。 最常用的将Python中的字典与Json字符串进行转换的两个方法是 123data = json.loads(data_str) # 将json字符串转为字典data_str = json.dumps(data) # 将字典转为json字符串 代码中是将字典转为字符串之后打印出来，但大家注意代码中的写法是json.dumps(data, indent=2)，这时因为json.dumps可以传入很多参数，如下： 1json.dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, encoding=&quot;utf-8&quot;, default=None, sort_keys=False, **kw) 无论对于Python的内置方法时候第三方库，我们在调用其方法的时候，都可以通过跳转的方式，去进入到其源码中去看到他的实现逻辑，这里最常做的就是去看这些方法的注释以及参数说明。 Tips: 常见的vscode或者Pycharm都可以通过快捷键的方式去进行跳转，windows可以按住Ctrl+单击即可跳转，mac的话按住command+单击即可跳转。 这里我们跳转到json.dumps方法中去看看它的参数注释，但我们本着没用到就先忽略的原则，我们只了解其中的indent参数即可，注释如下： 12345If ``indent`` is a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indentlevel of 0 will only insert new lines. ``None`` is the most compact representation.友情翻译如下：如果indent是一个非负整数，那么JSON数组或者对象成员将会被带着缩进层级优雅的打印出来，一个为0的缩进只会插入换行符，而当其为None时，JSON只会被最紧凑的表示出来。 所以通过注释可以理解为，indent其实就是帮助我们将json字符串好看的打印出来的参数，只要是正整数即可，大家可以自己去尝试不同的正整数打印出的结果。 现在我们已经了解了如何初始化一个地域，并且将其信息打印出来，但有一点需要注意的是，当程序结束后，我们的数据源data就会消失，当我们下次再次运行程序时，我们的数据源data仍然时空的。这里就涉及到数据的持久化。 Tips：持久化 因为Python程序运行过程中定义的所有变量，都只会存在于程序运行时计算机为其分配的内存空间里，此处设计到计算机组成原理的相关知识，我们会在番外篇中提到。而持久化的含义就是将数据永久的保存在磁盘上，这样我们每次都可以从磁盘上去读取数据。 Python中持久化数据的方式有很多种，而且在实际的企业应用中，肯定是将这些数据保存在数据库中，但因为我们目前还没有涉及到数据库的知识，且数据量较小，我们就暂且将其保存在文本文件中即可。这里我们仍然通过Python内置的json库来做数据的持久化和读取。 json库中还有两个常见的方法是 123data = json.load(f) # 从文件中读取内容并转为字典json.dump(data, d) # 将字典存到文件中 json数据的持久化如下 12345f = open(&quot;data.json&quot;, &quot;w+&quot;)json.dump(data, f)f.close() json数据的读取如下 12345f = open(&quot;data.json&quot;, &quot;r+&quot;)data = json.load(f)f.close() 划重点这里需要考虑一个特殊情况，当我们传入的region已经存在的时候会发生什么，比如有一个不太熟悉资产的同学使用了这个脚本，或者地域信息太多你自己也忘记已经存在哪些地域，那么这时候如果使用初始化功能去初始化一个已存在的地域时，这个地域的信息是不是就被清楚掉了，所以这里还需要补充一个判断条件 12345if region in data: print(&quot;region %s already exists&quot; % region) return 这里用到了一个条件判断，可以直接用 if key in dict的方式去判断字典中是否已存在这个键，当存在时我们就打印提示信息，并直接通过return退出该函数。 这里还用到了格式化字符串的操作，当我们的一个字符串中某个值为变量时，我们可以用上面的方式去表达，这里的%s表示字符，与之对应的还有%d表示整数，%f表示浮点数等，具体的其他格式化方法我们会在番外篇中提到。 Tips： 函数返回值 Python中的函数必然存在返回值，返回值可以是一个，也可以是多个，当函数中没有任何return语句时，函数的返回值即为None，当我们想要在指定地方显示的退出函数时可以直接用return，这时函数的返回值也为None。 当存在多个返回值时，可以直接使用return a, b，比如： 1234567891011121314151617181920212223def foo(): a = 1 b = 2 return a, b res = foo() # 这时的 res 值是一个元组类型， 输出结果为 (1, 2)a, b = foo() # 这里是用到了Python中的解包写法，解包的写法可以运用于Python的任何可迭代对象，比如 &gt; my_list1 = [1, 2]&gt; a, b = my_list # 最终的结果 a 为 1， b 为 2# 所以 a, b = foo() 等价于 res = foo()a, b = res 所以CMDB初始化地域的完整代码如下： 12345678910111213141516171819def init(region): with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) if region in data: print(&quot;region %s already exists&quot; % region) return data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} with open(&quot;data.json&quot;, &quot;w+&quot;) as f: json.dump(data, f, indent=2) print(json.dumps(data, indent=2)) 细心的读者可能会发现怎么持久化的操作不太一样，这里是用到了一个Python中的语法糖，with... as，这样可以在对文件进行操作时，可以避免最后手动执行f.close()，具体with...as的原理我们会在番外篇中提到，这里大家先记住即可。 那么我们对于源码的解读这一节就先到这里，我们这一节最主要的是需要去学习培养正确的编程思维，学会在阅读源码前如何思考，如何带着思考去拆解源码。后面的章节我们会继续阅读其他功能模块，带领大家更进一步的探索Python的世界。 篇后语 最近听到了一个词叫做”知识的诅咒“，含义大概是，当你对某一个知识了解较深时，就无法准确的向别人解释清楚这个知识，因为你总是假设别人也和你一样有相同的知识背景。所以我在向大家传递Python的相关知识的时候，我其实有在刻意的避开这种诅咒，我写这个系列的本意也是想让没有接触过编程的同学能够更为通俗易懂的了解编程，并且上手写出自己真正需要的代码。所以如果大家对于我的讲解顺序或者逻辑有什么疑问和建议，也欢迎提出。","link":"/posts/d35c3607.html"},{"title":"1.4 自动化运维新手村-Python基础-4","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 即了解Python，又了解CMDB的读者，可以出门左转，看下一篇。 上一节我带领读者们在阅读源码前进行了一系列思考，培养了一下大家的编程思想，并且紧接着阅读了CMDB v1.0.py的部分源码，那今天我们开篇就不再过多赘述，接上一节直接上干货。 上干货添加资产信息在初始化好地域信息之后，我们首先要做的就是添加资产信息了 按照我们上节课所讲，先要思考一下实现这个功能有哪些地方需要注意： 添加什么样的信息？ 将信息添加到哪里，如何定位到要添加的路径？ 如何把更新的数据持久化？ 这里我先依次给大家解答一下： 因为我们数据源是以JSON的格式存储在文本文件中，所以必须保证我们添加的信息也是json格式 JSON格式的数据源对应的是Python中的字典，所以字典是可以根据键进行索引的，那么我们可以通过多个键的排列去依次进行查找定位字典的位置，比如可以是key1/key2/key3的形式 数据的持久化依然选择通过JSON的方式将数据持久化到文本文件中 我们已经基本有了实现添加资产信息的思路，现在要做的就是将思路更进一步细化到可实现的伪代码： 我们需要定义一个add()函数来实现这个功能，并且这个函数需要接收两个参数，分别是要添加的信息和信息要更新到的指定路径，那么我们的函数签名应该是add(attrs, path) 这里的attrs是属性attributes的缩写，在写代码的过程中希望大家培养为变量起一个合适规范的名字的好习惯，初学者尽量避免使用拼音来给变量或函数命名，应该使用能表明变量含义的命名方式。 我们传入的attrs必须是一个json格式的字符串，传入的path必须是一个通过/分隔的字符串 通过path去按层级定位数据源中的指定位置，通过字典的赋值将attrs添加到数据源中 通过json.load和json.dump做数据持久化 接下来就是需要写出一份能实现上述功能的伪代码，如下： 123456789101112131415161718192021222324252627282930313233def add(path, attrs): # 判断attrs的合法性 if attrs is valid # 将attrs解析成Python类型 attrs = parse_attrs() # 从文本文件中读取数据源 data = read_file() # 分割path路径 seg = path.split() # 根据路径定位数据源的指定位置 target_path = position_data() # 将attrs添加到指定路径 data[target_path] = attrs # 将数据保存到文本文件 write_file(data) # 打印数据源 print(data) 那么最终添加资产信息的源代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def add(path, attrs=None): if attrs is None: # 判断attrs的合法性 print(&quot;add operation must have attrs arg&quot;) return try: # 将attrs解析成Python类型 attrs = json.loads(attrs) except Exception: print(&quot;input attribute is an invalid json string&quot;) return with open(&quot;data.json&quot;, &quot;r+&quot;) as f: # 从文本文件中读取数据源 data = json.load(f) path_seg = path.split(&quot;/&quot;) # 分割path路径 target_path = data for idx, seg in enumerate(path_seg[1:]): # 根据路径定位数据源的指定位置 if idx == len(path_seg)-2: if seg in target_path: print(&quot;%s already exists in %s, please use update operation&quot; % (seg, path)) return target_path[seg] = attrs # 将attrs添加到指定路径 break target_path = target_path[seg] with open(&quot;data.json&quot;, &quot;w+&quot;) as f: # 将数据保存到文本文件 json.dump(data, f, indent=2) print(json.dumps(data, indent=2)) # 打印数据源 很多读者可能第五步有点儿懵，有种刚学会1+1=2就跳到高等代数微积分的感觉，但其实把上述代码的每一部分单独拿出来还都是比较好理解的。 判断attrs的合法性并将attrs解析成Python类型 123456789101112131415if attrs is None: print(&quot;add operation must have attrs arg&quot;) returntry: # 将attrs解析成Python类型 attrs = json.loads(attrs)except Exception: print(&quot;input attribute is an invalid json string&quot;) return 这一步在解析参数的同时也是在判断attrs的合法性，因为我们要求输入的attrs参数不能为None且必须是一个json类型的字符串，那么我们理论上就可以通过json.loads将其进行解析，如果解析失败那么就说明该参数不是合法的json，就需要退出该函数；如果attrs为合法的json，那么我们就可以将其解析为Python中的数据类型应用于下面的代码中。关于try...except的详细讲解我们会在番外篇中提到。 Tips Python中None的判断 在Python中判断一个变量是否是None的写法不是 if var == None而是if var is None，这里我们推荐大家使用第二种方法，具体关于等于判断的区别我们会在番外篇中提到。 从文件中读取数据源 这一步就是运用我们上节课所讲的内容，此处不再赘述 分割path路径 1path_seg = path.split(&quot;/&quot;)[1:] # 分割path路径 这里就是运用到Python中对字符串的操作，str.split()用于分割字符串，通过传入分隔符，可以将字符串按分隔符切分成数组返回，所以这里的path_seg就是一个路径的数组，我们只需要根据这个数组，一层一层的定位到数据源的指定位置即可。这里同时运用到了数组切片的原理，因为我们的路径假设为/region/idc/switch的格式，所以按照/切割后，路径数组为[&quot;&quot;, &quot;region&quot;, &quot;idc&quot;, &quot;switch&quot;]，第一个元素为空字符串，所以通过path_seg[1:]的方式只取第二个到最后一个的路径元素。 根据路径定位数据源的指定位置并将attrs添加到指定路径 1234567891011121314151617target_path = datafor idx, seg in enumerate(path_seg): # 根据路径定位数据源的指定位置 if idx == len(path_seg)-1: if seg in target_path: print(&quot;%s already exists in %s, please use update operation&quot; % (seg, path)) return target_path[seg] = attrs # 将attrs添加到指定路径 break target_path = target_path[seg] 这一块可能是一个难点，需要大家对循环的有一定的理解，首先我们先定义一个目标路径的变量target_path，它一开始等于整个data的最外层，在后面的循环中它会不断的更新；在循环语句中用到了一个Python的语法糖enumerate()，通过传入一个可迭代对象(此处为我们的路径数组)，可以对于下标和内容同时进行循环遍历，所以for idx, seg in enumerate(path_seg[1:])这里的idx和seg分别表示路径数组中某一段路径的下标和内容。 这里向大家说明一下循环语句的本质原理，循环其实就是有一个可以重复的操作不停的在执行，当达到某一个边界条件时就退出循环，所以一般的循环语句都会存在边界条件，如果没有边界条件我们就称其为死循环。 我们上述代码块的边界条件就是遍历完整个路径数组，在每次遍历的时候对data一层一层的取值并返回，直到我们遍历到路径数组的最后一个元素（也就是其下标idx == len(path_seg)-1，之所以 -1是因为下标是从0开始的，所以数组的长度会比最后一个下标大1），这时候我们判断这个路径元素是否存在于当前位置，如果存在则说明不可以进行添加，直接通过return退出函数，如果不存在则我们通过字典赋值的方式将attrs添加到该位置，并通过break结束循环。 Tips: break与continue 对于刚接触编程的读者可能不太清楚break与continue的区别，break可以理解为直接退出这个循环，不管这个循环有没有到达边界条件；而continue则是跳过此次循环，如果还没有达到边界条件则继续进行下一次循环。如下代码 12345678910111213141516171819for i in [1, 2, 3]: if i == 2: continue print(i) # 输出 1 3 for i in [1, 2, 3]: if i == 2: break print(i) # 输出 1 上面的描述可能会有些晦涩难懂，下面我们通过Debug的方式看看每次循环时候的变量值就会清晰很多。 假设我们已经执行了语句python cmdb-v1.0.py init beijing，这时我们的数据源如下： 12345678910111213{ &quot;beijing&quot;: { &quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {} }} 那么这时候我们执行 python cmdb-v1.0.py add /beijing/switch/10.0.0.1 '{\\&quot;ip\\&quot;: \\&quot;10.0.0.1\\&quot;, \\&quot;role\\&quot;: \\&quot;asw\\&quot;}'进行调试 /beijing/switch/10.0.0.1就是我们要指定的路径 '{\\&quot;ip\\&quot;: \\&quot;10.0.0.1\\&quot;, \\&quot;role\\&quot;: \\&quot;asw\\&quot;}'就是我们要添加的信息，这里的信息是一个json格式的字符串 在还没有开始循环前的各变量值如下 此时的target_path = {&quot;beijing&quot;: {&quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {}}} 第一次循环结束后各变量值如下 此时的seg = &quot;beijing&quot; target_path = {&quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {}} 第二次循环结束后各变量值如下 此时的seg = &quot;switch&quot; target_path = {&quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {}} 当最后一次循环时 此时seg = &quot;10.0.0.1&quot; targe_path = {}，idc与path_seg长度相等，且seg原先不存在，所以可以将attrs更新到target_path上去。 根据上面一步一步的调试，我们可以清晰的看到每次循环中seg和target_path的变化，其实target_path是一个指针，它最开始指向字典的最外层，随着一次次的循环，它根据seg层层递进，直到指向目标路径，这时候将attrs添加上去就完成了最终操作。 最后的数据如下： 1234567891011121314151617181920212223{ &quot;beijing&quot;: { &quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: { &quot;10.0.0.1&quot;: { &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;role&quot;: &quot;asw&quot; } }, &quot;router&quot;: {} }} 这一节我们就先讲到这里，这次我们主要讲解了添加资产信息的详细源码，看起来虽然代码不长，但实际上需要注意的思想和需要新手朋友们注意的知识点还是有很多，希望大家可以自己亲自去调试运行一下，仔细体会一下每一次循环过程中变量的变化。一起期待我们下一节的继续讲解。 篇后语其实这一节除了代码细节的讲解之外，我们在阅读源码前的五个步骤是更为关键的部分。根 据我上面五个步骤的讲解，大家可以再次发现，在编程的过程中，前一到四步可以说是最终代码成型的地基，并且上面的四个步骤在进行的过程中并不需要我们真正掌握哪一门具体的编程语言，而是需要我们充分利用编程的思想，将要解决的问题逐步拆解；第五步才是真正需要利用代码实现，而且我们选择的Python是较为容易上手的语言，这也是对刚接触编程的朋友来说比较友好的。 最后希望大家能够在跟随我一步一步学习的过程中培养起良好的编程思想。��好的编程思想。","link":"/posts/4d38a3a4.html"},{"title":"1.5 自动化运维新手村-Python基础-5","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 即了解Python，又了解CMDB的读者，可以出门左转，看下一篇。 上一节可能对刚开始编程的读者朋友们有一点挑战，其中涉及到通过循环来对复杂数据结构的修改，但只要大家认真理解了上一节的内容，这一节的内容就会感觉简单很多，这一节我们接着阅读剩余的对CMDB进行删改查的代码部分。 上干货在讲解删改查之前，我们需要再次回顾一下前两节的代码，看看有没有什么可以优化的地方。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import jsondef init(region): with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) if region in data: print(&quot;region %s already exists&quot; % region) return data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} with open(&quot;data.json&quot;, &quot;w+&quot;) as f: json.dump(data, f, indent=2) print(json.dumps(data, indent=2)) def add(path, attrs=None): if attrs is None: print(&quot;add operation must have attrs arg&quot;) return try: attrs = json.loads(attrs) except Exception: print(&quot;input attribute is an invalid json string&quot;) return with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) path_seg = path.split(&quot;/&quot;)[1:] target_path = data for idx, seg in enumerate(path_seg): if idx == len(path_seg)-1: if seg in target_path: print(&quot;%s already exists in %s, please use update operation&quot; % (seg, path)) return target_path[seg] = attrs break target_path = target_path[seg] with open(&quot;data.json&quot;, &quot;w+&quot;) as f: data = json.dump(data, f, indent=2) print(json.dumps(data, indent=2)) 仔细的读者可能之前就已经发现，我们的初始化地域功能和新增资产信息功能都有对数据源的存取操作，而且看起来好像代码完全一样，那么在代码的重构原则中有涉及到，如果一段代码在两处及以上地方重复编写，那么就需要将其重构为单独的方法。 这个的意思就是说我们对数据源的存取已经有两处都使用了，而且可预见的是在之后的删改查中也会使用，那么我们就需要将其单独抽象为一个函数，这样就可以被不同的地方重复调用，重构后代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import jsondef read_file(): with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) return datadef write_file(data): with open(&quot;data.json&quot;, &quot;w+&quot;) as f: json.dump(data, f, indent=2)def init(region): data = read_file() if region in data: print(&quot;region %s already exists&quot; % region) return data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} write_file(data) print(json.dumps(data, indent=2)) def add(path, attrs=None): if attrs is None: print(&quot;add operation must have attrs arg&quot;) return try: attrs = json.loads(attrs) except Exception: print(&quot;input attribute is an invalid json string&quot;) return data = read_file() path_seg = path.split(&quot;/&quot;)[1:] target_path = data for idx, seg in enumerate(path_seg): if idx == len(path_seg)-1: if seg in target_path: print(&quot;%s already exists in %s, please use update operation&quot; % (seg, path)) return target_path[seg] = attrs break target_path = target_path[seg] write_file(data) print(json.dumps(data, indent=2)) 大家可以看到，已经完成的代码会随着合理的重构优化而减少，因为我们会避免掉冗余的代码块，所以编程绝不是看谁写的行数多谁就会更牛X。 更新资产信息同样的更新资产信息的功能我们也需要五步法进行思考和实践 先要思考一下实现这个功能有哪些地方需要注意： 更新什么样的信息？ 如何定位到要更新的路径？ 这里我先依次给大家解答一下： 如何定位到要更新的路径这里我们在上一节介绍过，还不太理解的读者可以看上一节内容 我们要更新的信息的类型是一个需要注意的地方，因为我们的信息可以是字典格式，也可以是字符串或数组 现在要做的就是将思路更进一步细化到可实现的伪代码： 我们需要定义一个update()函数来实现这个功能，并且这个函数需要接收两个参数，分别是要更新的信息和信息要更新到的指定路径，那么我们的函数签名应该是update(path, attrs) 我们传入的attrs必须是一个JSON格式的字符串，传入的path必须是一个通过/分隔的字符串 通过path去按层级定位数据源中的指定位置，通过字典的赋值将attrs更新到数据源指定位置上 通过json.load和json.dump做数据持久化 接下来就是需要写出一份能实现上述功能的伪代码，如下： 123456789101112131415161718192021222324252627282930313233def update(path, attrs): # 判断attrs的合法性 if attrs is valid # 将attrs解析成Python类型 attrs = parse_attrs() # 从文本文件中读取数据源 data = read_file() # 分割path路径 seg = path.split() # 根据路径定位数据源的指定位置 target_path = position_data() # 将attrs更新到指定路径 data[target_path] = attrs # 将数据保存到文本文件 write_file(data) # 打印数据源 print(data) 大家可以发现更新和添加的逻辑十分相似，事实上在实际的其他增删改查场景中，更新和添加也都是如此。 最终更新资产信息的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def update(path, attrs=None): if attrs is None: # 判断attrs的合法性 print(&quot;add operation must have attrs arg&quot;) return try: attr_json = json.loads(attrs) except Exception: print(&quot;attributes is not valid json string&quot;) return data = read_file() target_path = data path_seg = path.split(&quot;/&quot;) for idx, seg in enumerate(path_seg[1:]): if idx == len(path_seg)-2: if seg not in target_path: print(&quot;update path is not exists in data, please use add function&quot;) return if type(attr_json) != type(target_path[seg]): print(&quot;update attributes and target_path attributes are different type.&quot;) return if isinstance(attr_json, dict): target_path[seg].update(attr_json) elif isinstance(attr_json, list): target_path[seg].extend(attr_json) else: target_path[seg] = attr_json else: target_path = target_path[seg] write_file(data) print(json.dumps(data, indent=2)) 更新有两个关键点需要大家注意一下： 5.1 在更新操作时，我们是对数据源中已存在的路径进行更新，这时候就涉及到数据的安全性，如果**attrs**为**None**可能会造成将原有信息清除。 判断attrs的合法性 12345if attrs is None: # 判断attrs的合法性 print(&quot;add operation must have attrs arg&quot;) return 我们首先要保证的就是attrs这个参数不能为None，**None**是Python中的一个表示空的变量类型，所以如果我们没有从命令行获取到attrs时，那么我们的程序应该给出提示，这里我是打印了一行提醒，要求添加资产的操作必须有attrs参数，然后直接return退出函数 5.2 第二点就是对于更新信息的类型，在添加功能中由于是在原先不存在的路径上新增信息，所以我们无需考虑**attrs**的类型，直接利用字典的特性进行赋值即可；但更新时，由于路径上已经存在数据，所以我们就需要对其类型做较为详细的判断。 更新属性 1234567891011121314151617181920212223if seg not in target_path: print(&quot;update path is not exists in data, please use add function&quot;) returnif type(attr_json) != type(target_path[seg]): print(&quot;update attributes and target_path attributes are different type.&quot;) returnif isinstance(attr_json, dict): target_path[seg].update(attr_json)elif isinstance(attr_json, list): target_path[seg].extend(attr_json)else: target_path[seg] = attr_json 需要判断要更新的路径是否在数据源中存在，如果不存在的话就需要使用添加的功能进行添加 需要对数据源中指定路径的类型和attrs的类型进行比较，如果类型不同也不可以进行更新 数据源中指定路径的类型是字典的话不可以直接赋值，这样会将原先的属性信息抹除，这里需要用到字典的一个特性dict.update()，这个功能接收一个参数，可以将两个字典合并，并且用参数字典中的信息更新原始字典中的信息。 如果源路径上的信息类型是数组，那么我们就需要将要更新的attrs添加到原来的信息上，这里用到了数组的一个特性list.extend()，这个功能接收一个参数，可以将参数数组合并到原始数组后面。 Tips: extend 和 append 关于数组的这两个方法是平时经常使用到的，通过例子大家就可以很好的理解用法： 123456789101112131415&gt; my_list = [1, 2, 3]&gt; new_list = [4, 5]&gt; my_list.append(new_list)&gt; my_list# 输出 [1, 2, 3, [4, 5]]&gt; my_list.extend(new_list)&gt; my_list# 输出 [1, 2, 3, 4, 5] 可以发现，append是将某个元素整体添加到了原始数组的末尾，而extend是将新的数组整合到原始数组末尾，并且通过查看这两个方法参数也可以看出区别 123456def append(self, __object: _T) -&gt; None: ...# append 方法可以接收传入任意类型的参数，因为这个方法只是将参数添加到了原始数组末尾def extend(self, __iterable: Iterable[_T]) -&gt; None: ...# extend 方法要求传入的参数必须是一个可迭代类型，因为这个方法会去迭代参数中的所有元素，将其整合到原始数组末尾 如果源路径上的信息类型不是字典也不是数组就可以直接赋值 到目前为止更新属性的功能也已经讲解完了，更新和添加大体上的逻辑类型，但更新中用到了大量了逻辑判断，关于判断语句还有一个需要和读者们讲解的地方，比如以上面更新方法中的多个逻辑判断为例，很多刚接触编程的读者可能会这样写： 1234567891011121314151617181920212223if seg in target_path: if type(attr_json) == type(target_path[seg]): if isinstance(attr_json, dict): target_path[seg].update(attr_json) elif isinstance(attr_json, list): target_path[seg].extend(attr_json) else: target_path[seg] = attr_json else: print(&quot;update attributes and target_path attributes are different type.&quot;)else: print(&quot;update path is not exists in data, please use add function&quot;) 通过这样多层次的**if...else...**嵌套，虽然也可以实现相同的功能，但对于代码的可读性上会是很大的灾难，并且多层嵌套对于后期逻辑的修改也是十分困难的，所以我们在编程的同时一定要尽力避免这种多层的嵌套。 一般常用的标准是对于循环语句和判断语句不要存在三层及以上的嵌套。那么当我们遇到上面的这种情况时，我们可以参照我给出的代码示例，先去判断非法逻辑，如果命中非法逻辑则直接抛出异常或者退出函数，这样一个简单的改动对代码的可读性和可维护性都会大大提高。 这一节我们又着重复习了一次五步法，并且对于更新功能的逻辑做了详细的解读，本来想把删除和查询也一起在这一节讲解，但又担心知识点太多，大家一时不太容易接受，之后我就不会再带着大家去一步一步的练习五步法，但这是一个熟能生巧的过程，希望读者朋友们能在自己实践的过程中潜移默化的使用它，我们下一节见。 篇后语​ 不知道大家在这几篇的学习中有没有发现，不管是在编程还是阅读源码前的逻辑梳理都十分的重要，而且代码中的很多部分都是对一些边界case的处理，所以对于伪代码的抽象也可以帮助我们更好的去理解复杂的业务逻辑。但这些边界case对于代码的健壮性又起到了关键作用，所以读者朋友们在编程的同时，也应该去培养对于边界case的敏感度，从不同维度去预判代码或者业务逻辑可能出现的逻辑，并提前规避它。 ​ 除此之外，虽然很多读者朋友是刚接触编程，但我们仍然从编程思维的养成和源码的阅读上，向大家普及一些更深入的东西，比如重构的原则，和多层嵌套的优雅处理等，所以我的本意是能够将这些知识在刚开始学习的时候就耳濡目染的让大家去了解，而不是说新手就应该死记硬背一些基础的方法和规范，这对于学习来说反而会适得其反，所以希望大家能在阅读文章的同时仔细去感受体会。","link":"/posts/3a3f9332.html"},{"title":"1.6 自动化运维新手村-Python基础-6","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 即了解Python，又了解CMDB的读者，可以出门左转，看下一篇。 前面几节我们完成了CMDBv1.0版本最难的部分的讲解，这节内容我们就带领大家一次将删除和查询功能分析完成。话不多说上干货。 代码优化之前我们的新增和更新信息的功能中都有对attrs做校验和解析，那么我们是不是可以将其抽象成一个新的函数，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243def check_parse(attrs): if attrs is None: # 判断attrs的合法性 print(&quot;attributes is None&quot;) return try: attrs = json.loads(attrs) return attrs except Exception: print(&quot;attributes is not valid json string&quot;) return def add(path, attrs=None): attrs = check_parse(attrs) if not attrs: return ...def update(path, attrs=None): attrs = check_parse(attrs) if not attrs: return ... 恭喜我们的代码又成功减少几行 删除资产信息这一节我们就省略五步法的一些步骤，只对最关键的功能进行一下思考 在任何场景中一旦涉及到删除功能，就需要慎之又慎，绝不能多删误删，不然可能就要背锅走人了，那么我们在删除资产信息时需要注意什么呢，其实有时候为了保险期间我们会尽量使用更新去代替删除，但有一些多余的属性信息又不得不删除。 那么如果我们删除的路径上是一个字符串或者数字还比较简单，如果是一个字典，或者是一个数组，就需要格外注意了。 另外就是对于我们的参数，我们是否需要同时传入path和attrs。 源代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def delete(path, attrs=None): attrs = check_parse(attrs) path_seg = path.split(&quot;/&quot;)[1:] data = read_file() target_path = data for idx, seg in enumerate(path_seg): if seg not in target_path: print(&quot;delete target path not in data.&quot;) return if idx == len(path_seg)-1: if not attrs: target_path.pop(seg) break if isinstance(attrs, list): for attr in attrs: if attr not in target_path[seg]: print(&quot;attr %s not in target_path&quot; % attr) continue isinstance(target_path[seg], dict): target_path[seg].pop(attr) if isinstance(target_path[seg], list) target_path[seg].remove(attr) break target_path = target_path[seg] write_file(data) print(json.dumps(data, indent=2)) 这里首先仍然是对传入的属性值做解析，我们为什么不像add和update一样复用check_parse()方法，当解析到的attrs为None时就退出函数呢? 这里是因为我们的删除功能，可以不传attrs参数，有时候我们的目的就是直接删除数据源中的这个路径下的所有属性，那么就只需要传入path即可。 在查找指定路径的时候我们同样也做了优化，如下： 123456789for idx, seg in enumerate(path_seg): if seg not in target_path: print(&quot;delete target path not in data.&quot;) return ... 可以和之前定位路径的代码做一下对比： 1234567891011for idx, seg in enumerate(path_seg): if idx == len(path_seg)-1: if seg not in target_path: print(&quot;delete path is not exists in datan&quot;) return ... 我们之前在定位路径时，对path做了分割，只有在seg为path_seg的最后一个元素时才去判断是否这个seg在target_path上，这样就会导致程序运行很多无用的循环逻辑。 优化之后我们在每次循环的一开始就对seg做了判断，因为如果被分割开的path_seg中任何一段seg不在数据源路径中时，那么整段path就必然不可能在数据源中定位到，所以我们一旦检测到当前的**seg**不在**target_path**时就可以直接退出函数 删除功能中的核心代码块如下： 12345678910111213141516171819202122232425if idx == len(path_seg)-1: # 循环中定位到指定路径 if not attrs: target_path.pop(seg) if isinstance(attrs, list): for attr in attrs: if attr not in target_path[seg]: print(&quot;attr %s not in target_path&quot; % attr) continue if isinstance(target_path[seg], dict): target_path[seg].pop(attr) if isinstance(target_path[seg], list): target_path[seg].remove(attr) break 删除属性主要分为三个部分： 1.当我们没有传入要删除的attrs时，我们默认删除该路径下的所有内容，这里用到的操作是字典的删除功能dict.pop()，这个方法要求传入一个字典的键值，键值如果不存在会抛出异常，但由于我们在每次循环时都判断了seg是否在target_path中，所以程序运行到这里的话，这个路径就必然是存在的，那么我们通过target_path.pop(seg)就可以将该路径下面的属性全部删除 123if not attrs: target_path.pop(seg) Tips: 安全性 其实我们考虑到数据的安全性，应该在删除指定路径的全部属性时做一个判断，因为如果是忘记了输入attrs而造成了误删，那可能直接就一个P1了，所以我们可以这里将**attrs**传入一个**all**或者类似的标志，来表示确定删除指定路径下的全部属性。 2.当我们的指定路径下是一个字典并且传入的属性attrs是一个数组的时候，我们就去遍历attrs，将其元素一次从target_path下删除，这里有注意点就是我们在上面已经提到，dict.pop()必须传入字典中存在的键，所以我们在循环attrs时，需要先判断这个要删除的元素是否存在，如果不存在则使用**continue**跳过 12345678910111213if isinstance(attrs, list): for attr in attrs: if attr not in target_path[seg]: print(&quot;attr %s not in target_path&quot; % attr) continue if isinstance(target_path[seg], dict): target_path[seg].pop(attr) \\3. 当我们的指定路径下是一个数组，并且传入的属性attrs也是一个数组的时候，我们仍然通过遍历**attrs**的方式，将**attrs**中的元素依次从指定路径的数组下面移除，从数组中删除元素使用到了方法list.remove()，这个方法同样要求传入数组中已存在的元素，如果传入的元素不存在则会抛出异常。 12345678910111213if isinstance(attrs, list): for attr in attrs: if attr not in target_path[seg]: print(&quot;attr %s not in target_path&quot; % attr) continue if isinstance(target_path[seg], dict): target_path[seg].remove(attr) 查询资产信息终于到了增删改查的最后一个方法，其实查找是这四个方法中最为简单的，只需要定位到指定路径然后输出就好了，代码如下： 1234567891011121314151617181920212223def get(path): path_seg = path.split(&quot;/&quot;)[1:] data = read_file() target_path = data for idx, seg in enumerate(path_seg): if seg not in target_path: print(&quot;get path is not exists in data&quot;) return if idx == len(path_seg)-1: break target_path = target_path[seg] print(json.dumps(target_path, indent=2)) 不知道读者朋友们有没有觉得这段代码很眼熟，有没有触动你想要重构之前代码的想法。 完整重构：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259import jsonfrom os import readimport sysfrom typing import Iterabledef read_file(): with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) return datadef write_file(data): with open(&quot;data.json&quot;, &quot;w+&quot;) as f: json.dump(data, f, indent=2)def check_parse(attrs): if attrs is None: # 判断attrs的合法性 print(&quot;attributes is None&quot;) return try: attrs = json.loads(attrs) return attrs except Exception: print(&quot;attributes is not valid json string&quot;) returndef locate_path(data, path): target_path = data path_seg = path.split(&quot;/&quot;)[1:] for seg in path_seg[:-1]: if seg not in target_path: print(&quot;update path is not exists in data, please use add function&quot;) return target_path = target_path[seg] return target_path, path_seg[-1]def init(region): with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) if region in data: print(&quot;region %s already exists&quot; % region) return data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} with open(&quot;data.json&quot;, &quot;w+&quot;) as f: json.dump(data, f, indent=2) print(json.dumps(data, indent=2))def add(path, attrs=None): attrs = check_parse(attrs) if not attrs: return with open(&quot;data.json&quot;, &quot;r+&quot;) as f: data = json.load(f) target_path, last_seg = locate_path(data, path) if last_seg in target_path: print(&quot;%s already exists in %s, please use update operation&quot; % (last_seg, path)) return target_path[last_seg] = attrs with open(&quot;data.json&quot;, &quot;w+&quot;) as f: data = json.dump(data, f, indent=2) print(json.dumps(data, indent=2))def update(path, attrs): attrs = check_parse(attrs) if not attrs: return data = read_file() target_path, last_seg = locate_path(data, path) if type(attrs) != type(target_path[last_seg]): print(&quot;update attributes and target_path attributes are different type.&quot;) return if isinstance(attrs, dict): target_path[last_seg].update(attrs) elif isinstance(attrs, list): target_path[last_seg].extend(attrs) target_path[last_seg] = list(set(target_path[last_seg])) else: target_path[last_seg] = attrs write_file(data) print(json.dumps(data, indent=2))def delete(path, attrs=None): attrs = check_parse(attrs) data = read_file() target_path, last_seg = locate_path(data, path) if not attrs: target_path.pop(last_seg) if isinstance(attrs, list): for attr in attrs: if attr not in target_path[last_seg]: print(&quot;attr %s not in target_path&quot; % attr) continue if isinstance(target_path[last_seg], dict): target_path[last_seg].pop(attr) if isinstance(target_path[last_seg], list): target_path[last_seg].remove(attr) write_file(data) print(json.dumps(data, indent=2))def get(path): data = read_file() target_path, last_seg = locate_path(data, path) print(json.dumps(target_path[last_seg], indent=2))if __name__ == &quot;__main__&quot;: operations = [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] args = sys.argv if len(args) &lt; 3: print(&quot;please input operation and args&quot;) else: if args[1] == &quot;init&quot;: init(args[2]) elif args[1] == &quot;add&quot;: add(*args[2:]) elif args[1] == &quot;get&quot;: get(args[2]) elif args[1] == &quot;update&quot;: update(*args[2:]) elif args[1] == &quot;delete&quot;: delete(*args[2:]) else: print(&quot;operation must be one of get,update,delete&quot;) 总结经过我们一起不懈的努力，终于一行一行的读完了CMDBv1.0.py的源代码，理解了对资产信息增删改查的详细逻辑，并且在阅读源码的过程中逐步培养起良好的编程规范和编程思维，这对于大家以会起到至关重要的作用。 那么我们到此还没有结束，下一节我们会将CMDBv1.0利用面向对象的思想再次重构为CMDBv1.5，到时候将会是从函数式编程到面向对象编程的一个大的飞跃，敬请期待。","link":"/posts/a336c288.html"},{"title":"1.7 自动化运维新手村-Python基础-面向对象1","text":"摘要 首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 即了解Python，又了解面向对象，可以出门左转，看下一篇。 相信大家通过前几节的学习，对于Python已经有了一定的了解，并且也对CMDBv1.0的代码比较熟悉了。 我们这一节将迎来一个重头戏，这是所有学习编程的人的必经之路，那就是面向对象编程。在Python基础-2一节中，简短的提及到了面向对象这个概念，今天我们就来一探究竟。 面向过程与面向对象的区别其实不管是计算机专业的同学还是转型刚接触编程的朋友，在刚了解到面向对象这个名词时都很懵x，说实话我大学里刚学习的时候也是这样。 所以我尽量让大家少走弯路，用很通俗易懂的方式，再结合CMDB的场景为大家一次性讲解清楚什么叫面向对象。 概念\\1. 面向过程是直接将解决问题的步骤分析出来，然后用函数把步骤一步一步实现，然后再依次调用就可以了； \\2. 面向对象是将构成问题的事物，分解成若干个对象，建立对象的目的不是为了完成一个步骤，而是为了描述某个事物在解决问题过程中的行为。 上面两点是面向过程和面向对象的学术性的概念，在我看来，这两者本质的区别其实是看待问题的维度不同，并且程序员在解决问题时所扮演的角色也不同。 本质区别维度​ 1.1 面向过程要求我们从纵向看问题，将问题一步一步拆解然后去依次实现它们 ​ 1.2 面向对象要求我们从横向看问题，以整体的角度看待问题中存在的事物，这些事物就是对象 角色​ 2.1 在通过面向过程的方法编写代码的时候，我们是一个士兵，是一个执行者，做的就是实现每一个步骤 2.2 而以面向对象的方法编写代码时，我们更像一个元帅，纵观全局起到指挥作用。 结合场景\\1. CMDBv1.0就是一个十分经典的面向过程思维进行编程的例子，我们将CMDB要实现的基本功能进行了拆分，分为增删改查，然后一个一个函数去实现它们。 \\2. 而今天就是要以面向对象的思维方式去解决CMDB的这些需求。 面向对象方式看CMDB目前已知初版的CMDB主要是需要增删改查这些基本功能，那我们就对这个场景进行分析，大家可以先想一想其中涉及到哪些对象，它们分别需要具备什么属性和方法？ 资产对象整个CMDB的数据源，它被称为资产对象 属性 1.1 可执行操作：对资产对象目前可执行的操作包括新建地域以及增删改查 1.2 数据源版本：每对数据源做一次操作都需要给数据源的变更版本做一个记录 1.3 更新时间：每次对数据源中的数据做修改的时候都需要记录下更新的时间 方法​ 2.1 新建地域 ​ 2.2 增加资产信息 ​ 2.3 删除资产信息 ​ 2.4 修改资产信息 ​ 2.5 查询资产信息 存储对象数据源存储在什么地方，相当于存储介质，也是一个对象 属性 1.1 存储介质类型：当前数据源的存储介质是文本文件，之后可能还会存储在不同的数据库里 1.2 存储的名称：当前是文本文件的文件名，之后可能还会是数据库的URI连接地址 方法​ 2.1存储 ​ 2.2 读取 参数对象从哪里获取要执行的操作参数，这也是一个对象 属性方法 2.1 解析参数：目前是通过命令行传入参数去进行解析，之后可能会通过http请求或其他方式传入参数 Python 面向对象之前文章也有提到，Python本身就是一个面向对象的语言，Python中的一切变量都是对象，在Python中创建一个对象的方法也很简单，在这之前我们需要了解一些面向对象的基本概念。 但这些概念真的很多，对于刚接触面向对象的读者来说十分不友好，所以本着用不到就先不学的原则，我就先挑几个重要的给大家列一下，具体其他的概念我会在番外篇中提到。 类(Class)用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 数据成员类变量或者实例变量, 用于处理类及其实例对象的相关的数据。 实例化创建一个类的实例，类的具体对象。 方法类中定义的函数。 CMDBv1.5上面介绍了一堆思想和概念，现在结合CMDB更为深入的讲解一下关于面向对象中的一些概念 \\1. 很多读者可能还是不太理解类和对象的关系，通俗的讲类其实就是一个抽象的东西，而对象就是把这个类具象化了。那么在CMDB里如何体现呢，如下： 12345678910111213141516171819class Store: def __init__(self, store_type, store_uri): self.store_type = store_type # 存储介质类型 self.store_uri = store_uri # 存储的名称 def save(self, data): # 存储方法 pass def read(self) -&gt; dict: # 读取方法 pass 大家可以看到这里定义了一个Store类，那么这个类有什么作用呢，它其实就是一个抽象的对象，因为我们上面提到我们的存储对象包含了一些属性和方法。 也就是说不管最终这个CMDB的数据源存在哪里，存什么数据，读什么数据，我们都不关心，这个**Store**类的作用就是定义一个抽象的存储对象，告诉大家只要你是CMDB的存储对象，那你都应该长我**Store**类这个样子。 1file_store = Store(&quot;file&quot;, &quot;data.json&quot;) 上面这行代码所做的就是实例化对象的操作，通俗的讲就是我根据之前定义的抽象的Store类，创建了一个真正的可以用于我存储和读取数据源的一个对象，这个对象就是file_store，之后我就可以通过这个file_store去对我真正的数据源做一些操作。 所以本质上类和对象的关系就是抽象和具象的关系。 \\2. 大家现在大概了解了Python中类的定义和对象的创建，我们暂且先不对定义类和实例化过程中代码的细节是如何实现的。 这一节主要是让大家能够对使用面向对象的思维解决CMDB的基本需求有一个总体的认知，现在可以仿照上面Store类来定义出其他的类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class CMDB: operations = [&quot;init&quot;, &quot;add&quot;, &quot;delete&quot;, &quot;update&quot;, &quot;get&quot;] version = None update_time = None def __init__(self, data): pass def execute(self, op, args): pass def init(self, region): pass def add(self, path, attrs): pass def delete(self, path, attrs): pass def update(self, path, attrs): pass def get(self, path, attrs): pass class Params: def __init__(self): pass def parse(self, args) -&gt; list: pass 我们目前就已经定义好了三个类分别对应三个对象，并且定义好了它们应该具有的属性和方法，现在就可以去统筹的组织调用它们 1234567891011121314151617181920212223import sysif __name__ == &quot;__main__&quot;: cmd_params = Params() # 实例化从命令行获取参数的对象 op, args = cmd_params.parse(sys.argv) # 使用参数对象的解析方法解析出要做的操作和具体的参数 file_store = Store(&quot;file&quot;, &quot;data.json&quot;) # 实例化一个文件存储的存储对象 data = file_store.read() # 使用存储对象的读取操作读出数据源的内容 cmdb = CMDB(data) # 传入读出的数据源实例化一个CMDB的对象 if op not in cmdb.operation: # 判断参数对象解析出的操作是否在CMDB对象的操作属性内 print(&quot;%s cmdb operation is invalid&quot; % op) # 如果不在则打印错误提示 else: cmdb.execute(op, args) # 传入参数对象解析出的操作和具体参数，调用CMDB对象的执行操作方法 总结上面的内容主要就是对于使用面向对象的思维去解决CMDB基本需求的一个范例，大家可能心里会有很多疑问，这个面向对象看起来这么复杂繁琐，而且代码量看似也没有比面向过程的更少，反而有可能更多，为什么还要脱裤子放x去使用它呢？ 其实主要有两点： 因为面向对象在本质上是与面向过程完全不同的思维方式，当我们面对简单问题的时候可能会看起来面向过程更顺手，写起来更方便。 但一旦涉及到略微复杂的场景，就无法通过面向过程的方式去把这个复杂的问题一步一步拆开，然后去按顺序实现它，有时候也因为复杂的场景他本身就不是过程性的，所以自然也就不适合用面向过程的方式去解决。 当使用面向对象的思维去解决一个问题的时候，我们最终的方案是一个相对开放且可扩展的。 比如，CMDB中的存储对象，目前使用的是最简单的文件存储，后续可能会扩展到数据库存储（甚至扩展到不同的数据库类型）。如果不把其作为一个对象看待，将会使代码变得杂乱无章，难以维护；而如果将其看作一个对象，并且定义好了存储对象应该有的属性和方法，则可以使用继承和多态（面向对象的特性后续会提到）的方式去扩充原先使用的存储方式。 并且在一个复杂的场景中，类似需要可扩展的对象会有更多，所以面向对象的思维是在编程的路上必不可少的一个技能。 最后希望大家认真体会今天的内容，下一节我们一起来阅读CMDBv1.5完整的代码，更进一步的了解面向对象的实践，敬请期待。","link":"/posts/e40a4156.html"},{"title":"1.8 自动化运维新手村-Python基础-面向对象2","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 即了解Python，又了解CMDB的读者，可以出门左转，看下一篇。 面向对象是所有刚接触编程的朋友都会遇到的一个大坎，但这个坎如果过不去，但就这辈子只能是个脚本小子，而一旦迈过这个坎，那编程世界的大门才算正式为你敞开。 上一讲我带领大家用面向对象的思想来重新设计了CMDB v1.0，虽然面向对象中还有诸多概念，包括封装，继承和多态，但暂时先不去深入那么多，具体的细节我会另开一个番外来讲解。 对于刚接触编程的同学来说我们只要初步掌握这种思想即可，相信随着我们后续的深入学习，大家会越来越能体会到面向对象思想在程序设计上的精妙之处。 类（Class）我们已经知道面向对象是一种思想，那么在面向对象中最重要的就是类，并且在很多主流的编程语言中都存在类的概念，而Python又被称为所有变量皆对象，下面就详细探究一下Python中的类。 类是对象的模版，对象是类的实例化这句话是大家必须要最先理解的。类是一种蓝图，是同一类型对象的抽象，比如定义一个Student类 1234567891011class Student: def __init__(self, name, age): self.name = name self.age = age def study(self): print(&quot;%s is studying&quot; % self.name) 这个类具有name和age属性和study方法，那么这就表明定义的学生都抽象就是都应该具有名字和年龄，并且都会学习。 那么这时候就可以用这个抽象的类去实例化出很多学生 12345678910111213ethan = Student(&quot;ethan&quot;, 18)ethan.age = 18ethan.study() # 输出 ethan is studyingallen = Student(&quot;allen&quot;, 20)allen.age = 20allen.study() # 输出 allen is studying 上述代码就表示用学生类实例化出了两个学生对象，分别是ethan和allen，他们都具有各自的名字和年龄属性，并且都具备学习的方法。 Python类的语法定义类123class Student: pass 以Student类为例，Python中定义一个类的关键字是class，class后面跟类名然后加:，一般类名的首字母大写，这里省略了(Object)，这是属于继承的概念，我们番外篇中再讲。 实例化类1ethan = Student() 直接通过类名()就可以实例化一个类 初始化类1234567class Student: def __init__(self, name, age): self.name = name self.age = age \\1. init()是所有类都有的一个函数，它始终在实例化类时执行，可以把它看作是初始化类的方法，例如： ethan = Student()就是默认调用了Student的__init__()的方法 \\2. init()可以自己定义参数，这样在实例化类时就不能通过简单的**Student()**来操作，而必须传入相应的参数，例如： 例如ethan = Student(&quot;ethan&quot;, 18) \\3. init()第一个参数必须是**self**，它表示创建的实例本身，例如： 创建ethan时，初始化函数里就可以通过self.name = name来给创建的ethan的实例属性赋值 实例方法1234567891011class Student: def __init__(self, name, age): self.name = name self.age = age def study(self): print(&quot;%s is studying&quot; % self.name) study是Student的一个实例方法，该方法必须在实例化这个类后才能被调用，例如： 123ethan = Student(&quot;ethan&quot;, 18)ethan.study() 但假如直接调用 Student.study则会报错 \\1. 实例方法的特征为第一个参数必须为**self**，上面已经提到self表示创建的实例本身，所以可想而之必须将类实例化之后才能被调用。 2. 可以在实例方法中直接使用**self**中的属性，如上面代码中的print(&quot;%s is studying&quot; % self.name) 作用域通过上面的例子大家可以看出**self**的作用域为整个实例对象，也就是说只要将Student这个类实例化为对象后，这个**self**就会存在于整个实例对象中。 CMDB v1.5源码阅读在经过上述基本的类的语法讲解之后，就可以开始尝试着看源码了，虽然Python中还有和类相关的很多用法，但了解大概之后细节就可以边看边学。 存储类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import jsonimport osimport timeclass Store: version = None update_time = None def __init__(self, store_type, store_uri): self.store_type = store_type # 存储介质类型 self.store_uri = store_uri # 存储介质的路径 def write(self, data): # 存储方法 data[&quot;version&quot;] = (Store.version or 1) + 1 data[&quot;update_time&quot;] = time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()) with open(self.store_uri, &quot;w+&quot;) as f: json.dump(data, f, indent=2) def read(self): # 读取方法 with open(self.store_uri, &quot;r+&quot;) as f: data = json.load(f) Store.version = data.pop(&quot;version&quot;) Store.update_time = data.pop(&quot;update_time&quot;) return data if __name__ == &quot;__main__&quot;: file_path = os.path.join(os.path.dirname(__file__), &quot;data.json&quot;) file_store = Store(&quot;FILE&quot;, file_path) # 实例化一个文件存储的存取对象 上面这个类就是上一节提到的存储类，可以用它实例化出可以提供文件存取功能的存储对象（这里做了相应的优化，将数据源版本和数据更新时间属性归属到了存储类中） 先从入口函数看起 12345if __name__ == &quot;__main__&quot;: file_path = os.path.join(os.path.dirname(__file__), &quot;data.json&quot;) file_store = Store(&quot;FILE&quot;, file_path) # 实例化一个文件存取的存储对象 上述代码表示实例化了一个文件存取对象，并且在实例化时需要传入存储介质的类型以及存取的文件路径 在实例化好对象后就可以去调用对象的实例方法，如下： 12345data = file_store.read() # 调用实例对象的read方法获取CMDB的数据print(data)file_store.write(data) # 调用实例对象的write方法保存数据 参数类1234567891011121314151617181920212223242526272829class Params: def __init__(self, operations): self.operations = operations def parse(self, args): if len(args) &lt; 3: raise Exception(&quot;please input operation and args, operations: %s&quot; % &quot;,&quot;.join(self.operations)) operation = args[1] params = args[2:] return operation, params if __name__ == &quot;__main__&quot;: operations = [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;, &quot;add&quot;, &quot;init&quot;] cmd_params = Params(operations) # 实例化从命令行获取参数的对象 op, args = cmd_params.parse(sys.argv) # 使用参数对象的解析方法解析出要做的操作和具体的参数 上面代码中的类是参数类，这个类可以提供参数解析方法，解析命令行参数。 \\1. cmd_params = Params(operations)首先是实例化参数类，并传入CMDB中允许操作的方法。 \\2. op, args = cmd_params.parse(sys.argv)是调用实例对象的parse方法，判断命令行传入的操作是否合法，并且返回操作名称和该操作的params 总结相信大家看到这里应该对面向对象还有Python的类和实例不会那么发怵了，其实这就是一个从易到难的过程，我们后续会继续阅读剩余的源码，带大家更深入的了解Python的类。 篇后语读者朋友们需要注意的一个重点是，不管是数据结构这种底层原理还是面向对象这种上层思想，都是适用于所有的编程语言，并且这些通用性的知识对我们的编程起着奠基和指引作用。 所以我给大家讲解时并不是单纯的教会大家怎么写Python，而是要教给对编程感兴趣或者有需求的朋友一种在编程领域泛之四海而皆准的方式和理念。所以我一开始都会先构造一个场景，然后带领大家去感受用编程解决问题的思想，最后告诉大家用Python究竟应该如何来实现它。 希望大家可以理解，磨刀不误砍柴工，慢即是快。","link":"/posts/7d0310ec.html"},{"title":"1.9 自动化运维新手村-Python基础-面向对象3","text":"摘要首先说明，以下几类读者请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读前面几篇； 对Python了解较少只能写出简单脚本的读者，强烈建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； 即了解Python，又了解CMDB的读者，可以出门左转，看下一篇。 上一讲我们已经可以说已经摸到点儿Python中面向对象的门道了，虽然说只摸到一点儿，但这也足够支撑我们开启后面的学习， 今天我们就继续深入到CMDBv1.5的源码中。 CMDB v1.5源码阅读类的关系首先来看完整的入口函数 123456789101112131415161718192021if __name__ == &quot;__main__&quot;: try: file_path = os.path.join(os.path.dirname(__file__), &quot;data.json&quot;) file_store = Store(&quot;FILE&quot;, file_path) # 实例化一个文件存储的存储对象 cmdb = CMDB(file_store) # 传入读出的数据源实例化一个CMDB的对象 cmd_params = Params(cmdb.operations) # 实例化从命令行获取参数的对象 op, args = cmd_params.parse(sys.argv) # 使用参数对象的解析方法解析出要做的操作和具体的参数 result = cmdb.execute(op, args) # 传入参数对象解析出的操作和具体参数，调用CMDB对象的执行操作方法 print(result) except Exception as e: print(e) CMDBv1.5完整的代码中包括了三个类，分别是Params参数类，Store存取类以及CMDB类， 通过入口函数中类的实例化以及参数的传递可以看出三个对象之间的关系。 先实例化存取类负责数据的读取和保存 再实例化CMDB类，并将存取对象传入CMDB类中，负责具体操作时对数据的读写 最后实例化参数类，解析命令行参数，并将解析结果传入CMDB类的实例方法中 如下图所示： CMDB类今天的重头戏就是CMDB中最重要的类的实现，首先来看CMDB类的实例化 1cmdb = CMDB(file_store) # 传入读出的数据源实例化一个CMDB的对象 实例化的方法还是和之前讲的一样，但这里传入的参数比较特别，传入的是一个实例对象，将已经实例化过的存取对象传入到CMDB的实例中，这样可以在之后的其他操作中很方便的通过存取对象进行数据的读写。 Tips 这里用到了类之间的组合，这属于一种编程规范：在面向对象的过程中，尽量避免不必要的继承，而是要多使用对象的组合。 属性123456789101112131415161718192021222324252627class CMDB: def __init__(self, store): self.store = store self.operations = self.methods() def methods(self): ops = [] for m in dir(self): # 获取self变量的所有属性和方法 if m.startswith(&quot;__&quot;) or m.endswith(&quot;__&quot;): # 过滤掉内置属性和方法 continue if not callable(getattr(self, m)): # 过滤掉属性 continue ops.append(m) return ops CMDB类具有两个属性，一个是存取对象，另一个就是允许执行的操作，存取对象是通过__init__()函数在实例化类的时候传入的，而允许执行的操作需要我们调用自身类中的一个实例方法去获取。 \\1. dir()这个方法不知道大家是否还有印象，在之前的文章中提到过，由于Python中所有变量皆对象，可以用过dir()这个方法获取到某个变量具有的所有属性和方法，所以这里通过dir(self)来获取实例对象所有的属性和方法，并循环去进行判断。 2.在Python的类中，一般会将内置的属性前会加__，而内置的方法前后都会加__，所以在每次循环时，通过判断这个变量名是否以__开头或结尾，便可以过滤出内置的属性和方法。 \\3. callable()函数会返回一个bool值，可以判断传入的参数是否是可被调用的，如果返回True就说明传入的参数是一个函数。 \\4. getattr(obj, name)函数可以传入一个对象和一个字符串，会根据传入的name返回obj中对应的属性或方法 综上，CMDB类就包含两个属性，分别是store对象，和允许执行的operations，这两个属性都会在实例化CMDB类的时候确定。 实例方法1234567891011121314151617181920212223242526272829303132333435class CMDB: def __init__(self, store): self.store = store self.operations = self.methods() def execute(self, op, args): if op not in self.operations: raise Exception(&quot;%s is not valid CMDB operation, should is %s&quot; % (op, &quot;,&quot;.join(self.operations))) method = getattr(self, op) return method(*args) def init(self, region): data = self.store.read() if region in data: raise Exception(&quot;region %s already exists&quot; % region) data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} self.store.save(data) return region \\1. 上一讲已经提到，类的实例方法是类必须实例化之后才能被调用的，且第一个参数必须是self，CMDB的已知操作分别是init(), add(), delete(), update(), get()，这些都应该是实例方法，用法也都几乎一样。 \\2. execute(self, op, args)函数是在CMDB类中额外新增一个功能，它要求传入要执行的操作和操作所需的参数。 这个函数的功能是作为CMDB统一对外暴露的入口，执行增删改查的操作都通过这个函数来进行，主要目的是为了增加整个CMDB类的可扩展性。大家可以参照之前没有重构过的代码进行比较一下： 1234567891011121314151617181920212223if args[1] == &quot;init&quot;: init(args[2])elif args[1] == &quot;add&quot;: add(*args[2:])elif args[1] == &quot;get&quot;: get(args[2])elif args[1] == &quot;update&quot;: update(*args[2:])elif args[1] == &quot;delete&quot;: delete(*args[2:])else: print(&quot;operation must be one of get,update,delete&quot;) 同样是调用CMDB的增删改查操作，原先需要写很多的判断逻辑，并且这些函数的调用方式也都是相同的，所以完全可以把这个调用抽象成一个execute()方法，直接根据**operation**的名字去调用对应的函数即可。 再假设如果后续新增了其他对CMDB的操作，那么如果用if...else..的方式是不是每次都需要去修改代码，这其实就是扩展性不够好的体现，现在重构过之后使用execute()方法，则完全可以不用修改额外的代码，大家可以仔细体会一下。 \\3. init(self, region)这个函数是CMDB初始化地域的函数，大家注意与__init__()区分 12345678910111213def init(self, region): data = self.store.read() if region in data: raise Exception(&quot;region %s already exists&quot; % region) data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} self.store.save(data) return region 初始化地域包括另外的增删改查操作，逻辑都与重构之前的一模一样，唯一一点需要改动的地方就是，原先在面向过程编程的时候，对于数据的读写都是通过直接调用read_file()或者write_data()函数，但现在需要通过存取对象来实现。 在实例化CMDB类的时候就已经将存取类通过参数传递给了CMDB对象，所以self.store此刻就已经是存取对象，可以直接通过self.store.read()或者self.store.save()来实现数据的读写功能。 \\4. 大家可以参照init()的方法改写一下其他的增删改查操作，将原先的函数改写为实例方法 静态方法Python中除了实例方法还有静态方法和类方法，这三种方法在用法上会有所区别，但并没有明确的规定说我这个方法必须定义成实例方法或必须定义成静态方法。 定义成什么类型的方法更多的基于场景的分析，以及编程的习惯，更多关于这三种方法类型的区别会在番外篇中做出解释，因为此处只用到了静态方法，所以就暂时先讲解静态方法的语法和使用。 静态方法的语法为在方法的上面增加一行**@staticmethod**，这属于Python中的装饰器，装饰器也会单独在番外篇中详细讲解，也是属于Python中的一大特点。 除了加了一个标识为静态方法的装饰器以外，静态方法并不需要传入self参数，因为静态方法的特点就是该方法不属于任何类或者实例对象，它是静态的，它只是因为这个方法从面向对象的角度来说，可以让它放在CMDB类中，但它本身并不需要引用到任务与该类或者该类的实例相关的其他属性或方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import jsonclass CMDB: @staticmethod def check_parse(attrs): # 检查参数的合法性 if attrs is None: return try: attrs = json.loads(attrs) return attrs except Exception: raise Exception(&quot;attributes is not valid json string&quot;) @staticmethod def locate_path(data, path): # 根据path定位到data的位置 target_path = data path_seg = path.split(&quot;/&quot;)[1:] for seg in path_seg[:-1]: if seg not in target_path: print(&quot;location path is not exists in data, please use add function&quot;) return target_path = target_path[seg] return target_path, path_seg[-1] 大家可以根据之前的讲解了解到，检查参数的合法性以及根据path定位到data中相应的位置，这两个方法都是相对比较独立的功能。 并且从上面代码中也可以看出，这两个函数并不依赖任何其他第三方库或者具体的业务逻辑，这种函数我们一般称之为干净的函数。 如果后面我们需要在多处应用到相同的干净的函数，那么就可以将这些干净的函数单独写在一个文件中，供其他地方import调用，这样很多干净的函数就组成了一个工具包。 但基于目前CMDB的场景，将这两个函数归到CMDB类中也是十分合适的。 因为静态方法不需要传入self，并且它本身也与实例对象无关，它只是形式上归属于CMDB类，所以可以直接通过CMDB.check_parse()来调用，如下面的add()： 1234567891011121314151617181920212223class CMDB: def add(self, path, attrs): attrs = CMDB.check_parse(attrs) if not attrs: raise Exception(&quot;attrs is invalid json string&quot;) data = self.store.read() target_path, last_seg = CMDB.locate_path(data, path) if last_seg in target_path: raise Exception(&quot;%s already exists in %s, please use update operation&quot; % (last_seg, path)) target_path[last_seg] = attrs self.store.save(data) return attrs 总结根据上面的讲解，大家基本已经可以自己写出完整的CMDB类了，但我这里还是给大家附上重构后全部的源代码。 希望大家可以自己先根据我上面的讲解自行重构，然后再与我给出的代码做对比，这样才能发现你的思路上可以优化的地方，说不定你能重构出更优雅的代码，到时候希望我们可以互相多多交流，分享思路。 温馨提示：篇尾彩蛋 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339import jsonimport sysimport osimport timeclass Store: version = None update_time = None def __init__(self, store_type, store_uri): self.store_type = store_type # 存储介质类型 self.store_uri = store_uri # 存储介质的路径 def save(self, data): # 存储方法 data[&quot;version&quot;] = (Store.version or 1) + 1 data[&quot;update_time&quot;] = time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()) with open(self.store_uri, &quot;w+&quot;) as f: json.dump(data, f, indent=2) def read(self): # 读取方法 with open(self.store_uri, &quot;r+&quot;) as f: data = json.load(f) try: Store.version = data.pop(&quot;version&quot;) Store.update_time = data.pop(&quot;update_time&quot;) except Exception: pass return dataclass CMDB: version = None update_time = None def __init__(self, store): self.store = store self.operations = self.methods() def methods(self): ops = [] for m in dir(self): if m.startswith(&quot;__&quot;) or m.endswith(&quot;__&quot;): continue if not callable(getattr(self, m)): continue ops.append(m) return ops def execute(self, op, args): if op not in self.operations: raise Exception(&quot;%s is not valid CMDB operation, should is %s&quot; % (op, &quot;,&quot;.join(self.operations))) method = getattr(self, op) return method(*args) def init(self, region): data = self.store.read() if region in data: raise Exception(&quot;region %s already exists&quot; % region) data[region] = {&quot;idc&quot;: region, &quot;switch&quot;: {}, &quot;router&quot;: {}} self.store.save(data) return region def add(self, path, attrs): attrs = CMDB.check_parse(attrs) if attrs is None: raise Exception(&quot;attrs is invalid json string&quot;) data = self.store.read() target_path, last_seg = CMDB.locate_path(data, path) if last_seg in target_path: raise Exception(&quot;%s already exists in %s, please use update operation&quot; % (last_seg, path)) target_path[last_seg] = attrs self.store.save(data) return attrs def delete(self, path, attrs=None): attrs = CMDB.check_parse(attrs) data = self.store.read() target_path, last_seg = CMDB.locate_path(data, path) if attrs is None: if last_seg not in target_path: raise Exception(&quot;%s is not in data&quot; % path) target_path.pop(last_seg) if isinstance(attrs, list): for attr in attrs: if attr not in target_path[last_seg]: print(&quot;attr %s not in target_path&quot; % attr) continue if isinstance(target_path[last_seg], dict): target_path[last_seg].pop(attr) if isinstance(target_path[last_seg], list): target_path[last_seg].remove(attr) self.store.save(data) return attrs def update(self, path, attrs): attrs = CMDB.check_parse(attrs) if attrs is None: raise Exception(&quot;attrs is invalid json string&quot;) data = self.store.read() target_path, last_seg = CMDB.locate_path(data, path) if type(attrs) != type(target_path[last_seg]): raise Exception(&quot;update attributes and target_path attributes are different type.&quot;) if isinstance(attrs, dict): target_path[last_seg].update(attrs) elif isinstance(attrs, list): target_path[last_seg].extend(attrs) target_path[last_seg] = list(set(target_path[last_seg])) else: target_path[last_seg] = attrs self.store.save(data) return attrs def get(self, path): if &quot;/&quot; not in path: raise Exception(&quot;please input valid path&quot;) data = self.store.read() if path == &quot;/&quot;: return json.dumps(data, indent=2) try: target_path, last_seg = CMDB.locate_path(data, path) ret = target_path[last_seg] except KeyError: raise Exception(&quot;path %s is invalid&quot; % path) return json.dumps(ret, indent=2) @staticmethod def check_parse(attrs): if attrs is None: # 判断attrs的合法性 return None try: attrs = json.loads(attrs) return attrs except Exception: raise Exception(&quot;attributes is not valid json string&quot;) @staticmethod def locate_path(data, path): target_path = data path_seg = path.split(&quot;/&quot;)[1:] for seg in path_seg[:-1]: if seg not in target_path: print(&quot;location path is not exists in data, please use add function&quot;) return target_path = target_path[seg] return target_path, path_seg[-1]class Params: def __init__(self, operations) -&gt; None: self.operations = operations def parse(self, args): if len(args) &lt; 3: raise Exception(&quot;please input operation and args, operations: %s&quot; % &quot;,&quot;.join(self.operations)) operation = args[1] params = args[2:] return operation, paramsif __name__ == &quot;__main__&quot;: try: file_path = os.path.join(os.path.dirname(__file__), &quot;data.json&quot;) file_store = Store(&quot;FILE&quot;, file_path) # 实例化一个文件存储的存储对象 cmdb = CMDB(file_store) # 传入读出的数据源实例化一个CMDB的对象 cmd_params = Params(cmdb.operations) # 实例化从命令行获取参数的对象 op, args = cmd_params.parse(sys.argv) # 使用参数对象的解析方法解析出要做的操作和具体的参数 result = cmdb.execute(op, args) # 传入参数对象解析出的操作和具体参数，调用CMDB对象的执行操作方法 print(result) except Exception as e: print(e) 篇后语平时经常会有朋友问我，并且我也在知乎或者其他论坛上看到有人提问有没有好的Python的学习资料，有没有系统的学习Python的方法，其实对此我有一些自己的看法，我觉得问这样问题的朋友肯定是想要学习的，但什么是真正的学习可能鲜有人清楚 \\1. 学习本质上分为两种：记忆和泛化。 记忆顾名思义就是把一些固定的知识记住，比如一年有多少天这种知识。但学习编程语言很明显不属于记忆，它应该属于泛化的范畴。 \\2. 泛化又分为两种：指令学习和归纳学习。 - 指令学习就是提供一个知识点，比如time库中的时间格式转换函数，你这时候需要的就是把这个指令记住，然后在不停的使用这个函数转换时间的过程中去真正掌握它。 - 归纳学习则更多的是需要你根据某个知识点去触类旁通的解决更多的问题，比如我们学习了面向对象的思想，我带领大家去用这个思想实现CMDB，那你是否真的掌握了这个知识呢？这就要看你是不是可以用这样的思想去解决其他的问题，所以归纳学习会更具有难度，他需要你去不停的用新的问题来验证你是否真的学会了这个知识点。 在之前的直播答疑中有的朋友问起我需不需要刻意去背一些库函数的用法，我当时给的答案是肯定的，需要去背，因为他很明显属于指令学习的一种，但我还说了一句话：如果你能更多的去使用他，熟悉了之后你就会有一种感觉，在面对陌生的函数的时候也可以更快速的掌握，这就又是指令学习到归纳学习的转换。 所以如果一上来就希望有一本大而全的Python书籍或者资料，且不说有没有这样的书籍，倘若真的有，你拿到了也并不一定就表示你可以学会Python。 因为学习Python是一种泛化学习，需要你在获取新的知识点的过程中去用它解决实际更多的问题，不然就只能是停留在纸上谈兵的阶段。这也是为什么我的文章不愿意一点一点的去挨个讲知识点，我更愿意用从实际的场景出发，教会大家如何用具体的知识点去解决问题。 但更深一层的是虽然我是从实际的场景讲解运用知识点，但仍然是我带领大家去思考的，所以作为读者你仍然需要用去亲自实践它，这次的篇后语略微长了些，但更多的是希望告诉大家一些学习方法和理念，能对学习任何技能都起到帮助作用。","link":"/posts/a04207a.html"},{"title":"1.10 自动化运维新手村-Web框架序篇","text":"摘要首先大家需要先想清楚，为什么一定要学Web框架，有的朋友会觉得运维中最常用的应该是脚本，我只要脚本写的溜，能提高工作效率就好了。 但假设有一天你的同事也遇到了相同的场景，那你的脚本要直接拷贝给他吗？如果有一天你通过脚本解决的是一个需要跨部门合作的痛点，那你是不是也可以把执行脚本的权限交给对方呢？ 答案当然是否定的，自动化运维最大的意义是通过自动化来将从工作效率实现从量变到质变的提升，那如何提升呢，我觉得Web应用就是一个很好的方式。 Web应用很多朋友喜欢研究应用软件，比如在Windows上启动的一个应用程序，通过应用界面去点击操作，这样的应用属于C/S架构，即Client/Server架构，但有一个显著的弊端就是应用必须针对系统开发，Windows系统下的软件，在Linux下就无法使用，并且安装，变更，维护的难度都很大。 但针对自动化运维来说，我比较推荐构建Web应用，即B/S(Browser/Server)架构，服务端维护Server应用，用户只需要通过浏览器即可访问，B/S应用分布性强、维护方便、开发简单并且共享性强。 所以当想要从脚本小子更近一步的时候，学习Web应用几乎就是必经之路。B/S的完整应用如下图所示： 对于搞运维的朋友应该能很好的理解Web应用的底层原理，实际上Web应用就是在Server端监听了一个端口，然后浏览器通过Socket套接字和服务端进行信息交互。 Server端监听端口可以用Python很方便的实现，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import socketdef make_server(ip, port, app): # 处理套接字通信 sock = socket.socket() sock.bind((ip, port)) sock.listen(5) print('Starting development server at http://%s:%s/' %(ip,port)) while True: conn, addr = sock.accept() # 1、接收浏览器发来的请求信息 recv_data = conn.recv(1024) # 2、将请求信息直接转交给application处理，得到返回值 res = app(recv_data) # 3、向浏览器返回消息（此处并没有按照http协议返回） conn.send(res) conn.close()def app(environ): # 代表application # 处理业务逻辑 return b'hello world'if __name__ == '__main__': make_server('127.0.0.1', 8008, app) 在客户端浏览器输入：http://127.0.0.1:8008 会报错，这是因为简易的Server端只提供了传输层的信息交互，而浏览器与服务端通信是使用七层HTTP协议，所以服务端返回给浏览器的信息需要符合HTTP协议的标准，才能被浏览器正确的解析。 Web框架实际上开发人员可以通过自己拼接返回数据的格式使其符合HTTP协议的定义，但很多复杂且固定的东西其实完全可以抽象出来，我们只需要关注接受请求后的业务逻辑处理即可，比如只关注上例中的app。 处理消息的收发，以及对于HTTP协议报文的解析和封装属于WSGI(Web Server Gateway Interface)需要做的事情，具体关于WSGI的详细讲解会在番外篇中提到，大家暂时只需要理解WSGI的基本作用即可。 除了处理消息和解析HTTP协议之外，一个Server端还需要在存在多个业务逻辑时，定义不同的函数，将其与不同的URL关联，这就涉及到路由的映射；还有对于前端HTML的支持，数据库扩展和其他中间件的支持。 上述的诸多功能都属于业务逻辑之外的，所以我们无需完全自己实现，可以直接使用开源的Web框架。 目前Python语言中比较主流的Web框架有Flask和Django，很多朋友也都询问过我到底是学习Flask还是Django，这篇文章我就从我个人的使用感受来给大家分析一下这两个框架的优劣以及适用的场景。 Flask vs Django首先需要声明，我们只是单纯的对比两个框架的优缺点和适用的场景，并不存在捧一踩一的意思，大家根据我的介绍理智判断即可。 先大致介绍一下Django和Flask Django Django 是一个由 Python 编写的一个开放源代码的 Web 应用框架。 使用 Django，只要很少的代码，Python 的程序开发人员就可以轻松地完成一个正式网站所需要的大部分内容，并进一步开发出全功能的 Web 服务。 Django 本身基于 MVC 模型，即 Model（模型）+ View（视图）+ Controller（控制器）设计模式，MVC 模式使后续对程序的修改和扩展简化。 Flask Flask是基于Python的微框架 。 Flask是由奥地利开发商Armin Ronacher于2010年4月1日发布的。 微框架意味着Flask旨在保持其重量轻和简单但仍然可以进行高度扩展。 Flask的真正力量在于它非常灵活，具有十分活跃和丰富的社区提供各种扩展以供Flask应用进行集成。Flask本身不对设计模式做出硬性要求，开发人员十分容易上手。 下面从几个重要特征来对这两个框架做一个较为详细的对比： 上手难度 社区活跃度 灵活性 功能性 开发速度 上手难度从上手难易程度来说，我不得不说Flask要比Django好太多。 Flask只需要几行代码就可以启动一个Web应用的后端，并开始编写自己想要的路由函数。 而相比之下，刚接触编程的朋友甚至会直接卡在Django启动后端这一步，并且启动之后Django自带的诸多目录让新手常常眼花缭乱。 社区活跃度不少顶级网站使用Django和Flask，一些由Django开发的知名项目（Bitbucket，Eventbrite， Instagram的，Pinterest等）和Flask开发的知名项目（Netflix，Twilio，Uber）。 Django和Flask都属于在诸多Python的Web框架中脱颖而出的选手，它们的社区活跃度可以说不相上下，Github上会有各种各样的使用Django搭建的个人网站项目或者更为复杂的系统； 而Github上同样也有对于Flask的各种第三方扩展，几乎可以让开发人员集成任何想要的扩展功能。 灵活性我认为这两个框架最大的差异就是灵活性，没有之一。 Flask是一个扩展性很好的Web框架，可以使用各种第三方的扩展工具来灵活地开发Web应用程序。对于经验较少的开发人员，Flask可以让你简洁的实现自己想要的功能，不必受到框架本身的各种限制；对于经验丰富的开发人员可以自由地插入和使用他们喜欢的库和数据库。总的来说Flask框架很少会强制开发人员使用什么，这也是我最中意Flask的一点。 Django 可以在不使用太多第三方库和工具的条件下开发各种优秀的Web应用程序。但是，Django缺少部分对模块优化的空间，因此开发人员使用内置功能创建Web应用程序，这意味着如果想要修改Django一些默认的设定或者规则会比较困难。 功能性功能性是Flask与Django相比十分欠缺的，这块Flask不得不自认弟弟。 Django可以为Web应用程序开发提供了管理面板，数据库界面，目录结构和ORM的全方位体验，所以Django对于快速构建一个Web应用来说是不二之选。 相比，Flask的功能性并不体现在开箱即用上，Flask本质上并不缺少功能性，只是这些功能都需要开发人员自己来集成。 开发速度两者在适合框架自身的场景下都有极佳的开发速度。 Django框架可为复杂的Web应用程序提供快速的开发速度。由于它具有全部功能，提供所有必要工具。 Flask的简单性使经验丰富的开发人员可以在短时间内完成较小的应用程序。 对比图有两张十分生动的图可以描述Django和Flask之间的区别： 自动化运维使用什么？通过上述的几个因素对比，大家应该可以看出来，框架本身并没有优劣之分，只有适不适合之分。但对于自动化运维来说，从个人角度出发我更推荐使用Flask，下面同样也从上述因素来说明我推荐Flask的原因： 上手难度相信很多读者都是从事运维工作的，不管是网络运维还是系统运维或者应用运维，大多都对于编程不是特别熟悉，所以上手难度对于大家来说是最先遇到的难题，所以Flask具有最小化构建Web应用的能力，必然是刚接触编程的朋友的首选。 社区活跃度目前Django和Flask在社区活跃度上都已经足够好，并且有十分全面的文档以及各种项目供大家学习。 灵活性灵活性是我选择Flask的最重要的因素，自动化运维领域中，更多的是对于机器设备的操作，以及各种自动化脚本的调用，所以Django在使用ORM快捷地对数据库的增删改查方面的优势就难以体现，反倒由于“大而全”而显得“臃肿”。 功能性Django最引以为傲的功能性在自动化运维的体现上我个人认为也不是十分明显，因为运维领域很多数据的展示都需要做定制化处理，对于开发经验较少的朋友来说Django很多内置集成的页面也无法复用。 开发速度对于从事运维工作的朋友，如果只是想将自己的自动化脚本以API的形式提供给其他用户，那么Django就有一种“杀鸡用牛刀”的感觉。而Flask小巧灵活的特性正好十分契合这种需求。 在后续学习到更多自动化运维方面的技巧之后，可以将任务调度和数据采集分析等集成到Flask应用中，并且还可以自己灵活的设计系统的架构，这对于Django来说都会比较困难。 总结对于普通的工人来说将毛坯房装修为城市综合体会有一定的难度，但我会在今后一步一步的指引大家并且相信大家在日后的不断学习下，会慢慢的具备这样的能力，构建起自己的自动化运维系统。","link":"/posts/d3018aab.html"},{"title":"1.11 自动化运维新手村-初见Flask","text":"摘要在Web框架序篇中，主要分析了一下Flask和Django各自的适用场景，最终的结论是，更倾向于推荐大家使用Flask，所以接下来的专题内容都会围绕Flask进行展开。 当然，讲解的风格仍然是延续我们一直以来秉承的思想，那就是从场景出发，先学习迫切需要用到的那部分知识，用不到的先不学，最近还新造了一个词叫最小化上手范围（Minimize Range），就是指想要完成一个需求所需要的最小化的知识范围。 现在正式开始Flask的讲解 FlaskFlask作为一个轻量级Web框架，那就先发挥一下他”轻“的优势，我们快速与之前的CMDB专题进行结合，抛弃掉原先通过命令行对数据源进行增删改查的方式，而是通过API请求来进行操作。 启动Flask启动Flask的代码其实只需要几行（是真的几行），如下： 12345678910111213# app.pyfrom flask import Flaskapp = Flask(__name__)if __name__ == &quot;__main__&quot;: app.run() 先在环境中安装好Flask的第三方包，通常执行pip install flask即可，之后就可以在命令行输入python app.py启动Flask项目。 启动之后的输出如下： 这里的输出一共有六行，就包括了五个知识点，但这里只讲最后两个，其他在番外篇和后续的章节中会讲到（因为暂时用不到）。 Running on http://127.0.0.1:5000/这行表示Flask应用此时已经监听了本地的127.0.0.1地址和5000端口 通过app.run()函数的参数注释可以看出，该函数接受的前两个参数分别是host和port，当没有传这两个参数的时候默认值设为127.0.0.1和5000 现在已经可以通过浏览器对这个网址进行访问了，但打开浏览器输入上述网址之后页面如下： Not Found是HTTP状态码中的404错误，表示访问的链接资源不存在。这是因为我们只是启动了一个后端应用并监听了一个socket，并没有定义任何路由函数。 新增路由现在新增部分代码如下： 123456789101112131415161718192021# app.pyfrom flask import Flaskapp = Flask(__name__)@app.route(&quot;/&quot;)def index(): return &quot;hello world&quot;if __name__ == &quot;__main__&quot;: app.run(debug=True) 正常在修改代码之后需要停止程序并重新启动，但通过传入debug参数，可以在保存代码后动态加载让改动直接生效。 刷新页面后如下： 这里的@app.route(&quot;/&quot;)是Python中的装饰器语法，关于装饰器的详解，后续会在番外篇中提到，大家暂时只需要了解在一个普通的函数上面加上一行@app.route会把该函数注册到Flask的路由中。 而app.route(&quot;/&quot;)传的参数就是URL中去掉IP和端口号之后的Path路径。Flask的路由大家可以理解为一个字典，保存了Path对应的函数，这样当通过URL访问Flask后端应用的时候，就可以根据URL的Path找到对应函数去调用，然后返回数据。 结合CMDB现在，将之前的CMDB代码cmdb.py以及数据文件data.json，放在app.py同级目录下，按照第二步的方式就可以新增几个路由分别对应CDMB的增删改查功能。 首先在cmdb.py中新增一个函数，能够让Flask调用到CMDB的实例对象，如下所示 123456789101112131415# cmdb.pydef cmdb_handler(): try: file_store = Store(&quot;file&quot;, &quot;data.json&quot;) # 实例化一个文件存储的存储对象 cmdb = CMDB(file_store) # 传入读出的数据源实例化一个CMDB的对象 return cmdb except Exception as e: raise Exception(&quot;get cmdb handler failed, err: %s&quot; % str(e)) 在app.py中导入创建CMDB实例对象的函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# app.pyfrom flask import Flaskfrom cmdb import cmdb_handlerCMDB_HANDLER = cmdb_handler()app = Flask(__name__)@app.route(&quot;/&quot;)def index(): return &quot;hello world&quot; @app.route(&quot;/get&quot;)def get(): &quot;&quot;&quot;查询CMDB&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, path) return ret @app.route(&quot;/init&quot;, methods=[&quot;POST&quot;])def init(): &quot;&quot;&quot;初始化地域&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, region) return ret @app.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): &quot;&quot;&quot;添加信息&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, path, attr) return ret @app.route(&quot;/update&quot;, methods=[&quot;POST&quot;])def update(): &quot;&quot;&quot;修改信息&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, path, attr) return ret @app.route(&quot;/delete&quot;, methods=[&quot;POST&quot;])def delete(): &quot;&quot;&quot;删除信息&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, path, attr) return ret if __name__ == &quot;__main__&quot;: app.run(debug=True) 上述代码有一个需要注意的地方就是@app.route(&quot;/init&quot;, methods=[&quot;POST&quot;])，路由装饰器可以传入另外一个参数methods，这个参数会对HTTP请求的方式做出限制。 methods=[&quot;POST&quot;]表示该URL只接收POST请求，该参数不传时默认允许GET方法。 默认浏览器中输入网址访问属于对该URL发起GET请求，如果发起请求的方式与后端路由允许接收的方式不匹配，会发生如下图现象 Tips 此处需要说明，不同框架对于请求方法的限制处理方式不同。 Flask中只要请求的URL与路由定义的名称相同即可匹配成功，匹配成功后Flask会再对请求的Method做校验，判断请求方式与路由允许的接收方式是否一致，如果不一致则直接返回Method Not Allowed。 但在其他框架中，如Golang的Web框架，是将路由的名称和允许接收的请求方式组合后同时去和收到的HTTP请求做对比，如果不匹配就会报出404 Not Found错误。 也就是说 Flask是通过名称来唯一定义一个路由，而其他框架则是用名称和Method来唯一定义一个路由。 接收参数细心的朋友可能发现，上述代码中调用CMDB的功能时，需要传入一些参数，但Flask应用中并没有定义这些参数 GET请求参数CMDB查询功能的代码如下： 123456789@app.route(&quot;/get&quot;)def get(): &quot;&quot;&quot;查询CMDB&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, path) return ret 由于我们是HTTP请求方式，所以上述代码中的path参数需要从HTTP请求中获取，这里是GET请求，通常GET请求的传参方式为 1http://ip:port/path?key1=value1&amp;key2=value2 HTTP请求的参数会经过Flask框架的处理，将参数存放在request.args中，request.args是一个字典，所以代码可以修改如下： 123456789101112131415from flask import Flask, request@app.route(&quot;/get&quot;)def get(): &quot;&quot;&quot;查询CMDB&quot;&quot;&quot; path = request.args.get(&quot;path&quot;, &quot;/&quot;) # 如果没有传path参数，则path默认为 / ret = CMDB_HANDLER.execute(&quot;get&quot;, [path]) return ret POST请求参数在CMDB中新增数据的代码如下： 123456789@app.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): &quot;&quot;&quot;添加信息&quot;&quot;&quot; ret = CMDB_HANDLER.execute(&quot;get&quot;, [path, attr]) return ret 从上述代码可以看出，这个路由只允许接收POST请求。其实目前对于GET请求，或者必须要用POST请求，并没有明确的区分，或者准确的说就是没有区分，任何功能都可以通过GET请求来实现，这个我们会在番外篇中详细讲解。 但由于在CMDB中添加信息时，需要传入的内容是json字符串，放在GET请求的参数中存在转义问题和可读性问题，所以我们这里采用POST请求来处理。 POST请求可以接受的参数可以有很多种类型，类型由HTTP请求Headers中的content-type字段定义，常用的有application/json或者application/x-www-form-urlencoded，我们这里暂且采用前者，代码修改如下： 1234567891011121314151617from flask import Flask, request@app.route(&quot;/add&quot;)def add(): &quot;&quot;&quot;添加信息&quot;&quot;&quot; path = request.get_json().get(&quot;path&quot;) attr = request.get_json().get(&quot;attr&quot;) ret = CMDB_HANDLER.execute(&quot;add&quot;, [path, attr]) return ret 总结这一章节主要介绍了以下几个内容： 1.Flask的启动 2.路由函数的定义 3.HTTP参数的获取 4.Web应用与CMDB的结合 这些都还只是Flask的冰山一角，我们下一章节会针对这次的代码做出更进一步的改进，敬请期待。 篇后语大家可以看到，我们只应用了Flask的及其有限的知识，并且没有应用到特别复杂的项目架构，部分知识也没有十分深入的探索，实际上就可以完成很多需求，这就是我开头提到的最小化上手范围；这也十分符合编程的理念：不要过度设计，不要提早重构。 Flask目前有很多官方的文档，也有许多讲解十分全面的书籍。我个人也十分赞同可以系统的去学习某个技术，但平时知乎上经常有人提问我看了很多书为什么还是不会写代码，诸如此类的问题，其实这也是大多数编程经验较少的朋友都会遇到的一个问题：懂得太多。 这里的“懂”和“多”都是加引号的，因为如果真的懂，那必然是一个好事情，同样如果真的懂得多，也是个好事情。 但问题在于部分朋友对于很多知识点并不是真的懂，同样也就造成了多的错觉，因为确实也看了不少，这就涉及到了我之前在自动化运维新手村-Python基础-面向对象3 的篇后语中提到的：指令学习和归纳学习，没看过的朋友可以点链接去看看。 阅读书籍或文档中的知识讲解本质上属于指令学习，学到的是知识点，但学习编程是一种归纳学习，需要将学会的知识点不停的运用，去反复并且多角度的验证这个知识点是具有实操性的，最终才能叫做“学会它”。 所以有的朋友看完书之后就会有一种感觉，这么多知识点我都知道了，但还是不会写代码，工作中遇到需求还是无法下手，根本原因就是知识点在脑子中堆积太多，但无法消化。 所以为了避免从入门到放弃，希望大家切记慢既是快。","link":"/posts/4e4793c3.html"},{"title":"1.15 自动化运维新手村-ORM入门","text":"摘要到目前为止，自动化运维新手村中已经讲解了Python的基本数据类型及其操作，并且将其应用于一个简易的CMDB场景下； 除此之外还介绍了一个主流的Web框架——Flask，并且本着最小上手范围的原则，已经可以通过向后端API发起请求的方式，对CMDB进行增删改查，而且还有了更为健壮的异常处理和更为安全的认证鉴权。 但有一个比较重要的内容始终没有提，这也是在后续的自动化运维中一定会用到的知识，那就是数据库。 今天的章节，就对我们的后端应用进行改造，不管是用户信息还是CMDB信息，一律不再通过文件进行存储，而是使用关系型数据库来实现数据的读写。 数据库数据库其实可以笼统地理解为可以支撑大批量数据进行存取的应用程序。 数据本身又分为关系型数据和非关系型数据，比如后端应用的用户信息，都具有用户名，密码，角色等信息，所以它们都是属于结构相同的关系型数据，但现在为止，我们CMDB的数据就属于非关系型数据，因为并没有强制的规定每台设备应该具有什么字段，而是通过一个json来实现灵活的信息读写。 那与之对应的数据库也就分为关系型数据库和非关系型数据库，这两者之间存在着比较大的差异，因为它们在底层实现上需要根据关系型数据或非关系型数据来做出不同的性能优化，实现更快的数据读写性能。但这两种数据库并不存在优劣之分，只存在适用的场景不同之分。 大家经常听到的MySQL、Oracle就属于关系型数据库，而Mongo、Redis就属于非关系数据库。 我们这一章节就使用MySQL来作为数据库提供数据的存取能力，但并不会十分深入的讲解关系型数据库的底层原理以及各种范式，这些如果有必要的话会在番外篇中提到。 MySQL简介MySQL 是最流行的关系型数据库管理系统，在 WEB 应用方面 MySQL 是最好的 RDBMS(Relational Database Management System：关系数据库管理系统)应用软件之一。 RDBMS 即关系数据库管理系统(Relational Database Management System)的特点如下： 1.数据以二维表格的形式出现 2.许多的行和列组成一张表 3.若干的表单组成Database 例如我们的CMDB的数据结构如下： 1234567891011121314151617181920212223{ &quot;Beijing&quot;: { &quot;idc&quot;: &quot;Beijing&quot;, &quot;switch&quot;: { &quot;10.0.0.1&quot;: { &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;role&quot;: &quot;csw&quot;, &quot;port&quot;: [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;] } } }} 之所以一开始将数据结构定义成非关系型最主要的一个原因是为了匹配讲解Python的几大基本数据类型。但作为设备资产数据，其实大多数时候它们的字段都是已经固定的，所以这一章节就将非关系型数据“打平”，将其转为关系型数据，如下： 123456789[ [&quot;10.0.0.1&quot;, &quot;Beijing&quot;, &quot;R01&quot;, &quot;C01&quot;, &quot;Cisco&quot;, &quot;Nexus9000&quot;, &quot;BJ-R01-C01-N9K-00-00-01&quot;, &quot;CSW&quot;], [&quot;10.0.0.2&quot;, &quot;Beijing&quot;, &quot;R01&quot;, &quot;C02&quot;, &quot;Cisco&quot;, &quot;Nexus9000&quot;, &quot;BJ-R01-C01-N9K-00-00-02&quot;, &quot;CSW&quot;], [&quot;10.0.0.3&quot;, &quot;Beijing&quot;, &quot;R01&quot;, &quot;C03&quot;, &quot;Cisco&quot;, &quot;Nexus9000&quot;, &quot;BJ-R01-C01-N9K-00-00-03&quot;, &quot;CSW&quot;],] 显而易见上面的数据是一个二维数组，即二维表格，那么他按理说就是可以存在MySQL里的，但在存进去之前需要先创建一个表结构，这个表结构包含了，这一张二维表中有几列，每一列的名称，数据类型，数据长度，默认值，注释等等信息，只有定义出一个表结构后，数据才能有存放的“容器”。 表结构操作安装和连接MySQL的过程，省略不提。 其实MySQL的常用的操作并不多，无非是对表结构的增删改查（DDL），和对表数据的增删改查（DML）。 创建一个数据库的SQL语句： 1CREATE DATABASE python_ops; 使用一个数据库的SQL语句： 1USE python_ops; 创建设备表的SQL语句： 1234567891011121314151617181920212223CREATE TABLE IF NOT EXISTS `devices` ( `id` INT AUTO_INCREMENT COMMENT '自增主键', `ip` VARCHAR(16) COMMENT 'ip地址', `idc` VARCHAR(32) COMMENT '机房', `row` VARCHAR(8) COMMENT '机柜行', `column` VARCHAR(8) COMMENT '机柜列', `vendor` VARCHAR(16) COMMENT '厂商', `model` VARCHAR(16) COMMENT '型号', `hostname` VARCHAR(128) COMMENT '主机名', `role` VARCHAR(8) COMMENT '角色', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 关于表结构中的字段，有很多数据类型，较为常用的有 VARCHAR、INT、TEXT、TIMESTAMP、DATETIME、具体的使用会在用到的时候再进行具体讲解。 创建好后查看表结构的SQL语句如下： 12SHOW TABLES;DESC devices; 表数据操作通过desc查看的只是表的结构，目前devices表中仍然没有数据。 SELECT * FROM devices; 查询devices表的SQL语句 1234567INSERT INTO devices (`ip`, `idc`, `row`, `column`, `vendor`, `model`, `hostname`, `role`) VALUES (&quot;10.0.0.1&quot;, &quot;Beijing&quot;, &quot;R01&quot;, &quot;C01&quot;, &quot;Cisco&quot;, &quot;Nexus9000&quot;, &quot;BJ-R01-C01-N9K-00-00-01&quot;, &quot;CSW&quot;),(&quot;10.0.0.2&quot;, &quot;Beijing&quot;, &quot;R01&quot;, &quot;C02&quot;, &quot;Cisco&quot;, &quot;Nexus9000&quot;, &quot;BJ-R01-C01-N9K-00-00-02&quot;, &quot;CSW&quot;),(&quot;10.0.0.3&quot;, &quot;Beijing&quot;, &quot;R01&quot;, &quot;C03&quot;, &quot;Cisco&quot;, &quot;Nexus9000&quot;, &quot;BJ-R01-C01-N9K-00-00-03&quot;, &quot;CSW&quot;); 有一个需要注意的地方就是SQL语句中关于数据库名，表名，字段名在使用时最好加上反引号，这样可以避免与数据库内置的关键字冲突。 总结这一章节主要对数据库的基本概念做了一个简介，也是我们使用Web应用与数据库交互的前置条件，下一章节便会开始讲解如何使用Flask框架完成数据的增删改查。 篇后语可能部分有基础的朋友对Flask或者Django有一定的了解，这些Web框架会提供一些封装好的方法，通过代码中定义数据模型，来自动创建表结构。 但我并不推荐这种方式，我觉得单从运维的角度来说，应该有一个理念，那就是不该存在任何黑盒操作，很明显通过一条指定就直接建好表结构这就属于黑盒操作，对于一些还没有数据库基础的朋友来说，虽然短时间内可以按照流程走通，长期来看绝对是弊大于利。 一键式的操作存在的意义，一是可以让已经完全熟悉原理的人节省重复的步骤，提高效率；二是可以让完全不懂的人不必在意底层实现进行无脑操作。但对于还处在正在学习自动化运维阶段的朋友来说，一定要有一个想法就是坚决不做第二种人。 我一直提倡的理念最小上手范围，是指学习并运用有限但必要的知识来解决面对的场景，并不提倡在知识匮乏的情况下，通过一些奇技淫巧来一步登天。","link":"/posts/be12ef7b.html"},{"title":"1.17 自动化运维新手村-Flask-ORM增删改查","text":"摘要上一章节，已经是正式在我们的后端应用中引入了MySQL数据库，并使用了Flask-SQLAlchemy作为ORM框架，来更方便的对数据库进行读写，增删改查是数据库的基本操作，今天这一章节就在对原先的后端做兼容MySQL改造的同时，也对“删、改、查”做一个详细的讲解 重构模型定义上一章节在models.py中定义了Devices模型，现在将它放在app.py中，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from flask import Flask, request, jsonifyfrom flask_sqlalchemy import SQLAlchemyfrom sqlalchemy import text, DateTime, Numericapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = &quot;mysql+pymysql://root:root@127.0.0.1:3306/ops?charset=utf8&quot;app.config['SQLALCHEMY_ECHO'] = Truedb = SQLAlchemy(app)class Devices(db.Model): __tablename__ = 'devices' id = db.Column(db.Integer, primary_key=True, autoincrement=True, comment=&quot;自增主键&quot;) ip = db.Column(db.String(16), nullable=False, comment=&quot;IP地址&quot;) hostname = db.Column(db.String(128), nullable=False, comment=&quot;主机名&quot;) idc = db.Column(db.String(32), comment=&quot;机房&quot;) row = db.Column(db.String(8), comment=&quot;机柜行&quot;) column = db.Column(db.String(8), comment=&quot;机柜列&quot;) vendor = db.Column(db.String(16), comment=&quot;厂商&quot;) model = db.Column(db.String(16), comment=&quot;型号&quot;) role = db.Column(db.String(8), comment=&quot;角色&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;)if __name__ == &quot;__main__&quot;: app.run(host=&quot;127.0.0.1&quot;, port=5000, debug=True) 以上示例代码中为了突出本章节重点内容，暂时省略掉了之前的权限认证部分。 有的朋友可能会有疑问，为什么不能将Devices放在models.py中，然后在app.py中将其import进来，这是因为，目前路由函数和app变量以及db变量的定义都写在app.py中，而Devices类又需要用到db变量，这样会造成app.py和models.py文件的循环引用。 关于如何合理规划数据库模型与路由函数以及app变量的挂载，我们会在后续的Flask工厂化内容中详细讲解。 Python字典与模型转换上一章节中提到，通过模型进行添加数据记录的方法如下： 12345device = Devices(ip=&quot;10.0.0.1&quot;, hostname=&quot;BJ-R01-C01-N9K-00-00-01&quot;, idc=&quot;Beijing&quot;, row=&quot;R01&quot;, column=&quot;C01&quot;, vendor=&quot;Cisco&quot;, model=&quot;Nexus9000&quot;, role=&quot;CSW&quot;)db.session.add(device)db.session.commit() Devices是数据模型（表结构），将其实例化就可以得到一个device对象，相当于是数据库中的一行记录。 那么现在路由函数如下： 12345678910111213@app.route(&quot;/cmdb/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() device = Devices(ip=data.get(&quot;ip&quot;), hostname=data.get(&quot;hostname&quot;), idc=data.get(&quot;idc&quot;), row=data.get(&quot;row&quot;), column=data.get(&quot;column&quot;), verdor=data.get(&quot;vendor&quot;), model=data.get(&quot;model&quot;), role=data.get(&quot;role&quot;)) db.session.add(device) db.session.commit() return {&quot;status_code&quot;: HTTPStatus.OK} 上述路由函数中通过一个个指定属性值的方式来对Devices记录进行初始化，显然这样做不太优雅，我们可以在Devices类中增加一个方法来实现字典类型到Devices模型的转换，如下： 1234567891011121314151617181920212223class Devices(db.Model): __tablename__ = 'devices' ... @classmethod def to_model(cls, **kwargs): device = Devices() # 实例化一个device对象 columns = [c.name for c in cls.__table__.columns] # 获取Devices模型定义的所有列属性的名字 for k, v in kwargs.items(): # 遍历传入kwargs的键值 if k in columns: # 如果键包含在列名中，则为该device对象赋加对应的属性值 setattr(device, k, v) return device 上述代码在Devices类中新增了一个类方法，类方法的主要功能就是将kwargs转换为device对象，具体逻辑已通过注释给出。 类方法是用在该方法只与类本身有关联，而与类的实例无关的时候。 通俗的说就是类方法的第一个参数是cls，表示类本身，而实例方法的第一个参数是self表示类的实例；这里的cls和self都可以重命名，并没有强制要求。 关于类方法和实例方法这里不做过多解释，大家可以通过这一章节的示例代码仔细体会，也可以下来自行多做研究。 这里使用了一个Python中的小技巧——列表推导式，其实也可以简单的理解为就是将循环写在了一行里面，比如： 123456789columns = [c.name for c in cls.__table__.columns]# 等价于columns = []for c in cls.__table__.columns: columns.append(c.name) 对应的还有字典推导式，在后面的代码中会有示例。 在有了这个类方法后，路由函数就可以大大简化，如下： 12345678910111213@app.route(&quot;/cmdb/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() device = Devices.to_model(**data) db.session.add(device) db.session.commit() return {&quot;status_code&quot;: HTTPStatus.OK} 通过Postman发起请求结果如下图： 模型与Python字典转换既然在增加数据时需要将字典转换成ORM模型，那么在查询数据时同样也需要将ORM模型转换为字典，方便对其进行后续的操作。 新增使用Flask-SQLAlchemy进行查询所有设备记录的路由函数，代码如下： 123456789101112131415@app.route(&quot;/cmdb/get&quot;)def get(): &quot;&quot;&quot; 查询CMDB &quot;&quot;&quot; devices = Devices.query.all() res = [] for device in devices: res.append({&quot;id&quot;: device.id, &quot;ip&quot;: device.ip, &quot;hostname&quot;: device.hostname, &quot;idc&quot;: device.idc, &quot;row&quot;: device.row, &quot;column&quot;: device.column, &quot;vendor&quot;: device.vendor, &quot;model&quot;: device.model, &quot;role&quot;: device.role}) return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: res}) 在查询数据时先通过model.query获取到对数据表的操作句柄，之后再使用额外的查询条件对数据进行过滤查询，上述代码中直接使用了all()方法，来获取数据表的所有记录。 通过ORM模型查询到的记录都是模型的实例对象，而接口返回数据时，无法对实例对象做JSON序列化处理，所以需要将其手动转换为字典后再返回；但上述代码中的转换逻辑过于繁琐，可以通过在ORM模型类中增加一个方法来实现模型对象与Python字典的互相转换，代码如下： 1234567891011class Devices(db.Model): __tablename__ = 'devices' ... def to_dict(self): return {c.name: getattr(self, c.name) for c in self.__table__.columns} 上述代码中增加了一个实例方法来实现转换功能，大家可以思考一下为什么模型转字典用的是实例方法，而字典转模型用的是类方法？ 这里还用到了前文提到的字典推导式，同样字典推导式也可以理解为将字典的循环代码写到了一行，如下： 123456789{c.name: getattr(self, c.name) for c in self.__table__.columns}# 等价于res = {}for c in self.__table__.columns: res[c.name] = getattr(self, c.name) 现在有了模型到字典的转换方法之后，查询的路由函数简化如下： 123456789@app.route(&quot;/cmdb/get&quot;)def get(): &quot;&quot;&quot; 查询CMDB &quot;&quot;&quot; res = [d.to_dict() for d in Devices.query.all()] return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: res}) 通过Postman发起请求结果如下图： 从查询结果可以看出，已经可以顺利查询出所有记录，但红框处的时间却不是比较易读的时间格式，这是因为在进行模型到字典转化的过程中，并没有对时间类型做单独处理，所以现在修改Device.to_dict()方法如下： 123456789101112131415161718192021222324252627282930313233from sqlalchemy import DateTime, Numericclass Device(db.Model): ... def to_dict(self): res = {} for col in self.__table__.columns: if isinstance(col.type, DateTime): # 判断类型是否为DateTime if not getattr(self, col.name): # 判断实例中该字段是否有值 value = &quot;&quot; else: # 进行格式转换 value = getattr(self, col.name).strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) elif isinstance(col.type, Numeric): # 判断类型是否为Numeric value = float(getattr(self, col.name)) # 进行格式转换 else: # 剩余的直接取值 value = getattr(self, col.name) res[col.name] = value return res 修改后，通过Postman发起请求结果如下图： 查询分页ORM的查询有非常多的过滤方法和功能，文章中无法一一对其进行列举，只列举几个常用的供大家参考，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 查询所有设备Devices.query.all()# 查询有多少台设备Devices.query.count()# 查询第1个设备Devices.query.first()Devices.query.get(1) # 根据id查询# 查询id为4的设备[3种方式]Devices.query.get(4)Devices.query.filter_by(id=4).all() # 简单查询 使用关键字实参的形式来设置字段名Devices.query.filter(Devices.id == 4).all() # 复杂查询 使用恒等式等其他形式来设置条件# 查询主机名结尾字符为g的所有设备[开始 / 包含]Devices.query.filter(Devices.hostname.endswith(&quot;g&quot;)).all()Devices.query.filter(Devices.hostname.startswith(&quot;w&quot;)).all()Devices.query.filter(Devices.hostname.contains(&quot;n&quot;)).all()Devices.query.filter(Devices.hostname.like(&quot;%n%g&quot;)).all() # 模糊查询# 查询对应厂商和设备类型所有设备from sqlalchemy import and_Devices.query.filter(and_(Devices.model == &quot;Cisco&quot;, Devices.vendor == &quot;Nexus9000&quot;)).all()# 所有设备按id从大到小排序, 取前5个Devices.query.order_by(Devices.id.desc()).limit(5).all()# 分页查询, 查询第2页的数据, 每页10个,pn = Devices.query.paginate(page=2, per_page=10)# pn.items 获取该页的数据 pn.total 获取总共有多少条数据# 动态条件查询filters = {Devices.id &gt;=15, Device.hostname!=''} 可以在filters中添加或减少条件Devices.query.filter(*filters).all() 删除和修改删除和修改的功能相对来说较为简单，无非是在进行操作之前先通过查询条件将数据过滤出来，然后再进行删除或修改操作，具体实现大家可以看完整重构后的代码。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179import osimport timefrom http import HTTPStatusfrom flask import Flask, request, jsonifyfrom flask_sqlalchemy import SQLAlchemyfrom sqlalchemy import text, DateTime, Numericapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = &quot;mysql+pymysql://root:root@127.0.0.1:3306/ops?charset=utf8&quot;app.config['SQLALCHEMY_ECHO'] = Truedb = SQLAlchemy(app)class Devices(db.Model): __tablename__ = 'devices' id = db.Column(db.Integer, primary_key=True, autoincrement=True, comment=&quot;自增主键&quot;) ip = db.Column(db.String(16), nullable=False, comment=&quot;IP地址&quot;) hostname = db.Column(db.String(128), nullable=False, comment=&quot;主机名&quot;) idc = db.Column(db.String(32), comment=&quot;机房&quot;) row = db.Column(db.String(8), comment=&quot;机柜行&quot;) column = db.Column(db.String(8), comment=&quot;机柜列&quot;) vendor = db.Column(db.String(16), comment=&quot;厂商&quot;) model = db.Column(db.String(16), comment=&quot;型号&quot;) role = db.Column(db.String(8), comment=&quot;角色&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;) def to_dict(self): res = {} for col in self.__table__.columns: if isinstance(col.type, DateTime): if not getattr(self, col.name): value = &quot;&quot; else: value = getattr(self, col.name).strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) elif isinstance(col.type, Numeric): value = float(getattr(self, col.name)) else: value = getattr(self, col.name) res[col.name] = value return res @classmethod def to_model(cls, **kwargs): device = Devices() columns = [c.name for c in cls.__table__.columns] for k, v in kwargs.items(): if k in columns: setattr(device, k, v) return device@app.route(&quot;/cmdb/get&quot;)def get(): &quot;&quot;&quot; 查询CMDB &quot;&quot;&quot; page_size = request.args.get(&quot;pageSize&quot;, 10) page = request.args.get(&quot;page&quot;, 1) devices = Devices.query.paginate(page=page, per_page=page_size) res = [d.to_dict() for d in devices.items] return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: res, &quot;total&quot;: devices.total})@app.route(&quot;/cmdb/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() device = Devices.to_model(**data) db.session.add(device) db.session.commit() return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: device.to_dict()})@app.route(&quot;/cmdb/update&quot;, methods=[&quot;POST&quot;])def update(): data = request.get_json() Devices.query.filter_by(id=data.pop(&quot;id&quot;)).update(data) db.session.commit() return jsonify({&quot;status_code&quot;: HTTPStatus.OK})@app.route(&quot;/cmdb/delete&quot;, methods=[&quot;POST&quot;])def delete(): data = request.get_json() Devices.query.filter_by(id=data.get(&quot;id&quot;)).delete() db.session.commit() return jsonify({&quot;status_code&quot;: HTTPStatus.OK})if __name__ == &quot;__main__&quot;: app.run(host=&quot;127.0.0.1&quot;, port=5000, debug=True) 总结目前为止，已经对ORM框架有了初步的认识，并且在我们的后端应用中集成了Flask-SQLAlchemy插件，完成了资产数据增删改查的改造，重构过程中也涉及到了一些Python进阶的知识点，希望大家可以在阅读的过程中仔细体会。 这次只是一个基本的重构，关于路由函数接收的参数和异常的处理也并没有做到很完善，大家可以先自己尝试修改，在下一章节中，我会新增一个端口表，来丰富资产数据，并且讲解ORM的关联查询，同时给出更为完善的代码。","link":"/posts/f690c1a9.html"},{"title":"1.18 自动化运维新手村-Flask-ORM关联查询","text":"摘要到目前为止，Flask集成ORM扩展到基本操作，已经算是接近尾声了，上一章节已经将单表数据的增删改查，做了十分详细的讲解，并且从Flask应用的日志中可以看出每个ORM操作对应的数据库SQL语句，能够更为清晰的看到程序模型到数据库之间的映射关系，让大家可以对MySQL有一个基本的了解。 但几乎所有的后端应用都不可能只存在单独的一张数据表，大多数情况下都是存在多张数据表，并且这些数据表之间都存在关联，可能是一对一，或者一对多，等等。那么今天这一章节我们就着重讲解一下如何使用Flask-SQLAlchemy进行多表关联查询，并逐步完善后端应用的参数及异常处理。 一/一 到 多/多数据库两张表之间的关系主要存在以下几种关系： 一对一 一对多 多对多 一对一一对一是关系型数据库的两张表中较为普遍的映射关系，比如，设备信息表 - 设备详情表； 设备信息表中存储的设备基本信息包括ip, hostname, idc, row, column, vendor, model, role，那么一台设备除了具备这些基本信息外，可能还包含其他额外的信息，比如：资产号，最近一次启动时间，运行总时长，操作系统镜像版本，运行状态，过保时间，是否过保，Console口管理地址，IPv6管理地址，等等。 那么常用的做法就是将这些额外的信息单独建立一张设备详情表，一是避免原始表的数据列过多，二是基本信息和详细信息的查询频率也略有差异，并不是任何时候都需要将这些信息都查出来，所以建两张表是较为合适的做法。 如上图所示，略微调整了一下设备信息表，将部分字段放在了设备详情表中，并且在两张表中都增加sn（资产号）字段作为主键，来唯一标识一台设备，这是因为在堆叠交换机中，主备两台设备的IP是相同的，但资产号一直是可以保持唯一的。 所以将设备信息表和设备详情表通过资产号进行关联，形成一对一的关系。 一对多一对多在关系型数据库中是最为普遍的映射关系，因为一对一在不考虑过滤数据库范式的情况下，可以将其合并成一张表。 一对多比较好理解的例子就是设备信息表与设备端口表之间的关系，设备信息表中的一行数据可以表示一台设备，而一台设备可以具有多个端口，这多个端口在端口表中存储为多行，所以两张表之间就形成了一对多关系，如下： 如上图所示，设备端口表中需要有一列资产号字段，最终数据的内容中，多行端口信息的sn可能相同，这个sn就可以与设备进行关联。 多对多当两张表存在多对多关系时，通常的做法是额外新增一张中间表来进行关联，将一个多对多转换为两个多对一。 由于我们对后端应用中暂时没有多对多的场景，大家暂时只做初步的了解即可，如果十分感兴趣的朋友可以自行多做研究。 Flask关联查询定义模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879class Devices(db.Model): __tablename__ = &quot;devices&quot; sn = db.Column(db.String(128), primary_key=True, comment=&quot;资产号&quot;) ip = db.Column(db.String(16), nullable=False, comment=&quot;IP地址&quot;) hostname = db.Column(db.String(128), nullable=False, comment=&quot;主机名&quot;) idc = db.Column(db.String(32), comment=&quot;机房&quot;) vendor = db.Column(db.String(16), comment=&quot;厂商&quot;) model = db.Column(db.String(16), comment=&quot;型号&quot;) role = db.Column(db.String(8), comment=&quot;角色&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;)class DeviceDetail(db.Model): __tablename = &quot;device_detail&quot; sn = db.Column(db.String(128), db.ForeignKey(Devices.sn), primary_key=True, comment=&quot;资产号&quot;) ipv6 = db.Column(db.String(16), nullable=False, comment=&quot;IPv6地址&quot;) console_ip = db.Column(db.String(16), nullable=False, comment=&quot;console地址&quot;) row = db.Column(db.String(8), comment=&quot;机柜行&quot;) column = db.Column(db.String(8), comment=&quot;机柜列&quot;) last_start = db.Column(db.DateTime(), comment=&quot;最近启动时间&quot;) runtime = db.Column(db.Integer, comment=&quot;运行时长&quot;) image_version = db.Column(db.String(128), comment=&quot;镜像版本&quot;) over_warrant = db.Column(db.BOOLEAN, comment=&quot;是否过保&quot;) warrant_time = db.Column(db.DateTime(), comment=&quot;过保时间&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;)class Ports(db.Model): __tablename = &quot;ports&quot; sn = db.Column(db.String(128), db.ForeignKey(Devices.sn), primary_key=True, comment=&quot;资产号&quot;) port_id = db.Column(db.String(16), nullable=False, primary_key=True, comment=&quot;端口ID&quot;) port_name = db.Column(db.String(64), nullable=False, comment=&quot;端口名称&quot;) port_type = db.Column(db.String(16), comment=&quot;端口类型&quot;) bandwidth = db.Column(db.Integer, comment=&quot;端口速率&quot;) link_status = db.Column(db.String(8), comment=&quot;链路状态&quot;) admin_status = db.Column(db.String(8), comment=&quot;管理状态&quot;) interface_ip = db.Column(db.String(16), comment=&quot;端口IP&quot;) vlan_id = db.Column(db.String(8), comment=&quot;端口所属VLAN&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;) 创建数据表三张表的SQL语句如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485CREATE TABLE devices ( sn VARCHAR(128) NOT NULL COMMENT '资产号', ip VARCHAR(16) NOT NULL COMMENT 'IP地址', hostname VARCHAR(128) NOT NULL COMMENT '主机名', idc VARCHAR(32) COMMENT '机房', vendor VARCHAR(16) COMMENT '厂商', model VARCHAR(16) COMMENT '型号', `role` VARCHAR(8) COMMENT '角色', created_at DATETIME NOT NULL COMMENT '创建时间' DEFAULT CURRENT_TIMESTAMP ON_UPDATE CURRENT_TIMESTAMP, updated_at DATETIME NOT NULL COMMENT '修改时间' DEFAULT CURRENT_TIMESTAMP ON_UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (sn))CREATE TABLE device_detail ( sn VARCHAR(128) NOT NULL COMMENT '资产号', ipv6 VARCHAR(16) NOT NULL COMMENT 'IPv6地址', console_ip VARCHAR(16) NOT NULL COMMENT 'console地址', `row` VARCHAR(8) COMMENT '机柜行', `column` VARCHAR(8) COMMENT '机柜列', last_start DATETIME COMMENT '最近启动时间', runtime INTEGER COMMENT '运行时长', image_version VARCHAR(128) COMMENT '镜像版本', over_warrant BOOL COMMENT '是否过保', warrant_time DATETIME COMMENT '过保时间', created_at DATETIME NOT NULL COMMENT '创建时间' DEFAULT CURRENT_TIMESTAMP ON_UPDATE CURRENT_TIMESTAMP, updated_at DATETIME NOT NULL COMMENT '修改时间' DEFAULT CURRENT_TIMESTAMP ON_UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (sn), FOREIGN KEY(sn) REFERENCES devices (sn))CREATE TABLE ports ( sn VARCHAR(128) NOT NULL COMMENT '资产号', port_id VARCHAR(16) NOT NULL COMMENT '端口ID', port_name VARCHAR(64) NOT NULL COMMENT '端口名称', port_type VARCHAR(16) COMMENT '端口类型', bandwidth INTEGER COMMENT '端口速率', link_status VARCHAR(8) COMMENT '链路状态', admin_status VARCHAR(8) COMMENT '管理状态', interface_ip VARCHAR(16) COMMENT '端口IP', vlan_id VARCHAR(8) COMMENT '端口所属VLAN', created_at DATETIME NOT NULL COMMENT '创建时间' DEFAULT CURRENT_TIMESTAMP ON_UPDATE CURRENT_TIMESTAMP, updated_at DATETIME NOT NULL COMMENT '修改时间' DEFAULT CURRENT_TIMESTAMP ON_UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (sn, port_id), FOREIGN KEY(sn) REFERENCES devices (sn)) 一对一通常在主表中定义relationship，在附表中定义外键，如下： 1234567891011121314151617class Devices(db.Model): __tablename__ = &quot;devices&quot; ... detail = db.relationship(&quot;DeviceDetail&quot;, uselist=False, backref=&quot;device&quot;)class DeviceDetail(db.Model): __tablename = &quot;device_detail&quot; sn = db.Column(db.String(128), db.ForeignKey(Devices.sn), primary_key=True, comment=&quot;资产号&quot;) ... 上述代码中的relationship，是关联属性的意思，是SQLAlchemy提供给开发者快速引用外键模型的一个对象属性，本身并不存在于MySQL中； relationship的参数backref表示反向引用，通过外键模型查询主模型数据时的关联属性，通俗的讲就是在查DeviceDetail数据时，可以通过backref引用到Devices。 useList表示关联模型是否为List，如果为False，则不使用列表，而使用变量。一对一关系中，需要设置relationship中的uselist=Flase。 一对多通常在“一”表中定义relationship，在“多”表中定义外键 1234567class Devices(db.Model): __tablename__ = &quot;devices&quot; ... ports = db.relationship(&quot;Ports&quot;, uselist=True, backref=&quot;device&quot;, lazy='dynamic') 由于Deviecs表和Ports表之间为一对多，通过Devices会关联查询到一个或多个端口记录，所以需要将useList设为True 参数backref可以在Ports中自动创建一个device属性，作为Devices的反向引用 参数lazy决定了ORM框架何时从数据库中加载数据： lazy='subquery'，查询当前数据模型时，采用子查询(subquery)，把外键模型的属性也瞬间查询出来了。 lazy=True或lazy='select'，查询当前数据模型时，不会把外键模型的数据查询出来，只有操作到外键关联属性时，才进行连表查询数据 lazy='dynamic'，查询当前数据模型时，不会把外键模型的数据查询出来，只有操作到外键关联属性并操作外键模型具体属性时，才进行连表查询数据 Flask改造统一给三个模型都加上to_dict()和to_model()方法。 一对一获取设备详情的代码如下： 123456789101112131415161718192021222324252627@app.route(&quot;/cmdb/get_detail&quot;)def get_detail(): &quot;&quot;&quot; 查询设备详情 &quot;&quot;&quot; sn = request.args.get(&quot;sn&quot;) if not sn: return jsonify({&quot;status_code&quot;: HTTPStatus.BAD_REQUEST, &quot;message&quot;: &quot;must have sn arg&quot;}) try: device = Devices.query.filter_by(sn=sn).first() if not device: return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: {}}) res = {**device.to_dict(), **device.detail.to_dict()} # 通过device类的detail属性获取DeviceDetail的实例 return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: res}) except Exception as e: return jsonify({&quot;status_code&quot;: HTTPStatus.INTERNAL_SERVER_ERROR, &quot;message&quot;: str(e)}) 上述代码中使用到了字典的一个小技巧，将多个字典合并可以使用{**dict1, dict2} 一对一添加设备详情的代码如下： 1234567891011121314151617181920212223242526272829303132333435@app.route(&quot;/cmdb/add_devices&quot;, methods=[&quot;POST&quot;])def add_devices(): data = request.get_json() if not data: return jsonify({&quot;status_code&quot;: HTTPStatus.BAD_REQUEST, &quot;message&quot;: &quot;must have sn arg&quot;}) if not isinstance(data, list): data = [data] try: devices = [] for d in data: device = Devices.to_model(**d) # 生成Device模型实例 device.detail = DeviceDetail.to_model(**d) # 生成DeviceDetail模型实例，并赋值给device对象 devices.append(device) # 插入数据库 db.session.add_all(devices) db.session.commit() return jsonify({&quot;status_code&quot;: HTTPStatus.OK}) except Exception as e: return jsonify({&quot;status_code&quot;: HTTPStatus.INTERNAL_SERVER_ERROR, &quot;message&quot;: str(e)}) 一对多添加端口的代码如下： 1234567891011121314151617181920212223242526272829303132333435def add_ports(): data = request.get_json() if not isinstance(data, list): data = [data] sns = list(set([p.get(&quot;sn&quot;, &quot;&quot;) for p in data])) # 获取传入端口参数中的资产号，并去重 devices = Devices.query.with_entities(Devices.sn).filter(Devices.sn.in_(sns)).all() # 查询对应资产号的设备 exists_sn = [d.sn for d in devices] # 获取数据库中已存在的资产号 try: ports = [] for p in data: if p.get(&quot;sn&quot;, &quot;&quot;) not in exists_sn: # 如果端口所属的设备不存在，则返回错误 return jsonify({&quot;status_code&quot;: HTTPStatus.INTERNAL_SERVER_ERROR, &quot;message&quot;: p.get(&quot;sn&quot;, &quot;&quot;) + &quot; device is not exists&quot;}) ports.append(Ports.to_model(**p)) db.session.add_all(ports) db.session.commit() return jsonify({&quot;status_code&quot;: HTTPStatus.OK}) except Exception as e: return jsonify({&quot;status_code&quot;: HTTPStatus.INTERNAL_SERVER_ERROR, &quot;message&quot;: str(e)}) 如代码中注释，需要在添加端口前判断是否已存在该端口所属的设备，如果设备不存在则应该直接返回错误； 而实际上，即使不做这个判断，由于数据库中外键约束的存在，也会导致插入数据出错，但在接口编写时，应该遵循的原则是，将非法检查前置，避免压力集中在数据库上，这样有利于提高应用整体性能。 一对多获取设备端口的代码如下： 1234567891011121314151617181920212223242526272829@app.route(&quot;/cmdb/get_ports&quot;)def get_ports(): &quot;&quot;&quot; 查询设备端口 &quot;&quot;&quot; sn = request.args.get(&quot;sn&quot;) if not sn: return jsonify({&quot;status_code&quot;: HTTPStatus.BAD_REQUEST, &quot;message&quot;: &quot;must have sn arg&quot;}) try: device = Devices.query.filter_by(sn=sn).first() if not device: return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: {}}) ports = [p.to_dict() for p in device.ports] res = {**device.to_dict(), &quot;ports&quot;: ports} return jsonify({&quot;status_code&quot;: HTTPStatus.OK, &quot;data&quot;: res}) except Exception as e: return jsonify({&quot;status_code&quot;: HTTPStatus.INTERNAL_SERVER_ERROR, &quot;message&quot;: str(e)}) 总结这一章节我们对Flask-SQLAlchemy中关联查询的方法做了较为详细的讲解，并且从数据库层面分析了一对一，一对多等关系，除此之外还实现了一对一/多的查询和添加，其中使用到了一些较为Pythonic的语法和逻辑，需要大家慢慢消化。 最终整体的代码由于篇幅原因暂时就不放在文章中，如果有需要的朋友可以通过微信公众号加入读者交流群后获取。","link":"/posts/771bf9cc.html"},{"title":"1.12 自动化运维新手村-Flask-请求处理","text":"摘要上一章节主要给大家讲解了一下Flask的最小化上手范围，这一章节的内容仍然不会脱离这个“范围”，而是在这个“范围”中，让我们的代码变得更健壮，并且能够在生产环境中真正使用。 POST请求上一篇关于POST请求，提到的比较少，这里暂时放一张使用Postman发送POST请求的截图，大家可以模仿着使用，具体关于HTTP请求方法的讲解和关于更多GET，POST之间的对比，我们在番外篇中会详细讲解。 Flask精简路由上一章节在cmdb.py中新增了如下代码，来获取CMDB的实例化对象， 123456789101112131415# cmdb.pydef cmdb_handler(): try: file_store = Store(&quot;file&quot;, &quot;data.json&quot;) # 实例化一个文件存储的存储对象 cmdb = CMDB(file_store) # 传入读出的数据源实例化一个CMDB的对象 return cmdb except Exception as e: raise Exception(&quot;get cmdb handler failed, err: %s&quot; % str(e)) 同时新增了五个路由来分别处理对CMDB不同的操作，如下： 12345678910111213141516171819202122232425262728293031323334353637@app.route(&quot;/get&quot;)def get(): pass @app.route(&quot;/init&quot;, methods=[&quot;POST&quot;])def init(): pass @app.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): pass @app.route(&quot;/update&quot;, methods=[&quot;POST&quot;])def update(): pass @app.route(&quot;/delete&quot;, methods=[&quot;POST&quot;])def delete(): pass 但这些路由函数中都统一调用了CMDB_HANDLER.execute()这一个方法，那么是不是表明可以将这五个操作合成一个路由函数。代码如下： 12345678910111213141516171819202122232425from cmdb import cmdb_handler, HTTPParams@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])def cmdb(): &quot;&quot;&quot;操作CMDB&quot;&quot;&quot; operation = request.form.get(&quot;operation&quot;) region = request.form.get(&quot;region&quot;) path = request.form.get(&quot;path&quot;) attr = request.form.get(&quot;attr&quot;) params = HTTPParams(CMDB_HANDLER.operations) op, args = params.parse(operation=operation, path=path, attr=attr, region=region) ret = CMDB_HANDLER.execute(op, [*args]) return ret 经过重构后，抛弃掉原先使用路由PATH来区分对CMDB的操作类型，而是通过请求中新增一个参数operation来进行区分。 并且这里使用了一个新的类HTTPParams，之前我们提到过CMDB的面向对象方案中，有一个Params的类，用于处理操作CMDB的参数。 原始的Params是对命令行参数做解析，而现在就可以定义一个HTTPParams来继承Params，并重写parse函数即可，如下： 12345678910111213141516171819202122232425class HTTPParams(Params): def parse(self, **kwargs): if len(kwargs) &lt; 2: raise Exception(&quot;please input operation and args, operations: %s&quot; % &quot;,&quot;.join(self.operations)) operation = kwargs.get(&quot;operation&quot;) region = kwargs.get(&quot;region&quot;) path = kwargs.get(&quot;path&quot;) attr = kwargs.get(&quot;attr&quot;) if operation is not None and region is not None: # 处理init return operation, [region] if attr is not None: return operation, [path, attr] return operation, [path] # 处理get 这里简单提一下，一个子类继承了父类，当子类没有实现__init__()方法的时候，会默认调用父类的__init__()方法。 重写的parse方法中，对init和get操作分别做了特殊处理，以方便后续调用cmdb.execute()方法。 异常处理目前为止，Flask后端应用的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# app.pyfrom flask import Flask, requestfrom cmdb import cmdb_handler, HTTPParamsapp = Flask(__name__)CMDB_HANDLER = cmdb_handler()@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])def cmdb(): &quot;&quot;&quot;操作CMDB&quot;&quot;&quot; operation = request.form.get(&quot;operation&quot;) region = request.form.get(&quot;region&quot;) path = request.form.get(&quot;path&quot;) attr = request.form.get(&quot;attr&quot;) params = HTTPParams(CMDB_HANDLER.operations) op, args = params.parse(operation=operation, path=path, attr=attr, region=region) ret = CMDB_HANDLER.execute(op, [*args]) return retif __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, port=5000, debug=True) 终于让代码又成功减少了很多行… 在HTTPParams和CMDB中，对于异常情况都直接做了raise Exception()的处理，所以在发生异常时，这个错误会通过调用栈一直向上层抛出，这样会发生如下情况： HTTP的API请求会直接报出500 Internal Server Error，表示服务器端发生了错误，这样的情况很明显是不友好的。 因为后端程序中主动抛出的异常都属于我们可预见的异常，这时候从规范的角度讲都需要对其进行捕获，然后做出相应的处理，再通过API返回。 所以修改路由函数代码如下： 123456789101112131415161718192021222324252627@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])def cmdb(): &quot;&quot;&quot;操作CMDB&quot;&quot;&quot; operation = request.form.get(&quot;operation&quot;) region = request.form.get(&quot;region&quot;) path = request.form.get(&quot;path&quot;) attr = request.form.get(&quot;attr&quot;) try: params = HTTPParams(CMDB_HANDLER.operations) op, args = params.parse(operation=operation, path=path, attr=attr, region=region) ret = CMDB_HANDLER.execute(op, [*args]) return ret except Exception as e: return str(e) 现在将attr传入一个非法的json格式字符串，就会返回如下提示。 接口返回处理细心的朋友可能已经发现，不管是正常请求还是返回异常信息的请求，经过我们修改后Response的提示都是200 OK。 Tips Response 消息中的第一行叫做状态行，由HTTP协议版本号， 状态码， 状态消息 三部分组成。 状态码用来告诉HTTP客户端,HTTP服务器是否产生了预期的Response. HTTP/1.1中定义了5类状态码， 状态码由三位数字组成，第一个数字定义了响应的类别 1XX 提示信息 - 表示请求已被成功接收，继续处理 2XX 成功 - 表示请求已被成功接收，理解，接受 3XX 重定向 - 要完成请求必须进行更进一步的处理 4XX 客户端错误 - 请求有语法错误或请求无法实现 5XX 服务器端错误 - 服务器未能实现合法的请求 通俗的来说，状态码其实就是表示此次HTTP请求的状态结果，很明显通过API对CMDB进行操作，不管后端程序操作成功，或者操作失败，都属于此次HTTP请求成功，所以这里是200 OK合情合理。 那么既然都是200 OK，又如何来区分到底是不是对CMDB操作成功了呢，这时候就可以在返回的响应体中做文章。 目前的通用规范是在响应体中自定义状态码，接口返回数据，和提示信息来实现，格式如下： 123456789{ &quot;status_code&quot;: &quot;&quot;, &quot;data&quot;: &quot;&quot;, &quot;message&quot;: &quot;&quot;} 所以提供接口时，需要将后端程序可以预知的错误情况进行处理，然后返回相应的提示信息。 而只有当后端程序发生不可预知的异常时，这个异常才会被Flask捕获到，并返回500服务器异常。 再次修改代码如下： 123456789101112131415161718192021222324252627@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])def cmdb(): &quot;&quot;&quot;操作CMDB&quot;&quot;&quot; operation = request.form.get(&quot;operation&quot;) region = request.form.get(&quot;region&quot;) path = request.form.get(&quot;path&quot;) attr = request.form.get(&quot;attr&quot;) try: params = HTTPParams(CMDB_HANDLER.operations) op, args = params.parse(operation=operation, path=path, attr=attr, region=region) ret = CMDB_HANDLER.execute(op, [*args]) return {&quot;data&quot;: ret, &quot;status_code&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;operate cmdb success&quot;} except Exception as e: return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Fail&quot;, &quot;message&quot;: str(e)} 这时候再次发起一个请求，传入非法格式的json字符串，接口返回如下： 传入合法的参数时，接口返回如下： 总结这一章节主要讲解了三个重要的概念： 精简路由：编程规范中有一个原则是单一功能原则，就是一个函数方法只实现一个功能，那我们这里的路由函数却同时处理多个CMDB操作，是不是不太合理？显然不是，路由的定义在Web框架中是一个比较重要的部分，目前主流有两种方式，一种是分散式的，一种是集中式的，各有各的优势，我们这里对CMDB的操作就是使用了集中式的方式，主要是因为CMDB的操作属于核心方法，需要对其做统一的管理。 异常处理异常处理不管是在普通程序或者Web应用中都十分关键，一个好的异常处理能让程序说话。而在Web应用中，由于后端处理对接口调用方完全不可见，所以清晰的异常提示就显得尤为重要。所以我们要在保证后端应用不引发错误而崩溃的同时，尽量捕获到异常，并对其进行处理。 接口返回接口的返回通常应该遵守一定的规范，但本质上只要能够清晰的表达接口提供的信息和接口的请求状态即可。 篇后语这一章节可能并没有像大家所预料的新增一些Flask更高级的用法，这里我的看法是： 写代码并没有高级和低级之分，并不是用了某个工具多么高级的特性，就会怎么怎么样，而是要真正掌握某个工具的精髓，用最简单的方法实现眼下最适合的场景； 除此之外就是高级的用法是在项目达到一定的规模之后才迫切需要的，并不是说实现一个简单的功能，一上来就先搭一个很复杂的代码架构，把各种Flask的特性都用上，通俗点儿说，这就是脱裤子放x。 其实我并没有很快的把Flask的特性一一详解，有一个最重要的原因是，在这个专题真正要学的是Web框架，而Flask恰好是Web框架中符合预期的一种，所以主次关系高下立见，如果我不给大家讲解清楚Web框架中真正需要注意的地方，而是单一的教大家Flask的高级功能，那完全是舍本逐末，并且也不符合我公众号的整体风格。 我虽然不推崇所有的传统运维工程师转型自动化运维时，过多的将重心放在编程基础或者细节的追求上，但也不希望大家为了快速的上手，而舍弃掉最根本的东西，那就是发展空间。 所以我希望我的公众号不仅能够给大家提供适合当下场景的最小的上手范围，并且也能够让大家了解到更多精髓的东西，那就是编程思想和产品思维，最终达到一个低下限和高上限的最佳平衡。","link":"/posts/d3770b06.html"},{"title":"1.13 自动化运维新手村-Flask-认证","text":"摘要在Flask专题的上一章节中，主要对Web应用的路由，异常处理和接口返回做了进一步的讲解，虽然代码更健壮，但离在生产环境中使用还差了最关键的一步，那就是认证。 认证在任何存在交互的场景中都是十分重要的环节。 认证大家需要先有一个概念，那就是认证其实是两个操作： 身份认证 权限控制 通俗的说就是： 先验证用户是否合法，在Web应用中用户不合法的体现是401(Unauthorized) 再去判断用户是否具有所进行操作的权限，在Web应用中没有权限的提示是403（Forbidden）。 Flask应用身份认证AK/SK对于身份认证最简单的方式，就是给调用方一个固定的access_key和secret_key，通常也叫做AK/SK，这在系统被第三方调用的场景中是十分常见的。 代码实现也很简单，如下： 123456789101112131415@app.route(&quot;/index&quot;)def index(): ak = request.headers.get(&quot;access_key&quot;, &quot;&quot;) sk = request.headers.get(&quot;secret_key&quot;, &quot;&quot;) if ak != &quot;admin&quot; or sk != &quot;admin_secret&quot;: return &quot;认证失败&quot;, 401 # 具体的业务逻辑 pass 上述代码中，假设调用方将AK/SK放在了请求的Headers中，并且我们的后端应用只允许admin这一个用户调用，如果AK/SK不符，那就返回认证失败，如果成功则可以执行具体的业务逻辑。 相信有的朋友应该已经有了一个想法，那就是将认证的逻辑代码写在路由函数中，那岂不是每个路由函数都得写重复的认证代码？ 每个路由函数编写重复的认证逻辑这显然是不合理的，那如何进行优化呢？ 可能有的朋友又会觉得，那把认证逻辑抽象成一个单独的函数，每次调用一下不就行了，如下： 1234567891011121314151617181920212223242526272829from flask import Flask, requestdef permission(): ak = request.headers.get(&quot;access_key&quot;, &quot;&quot;) sk = request.headers.get(&quot;secret_key&quot;, &quot;&quot;) if ak == &quot;admin&quot; and sk == &quot;admin_secret&quot;: return True return False@app.route(&quot;/index&quot;)def index(): if not permission(): return &quot;认证失败&quot;, 401 # 具体的业务逻辑 pass 上述代码虽然表面上看起来精简很多，但仍然没有改变将认证逻辑与业务逻辑耦合在一起的事实。 这里大家可以转变一下思路，如果一个路由函数就代表一个业务逻辑，如果需要在执行业务逻辑前做认证，那是不是就相当于要在调用路由函数前做认证？ 这样问题的本质就变成了，在调用一个函数前，做一系列的操作，如果合法就调用该函数，如果不合法则不去调用。听起来好像就完全是装饰器的功能（如果还不了解装饰器，强烈建议先去阅读【自动化运维番外篇】- Python装饰器）。代码修改如下： 123456789101112131415161718192021222324252627282930313233343536373839from functools import wrapsfrom flask import Flask, requestapp = Flask(__name__)def permission(func): @wraps(func) def inner(): ak = request.headers.get(&quot;access_key&quot;, &quot;&quot;) sk = request.headers.get(&quot;secret_key&quot;, &quot;&quot;) if ak == &quot;admin&quot; and sk == &quot;admin_secret&quot;: return &quot;认证失败&quot;, 401 return func() return inner @app.route(&quot;/index&quot;)@permissiondef index(): # 具体的业务逻辑 pass 注册现在已经可以对固定的AK/SK进行验证了，那下一步就考虑是否可以让用户通过注册来实现自助获取AK/SK，其实相当于就是注册的功能，下面通过用户名密码的方式来实现注册的功能。 但目前后端应用还没有引入数据库，所以可以暂且通过JSON文件的方式来记录用户信息，在用户调用注册接口时，记录其传递的username/password 到JSON文件中，下次就可以通过检索文件，判断该用户是否合法。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import osimport jsonfrom functools import wrapsfrom hashlib import md5from flask import Flask, requestapp = Flask(__name__)ACCOUNTS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;accounts.json&quot;)def permission(func): @wraps(func) def inner(): username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) usernames = [account[&quot;username&quot;] for account in accounts] if username not in usernames: # 判断是否用户已存在 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;username is not exists&quot;} for account in accounts: if account[&quot;username&quot;] == username: if md5(password.encode()).hexdigest() != account[&quot;password&quot;]: # 判断用户名和密码是否一致 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Unauthorized&quot;, &quot;message&quot;: &quot;password is not correct&quot;} return func() return inner@app.route(&quot;/register&quot;, methods=[&quot;POST&quot;])def register(): &quot;&quot;&quot;注册用户信息&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not username or not password: # 判断用户输入的参数 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;InvalidParams&quot;, &quot;message&quot;: &quot;must have username and password&quot;} if not os.path.exists(ACCOUNTS_FILE): # 判断是否存在指定文件 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) for account in accounts: if account[&quot;username&quot;] == username: # 判断是否用户已存在 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Duplicated&quot;, &quot;message&quot;: &quot;username is already exists&quot;} accounts.append({&quot;username&quot;: username, &quot;password&quot;: md5(password.encode()).hexdigest()}) with open(&quot;accounts.json&quot;, &quot;w&quot;) as f: json.dump(accounts, f) return {&quot;data&quot;: username, &quot;status_code&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;register username successfully&quot;}@app.route(&quot;/index&quot;)@permissiondef index(): # 具体的业务逻辑 return &quot;success&quot;if __name__ == '__main__': app.run() 上述代码中指定了一个存放用户信息的文件accounts.json，需要先将其内容初始化，如果不对文件做初始化，那么在进行json.load()时就会抛异常，提示文件内容不是合法的json，初始化如下： 123// accounts.json 文件[] 先通过os.path.abspath(__file__)获取到当前启动文件所在的绝对路径，再通过os.path.dirname(os.path.abspath(__file__))获取到该绝对路径的目录，最后通过os.path.join()将目录与account.json文件名组合在一起，就得到了该文件的绝对路径。 这里我们的用户信息通过数组的方式进行存储，大概模型如下： 1234567[ {&quot;username&quot;: &quot;&quot;, &quot;password&quot;: &quot;&quot;}, {&quot;username&quot;: &quot;&quot;, &quot;password&quot;: &quot;&quot;}] 注册功能乍一想比较简单，但实际中需要进行的异常判断仍然不少，在注册用户的路由函数中就对多种可预见的异常进行了提前处理，并返回错误信息。并且在保存密码阶段，做了特殊的处理，因为原则上即使是应用方也无权知道用户的真实密码，所以需要在保存用户信息时，对密码做哈希处理，如下： 1accounts.append({&quot;username&quot;: username, &quot;password&quot;: md5(password.encode()).hexdigest()}) 并且在验证时同样使用密码的哈希去进行比较，如下： 1if md5(password.encode()).hexdigest() != account[&quot;password&quot;] 最终通过postman调用接口如下： 登录现在用户已经可以通过注册的方式，在后端应用保存自己的用户名密码，这样每次在请求中携带用户名密码信息就可以通过认证了。 但如果可以实现用户登录的话，用户就可以只登录一次，在登录的有效时间内，都可以进行正常的请求访问，且无需每次请求都传递用户名和密码。 所以代码需要做如下修改： 注册逻辑保持不变。 新增全局常量LOGIN_TIMEOUT，设置一个固定的登录有效期。 新增全局变量SESSION_IDS用来记录已登录用户的信息，以及用户的登录时间。 新增登录的路由函数，验证用户是否已注册，已注册的用户且用户名密码正确则登录成功，记录该用户的登录信息，并返回生成的session_id。 **5.**修改装饰器函数，获取请求头中的session_id字段，判断用户是否已登录且是否在有效期内，如果超过有效期则将该用户的登录信息从SESSION_IDS中移除，每次发起请求且认证通过后都更新登录时间戳，以延长登录有效时间 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165import osimport timeimport jsonfrom hashlib import md5from functools import wrapsfrom flask import Flask, requestapp = Flask(__name__)ACCOUNTS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;accounts.json&quot;)SESSION_IDS = {}LOGIN_TIMEOUT = 60 * 60 * 24def permission(func): @wraps(func) def inner(): session_id = request.headers.get(&quot;session_id&quot;, &quot;&quot;) global SESSION_IDS if session_id not in SESSION_IDS: # 是否存在会话信心 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;FORBIDDEN&quot;, &quot;message&quot;: &quot;username not login&quot;} if time.time() - SESSION_IDS[session_id][&quot;timestamp&quot;] &gt; LOGIN_TIMEOUT: # 是否会话仍有效 SESSION_IDS.pop(session_id) # 如果失效则移除会话信息 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;FORBIDDEN&quot;, &quot;message&quot;: &quot;username login timeout&quot;} SESSION_IDS[session_id][&quot;timestamp&quot;] = time.time() # 更新会话时间 return func() return inner@app.route(&quot;/register&quot;, methods=[&quot;POST&quot;])def register(): &quot;&quot;&quot;注册用户信息&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not username or not password: # 判断用户输入的参数 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;InvalidParams&quot;, &quot;message&quot;: &quot;must have username and password&quot;} if not os.path.exists(ACCOUNTS_FILE): # 判断是否存在指定文件 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) for account in accounts: if account[&quot;username&quot;] == username: # 判断是否用户已存在 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Duplicated&quot;, &quot;message&quot;: &quot;username is already exists&quot;} accounts.append({&quot;username&quot;: username, &quot;password&quot;: md5(password.encode()).hexdigest()}) with open(&quot;accounts.json&quot;, &quot;w&quot;) as f: json.dump(accounts, f) return {&quot;data&quot;: username, &quot;status_code&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;register username successfully&quot;}@app.route(&quot;/login&quot;, methods=[&quot;POST&quot;])def login(): &quot;&quot;&quot;用户登录&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not os.path.exists(ACCOUNTS_FILE): # 是否存在用户信息文件 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) usernames = [account[&quot;username&quot;] for account in accounts] if username not in usernames: # 是否用户已注册 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;username is not exists&quot;} current_user = None for account in accounts: if account[&quot;username&quot;] == username: current_user = account if md5(password.encode()).hexdigest() != account[&quot;password&quot;]: # 是否用户名密码正确 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Unauthorized&quot;, &quot;message&quot;: &quot;password is not correct&quot;} session_id = md5((password + str(time.time())).encode()).hexdigest() # 生成会话ID global SESSION_IDS SESSION_IDS[session_id] = {&quot;user_info&quot;: current_user, &quot;timestamp&quot;: time.time()} # 记录会话信息 return {&quot;data&quot;: {&quot;session_id&quot;: session_id}, &quot;status_code&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;login successfully&quot;}@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])@permissiondef index(): pass return &quot;success&quot;if __name__ == &quot;__main__&quot;: app.run(host=&quot;127.0.0.1&quot;, port=5000, debug=True) 通过Postman发起登录请求如下： 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165import osimport timeimport jsonfrom hashlib import md5from functools import wrapsfrom flask import Flask, requestapp = Flask(__name__)ACCOUNTS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;accounts.json&quot;)SESSION_IDS = {}LOGIN_TIMEOUT = 60 * 60 * 24def permission(func): @wraps(func) def inner(): session_id = request.headers.get(&quot;session_id&quot;, &quot;&quot;) global SESSION_IDS if session_id not in SESSION_IDS: # 是否存在会话信心 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;FORBIDDEN&quot;, &quot;message&quot;: &quot;username not login&quot;} if time.time() - SESSION_IDS[session_id][&quot;timestamp&quot;] &gt; LOGIN_TIMEOUT: # 是否会话仍有效 SESSION_IDS.pop(session_id) # 如果失效则移除会话信息 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;FORBIDDEN&quot;, &quot;message&quot;: &quot;username login timeout&quot;} SESSION_IDS[session_id][&quot;timestamp&quot;] = time.time() # 更新会话时间 return func() return inner@app.route(&quot;/register&quot;, methods=[&quot;POST&quot;])def register(): &quot;&quot;&quot;注册用户信息&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not username or not password: # 判断用户输入的参数 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;InvalidParams&quot;, &quot;message&quot;: &quot;must have username and password&quot;} if not os.path.exists(ACCOUNTS_FILE): # 判断是否存在指定文件 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) for account in accounts: if account[&quot;username&quot;] == username: # 判断是否用户已存在 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Duplicated&quot;, &quot;message&quot;: &quot;username is already exists&quot;} accounts.append({&quot;username&quot;: username, &quot;password&quot;: md5(password.encode()).hexdigest()}) with open(&quot;accounts.json&quot;, &quot;w&quot;) as f: json.dump(accounts, f) return {&quot;data&quot;: username, &quot;status_code&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;register username successfully&quot;}@app.route(&quot;/login&quot;, methods=[&quot;POST&quot;])def login(): &quot;&quot;&quot;用户登录&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not os.path.exists(ACCOUNTS_FILE): # 是否存在用户信息文件 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) usernames = [account[&quot;username&quot;] for account in accounts] if username not in usernames: # 是否用户已注册 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;NotFound&quot;, &quot;message&quot;: &quot;username is not exists&quot;} current_user = None for account in accounts: if account[&quot;username&quot;] == username: current_user = account if md5(password.encode()).hexdigest() != account[&quot;password&quot;]: # 是否用户名密码正确 return {&quot;data&quot;: None, &quot;status_code&quot;: &quot;Unauthorized&quot;, &quot;message&quot;: &quot;password is not correct&quot;} session_id = md5((password + str(time.time())).encode()).hexdigest() # 生成会话ID global SESSION_IDS SESSION_IDS[session_id] = {&quot;user_info&quot;: current_user, &quot;timestamp&quot;: time.time()} # 记录会话信息 return {&quot;data&quot;: {&quot;session_id&quot;: session_id}, &quot;status_code&quot;: &quot;OK&quot;, &quot;message&quot;: &quot;login successfully&quot;}@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])@permissiondef index(): pass return &quot;success&quot;if __name__ == &quot;__main__&quot;: app.run(host=&quot;127.0.0.1&quot;, port=5000, debug=True) 总结这一章节主要讲解了关于用户身份认证的原理和具体实现，关于权限校验下一章节再详细介绍。 其实时Flask中也有第三方插件可以实现登录，叫做flask-login，感兴趣的同学可以了解一下，但我们的目的主要是要了解身份认证的具体逻辑，而不是当一个“调包侠”。 除此之外，有一点需要大家仔细思考的是，编程是面对的计算机，所以对于逻辑的严谨和异常的处理，都需要十分仔细，平时在实现某个功能时并不是说通过正向思维，把这个功能的具体逻辑用代码翻译过来就可以了；而是要对这个功能的流程中所牵扯到的各种边界情况做充足的考虑。 最后留一个思考题，如果大家能仔细思考各种边界条件的话，会发现最终的代码存在一个并发问题： 当很多人同时发起请求进行注册的时候，是否会导致文件出现写冲突，如果会，那应用如何解决呢？","link":"/posts/9228f5b3.html"},{"title":"1.14 自动化运维新手村-Flask-权限校验","text":"摘要上一章节，我们主要对Web应用的用户认证做了详细的讲解，包括使用Flask实现用户注册，登录，并通过Session机制实现用户保持登录。那么在了解了用户认证之后，这一章节我们就着重介绍一下权限校验的原理以及实现方式。 为什么需要鉴权 用户在通过认证之后，已经可以正常访问我们的后端应用，但当后端应用越来越完善，功能越来越丰富，并且牵扯的资源以及用户的范围都足够广的时候，用户的权限校验就显得尤为重要。例如： 是否所有用户都可以通过接口获取CMDB的数据信息； 是否所有的用户都可以调用接口对设备进行操作； 是否拥有获取CMDB信息权限的用户也拥有对设备进行操作的权限； 是否拥有对设备操作权限的用户，就可以对全部的设备进行操作； 诸如上述的权限问题还有很多，下面就一起来看看如何在Flask应用中进行鉴权。 Flask实现对用户鉴权相信大家都可以理解，其实就是判断用户是否有权限访问某个API，在实现上也相对比较简单。 上一章节中，我们通过装饰器实现了登录认证，伪代码如下： 1234567891011121314151617181920212223def permission(func): @wraps(func) def inner(): if not auth(): return Fail return func() return inner @app.route(&quot;/index&quot;)@permissiondef index(): return &quot;success&quot; 登录认证的具体实现其实就是将该装饰器加在需要认证的路由函数上，这样就可以在调用该路由函数前进行一系列的认证过程。 基于用户鉴权如果想要在登录认证的前提下再进行用户鉴权，则只需要修改permission装饰器，使其可以对用户进行判断，然后决定是否允许该用户访问。但是我们需要先明确具体的路由函数允许哪个用户访问，并且将这个限制传入装饰器中，这时候就需要用到【自动化运维番外篇】Python装饰器-进阶中的带参数的装饰器，伪代码如下： 1234567891011121314151617181920212223242526272829def permission(permit_users): def login_acquired(func): @wraps(func) def inner(): if not auth() or current_user not in permit_users: return Fail return func() return inner return login_acquired @app.route(&quot;/index&quot;)@permission([&quot;ethan&quot;, &quot;john&quot;, &quot;jack&quot;])def index(): return &quot;success&quot; 经过改进的装饰器加在需要鉴权的路由函数上，并且传入该路由函数允许访问的用户列表，这样在登录认证时，通过session_id获取到当前用户，判断该用户是否在允许访问的用户列表中即可。 其实不难看出，通过用户去区分权限显然是不太现实的，用户数量增多的时候，可能会让权限控制变得十分难以维护。 那么最先想到的改进方法，应该就是将不同的用户赋予不同的角色，这样在权限控制的时候，鉴权粒度就由用户变成了角色。 基于角色鉴权关于通过角色鉴权目前业内已经有一套成熟的规范 —— RBAC（Role-Based Access Control, 基于角色的访问控制），就是用户通过角色与权限进行关联。 RBCA本质上是对用户进行分组管理，赋予角色，对权限进行合理的划分，最终实现一个用户拥有若干角色，每一个角色拥有若干权限。并且RBCA具有十分完善的权限模型设计，对于大型系统的权限管理是非常重要的，但这一章节目的就是化繁为简，学会其权限管理的基本原理和底层实现。 定义角色这里定义角色时引入了一个新的概念，叫做枚举类型，枚举类型可以看作是一种标签或是一系列常量的集合，通常用于表示某些特定的有限集合，例如星期、月份、状态等，那么我们这里的角色显然也适合用枚举类型来定义。 1234567891011from enum import Enumclass Role(Enum): ADMIN = &quot;admin&quot; CMDB = &quot;cmdb&quot; GUEST = &quot;guest&quot; 由于到目前为止，我们的后端应用还没有引入数据库的概念，所以权限信息可以和用户信息一起暂时保存在JSON文件中，这里用户的权限信息可以通过在用户信息中新增一个role字段来进行标识。模型如下： 12345678910111213[ { &quot;username&quot;: &quot;&quot;, &quot;password&quot;: &quot;&quot;, &quot;role&quot;: &quot;&quot; }] 鉴权逻辑上文中已经提到需要将装饰器修改为可传参的装饰器，修改后整体逻辑如下： 1.允许传入参数roles，可以是多个角色或单个角色，参数类型为列表，如果不传默认为None，表示不限制角色 2.判断用户是否登录的逻辑保持不变 3.根据session_id获取当前已登陆用户 4.判断该用户的角色是否包含在传入的参数roles中 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041from http import HTTPStatus # 引入了http包中的状态码def permission(roles=None): def login_required(func): @wraps(func) def inner(): session_id = request.headers.get(&quot;session_id&quot;, &quot;&quot;) global SESSION_IDS if session_id not in SESSION_IDS: # 是否存在会话信息 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.UNAUTHORIZED, &quot;message&quot;: &quot;username not login&quot;} if SESSION_IDS[session_id][&quot;timestamp&quot;] - time.time() &gt; LOGIN_TIMEOUT: # 是否会话仍有效 SESSION_IDS.pop(session_id) # 如果失效则移除会话信息 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.UNAUTHORIZED, &quot;message&quot;: &quot;username login timeout&quot;} SESSION_IDS[session_id][&quot;timestamp&quot;] = time.time() # 更新会话时间 current_user = SESSION_IDS[session_id] role_values = [role.value for role in roles] if roles is not None and current_user[&quot;user_info&quot;].get(&quot;role&quot;) not in role_values: return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.FORBIDDEN, &quot;message&quot;: &quot;user has no permission&quot;} return func() return inner return login_required 为用户授权现在新增一个为用户授权的路由函数，但这个函数同样应该设置权限，只允许管理员角色调用它，所以一开始需要在用户信息中初始化一个管理员账户，如下： 12345678910111213[ { &quot;username&quot;: &quot;yuefeiyu&quot;, &quot;password&quot;: &quot;af058879880f293b3b9b4a7072e5d0bf&quot;, &quot;role&quot;: &quot;admin&quot; }] 为用户授权的大致逻辑如下： 1.通过POST请求传入username和role表单参数 2.判断参数是否合法，role是否属于枚举类型中已定义的角色 3.获取已注册的用户信息 4.判断被授权用户是否已注册 5.修改该用户的角色信息并保存 6.如果授权用户已登陆则修改session中该用户的角色信息 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@app.route(&quot;/permission_manage&quot;, methods=[&quot;POST&quot;])@permission(roles=[Role.ADMIN])def permission_manage(): username = request.form.get(&quot;username&quot;) role = request.form.get(&quot;role&quot;) if not username or not role: return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.BAD_REQUEST} roles = [role.value for role in Role] if role not in roles: # 判断输入的角色名称是否合法 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.BAD_REQUEST} if not os.path.exists(ACCOUNTS_FILE): # 是否存在用户信息文件 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) permit_user = None for account in accounts: # 查找被授权用户 if account.get(&quot;username&quot;, &quot;&quot;) == username: permit_user = account break if permit_user is None: # 是否用户已注册 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;username is not exists&quot;} permit_user[&quot;role&quot;] = role global SESSION_IDS for _, session_info in SESSION_IDS.items(): # 如果授权用户已登陆则修改session中该用户的角色信息 if session_info[&quot;user_info&quot;].get(&quot;username&quot;) == username: session_info[&quot;user_info&quot;][&quot;role&quot;] = role with open(&quot;accounts.json&quot;, &quot;w&quot;) as f: json.dump(accounts, f, indent=2) return {&quot;data&quot;: &quot;&quot;, &quot;status_code&quot;: HTTPStatus.OK, &quot;message&quot;: &quot;successfully&quot;} 接口演示如下： 登录管理员用户 为用户jack授予guest权限 jack登录后携带jack的session_id访问cmdb接口 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267import osimport timeimport jsonfrom hashlib import md5from functools import wrapsfrom enum import Enumfrom http import HTTPStatusfrom flask import Flask, requestapp = Flask(__name__)ACCOUNTS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;accounts.json&quot;)SESSION_IDS = {}LOGIN_TIMEOUT = 60 * 60 * 24class Role(Enum): ADMIN = &quot;admin&quot; CMDB = &quot;cmdb&quot; GUEST = &quot;guest&quot;def permission(roles=None): def login_required(func): @wraps(func) def inner(): session_id = request.headers.get(&quot;session_id&quot;, &quot;&quot;) global SESSION_IDS if session_id not in SESSION_IDS: # 是否存在会话信心 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.UNAUTHORIZED, &quot;message&quot;: &quot;username not login&quot;} if SESSION_IDS[session_id][&quot;timestamp&quot;] - time.time() &gt; LOGIN_TIMEOUT: # 是否会话仍有效 SESSION_IDS.pop(session_id) # 如果失效则移除会话信息 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.UNAUTHORIZED, &quot;message&quot;: &quot;username login timeout&quot;} SESSION_IDS[session_id][&quot;timestamp&quot;] = time.time() # 更新会话时间 current_user = SESSION_IDS[session_id] role_values = [role.value for role in roles] if roles is not None and current_user[&quot;user_info&quot;].get(&quot;role&quot;) not in role_values: return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.FORBIDDEN, &quot;message&quot;: &quot;user has no permission&quot;} return func() return inner return login_required@app.route(&quot;/register&quot;, methods=[&quot;POST&quot;])def register(): &quot;&quot;&quot;注册用户信息&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not username or not password: # 判断用户输入的参数 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.BAD_REQUEST, &quot;message&quot;: &quot;must have username and password&quot;} if not os.path.exists(ACCOUNTS_FILE): # 判断是否存在指定文件 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) for account in accounts: if account[&quot;username&quot;] == username: # 判断是否用户已存在 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.CONFLICT, &quot;message&quot;: &quot;username is already exists&quot;} accounts.append({&quot;username&quot;: username, &quot;password&quot;: md5(password.encode()).hexdigest()}) with open(&quot;accounts.json&quot;, &quot;w&quot;) as f: json.dump(accounts, f, indent=2) return {&quot;data&quot;: username, &quot;status_code&quot;: HTTPStatus.OK, &quot;message&quot;: &quot;register username successfully&quot;}@app.route(&quot;/login&quot;, methods=[&quot;POST&quot;])def login(): &quot;&quot;&quot;用户登录&quot;&quot;&quot; username = request.form.get(&quot;username&quot;) password = request.form.get(&quot;password&quot;) if not username or not password: return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.BAD_REQUEST, &quot;message&quot;: &quot;invalid parameters&quot;} if not os.path.exists(ACCOUNTS_FILE): # 是否存在用户信息文件 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) current_user = None for account in accounts: if account[&quot;username&quot;] == username: current_user = account break if current_user is None: # 是否用户已注册 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;username is not exists&quot;} if md5(password.encode()).hexdigest() != current_user[&quot;password&quot;]: # 是否用户名密码正确 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.UNAUTHORIZED, &quot;message&quot;: &quot;password is not correct&quot;} global SESSION_IDS for session_id, session_info in SESSION_IDS.items(): # 判断用户是否已经登陆 if session_info[&quot;user_info&quot;].get(&quot;username&quot;) == username: # 如果已经登录则更新时间戳并返回已登陆的sessionID session_info[&quot;timestamp&quot;] = time.time() return {&quot;data&quot;: {&quot;session_id&quot;: session_id}, &quot;status_code&quot;: HTTPStatus.OK, &quot;message&quot;: &quot;login successfully&quot;} session_id = md5((password + str(time.time())).encode()).hexdigest() # 生成会话ID SESSION_IDS[session_id] = {&quot;user_info&quot;: current_user, &quot;timestamp&quot;: time.time()} # 记录会话信息 return {&quot;data&quot;: {&quot;session_id&quot;: session_id}, &quot;status_code&quot;: HTTPStatus.OK, &quot;message&quot;: &quot;login successfully&quot;}@app.route(&quot;/permission_manage&quot;, methods=[&quot;POST&quot;])@permission(roles=[Role.ADMIN])def permission_manage(): username = request.form.get(&quot;username&quot;) role = request.form.get(&quot;role&quot;) if not username or not role: return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.BAD_REQUEST} roles = [role.value for role in Role] if role not in roles: # 判断输入的角色名称是否合法 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.BAD_REQUEST} if not os.path.exists(ACCOUNTS_FILE): # 是否存在用户信息文件 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;not found accounts file&quot;} with open(&quot;accounts.json&quot;, &quot;r+&quot;) as f: accounts = json.load(f) permit_user = None for account in accounts: # 查找被授权用户 if account.get(&quot;username&quot;, &quot;&quot;) == username: permit_user = account break if permit_user is None: # 是否用户已注册 return {&quot;data&quot;: None, &quot;status_code&quot;: HTTPStatus.NOT_FOUND, &quot;message&quot;: &quot;username is not exists&quot;} permit_user[&quot;role&quot;] = role with open(&quot;accounts.json&quot;, &quot;w&quot;) as f: json.dump(accounts, f, indent=2) global SESSION_IDS for _, session_info in SESSION_IDS.items(): # 如果授权用户已登陆则修改session中该用户的角色信息 if session_info[&quot;user_info&quot;].get(&quot;username&quot;) == username: session_info[&quot;user_info&quot;][&quot;role&quot;] = role return {&quot;data&quot;: &quot;&quot;, &quot;status_code&quot;: HTTPStatus.OK, &quot;message&quot;: &quot;successfully&quot;}@app.route(&quot;/cmdb&quot;, methods=[&quot;POST&quot;])@permission(roles=[Role.CMDB])def cmdb(): pass return &quot;success&quot;if __name__ == &quot;__main__&quot;: app.run(host=&quot;127.0.0.1&quot;, port=5000, debug=True) 总结当用户范围足够广的时候，角色的定义就会变得复杂，可能会涉及到角色的继承，或者先将多个用户分到同一个用户组，然后再给这个用户组赋予一个角色，等等。除此之外除了对接口进行鉴权，有时候还需要对访问的资源进行鉴权，比如访问的数据，或者操作的设备等。 鉴权如果复杂可以做得很复杂，想要简单同样也可以很简单，我们这一章节其实就是简化鉴权逻辑，让大家能够了解到鉴权的底层原理和实现方式，希望大家可以仔细阅读体会。","link":"/posts/529a25d.html"},{"title":"1.19 自动化运维新手村-Flask-部署上线","text":"摘要在自动化运维新手村中，我们已经依次讲解了Python的基础知识，函数与面向对象设计，使用了Flask框架作为Web应用的后端，与此同时还学习了数据库的相关知识，最终完成了一个以Flask为后端的资产管理服务。 到目前为止，如果大家可以充分理解并灵活应用所讲的知识点，就已经可以按照自己的实际需求，做出一些基本的运维工具，但如果需要对外提供运维能力，还需要最后一步，那就是将Flask后端部署上线。 Flask启动在讲解Flask框架的第一章节提到，启动Flask可以直接运行如下代码： 123if __name__ == &quot;__main__&quot;: app.run() 但启动之后的日志中会包含如下提示： 123WARNING: This is a development server. Do not use it in a production deployment.Use a production WSGI server instead. 翻译过来的含义就是，当前使用的是开发模式下的服务器，请不要在生产环境使用它，而是要使用一个生成环境下的WSGI服务器 那到底什么是开发模式服务器，什么又是生产环境的WSGI服务器呢？ Web服务的组成准确来说，一个Flask后端应用，并不等同于一个完整的Web服务，一个完整的Web服务如下图所示： 需要由一个Web服务器接收浏览器发出的HTTP请求，并经由WSGI标准接口与APP进行通信，APP处理完请求之后，再将响应经由WSGI处理，最终由Web服务器发送给前端。 Flask应用就是APP的角色，而Server通常会由另一个组件来实现，当通过app.run()启动Flask应用时，其实是Flask内置了一个仅用于开发调试的低性能、简易的Server，这也是为什么不建议直接在生产环境使用app.run()来部署Flask应用（不建议并不是不能）。 WSGI那什么又是WSGI呢？ 百度百科定义如下： Web服务器网关接口（Python Web Server Gateway Interface，缩写为WSGI）是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。 上文提到，Server需要由单独的组件来充当，那么Server在与APP交互过程中，就需要遵循一种规范，这个规范就是WSGI。 更为通俗地讲，充当WebServer角色的可以有很多组件；也有很多框架可以充当WebApp的角色，但只要它们双方都遵守WSGI规范，那么编程人员就可以用任意一个WebServer组件去和任意一种WebApp对接。 WSGI区分为两个部分：一边为“服务器”或“网关”，另一边为“应用程序”或“应用框架”。在处理一个WSGI请求时，服务器会为应用程序提供环境信息及一个回调函数（Callback Function）。当应用程序完成处理请求后，透过前述的回调函数，将结果回传给服务器。 所谓的WSGI中间件同时实现了API的两方（南北向接口），因此可以在WSGI服务器和WSGI应用之间起调解作用：从Web服务器的角度来说，中间件扮演应用程序，而从应用程序的角度来说，中间件扮演服务器。“中间件”组件可以执行以下功能： 重写环境变量后，根据目标URL，将请求消息路由到不同的应用对象。 允许在一个进程中同时运行多个应用程序或应用框架。 负载均衡和远程处理，通过在网络上转发请求和响应消息。 进行内容后处理。 Flask作为应用程序，它适配实现WSGI规范的源代码大致如下： 1234567891011121314151617181920212223242526272829303132333435def wsgi_app(self, environ: dict, start_response: t.Callable) -&gt; t.Any: ctx = self.request_context(environ) error: t.Optional[BaseException] = None try: try: ctx.push() response = self.full_dispatch_request() except Exception as e: error = e response = self.handle_exception(e) except: error = sys.exc_info()[1] raise return response(environ, start_response) finally: if self.should_ignore_error(error): error = None ctx.auto_pop(error) 这一章节暂时不对源码做深入解读，Flask源码会在后续的Flask进阶内容中进行讲解，感兴趣的读者可以先自行了解。 服务器/网关目前能够充当Python HTTPServer角色的组件有很多，这一章节主要讲解Gunicorn，该服务器与各种Web框架兼容，实现非常简单，轻量级的资源消耗。可以直接用命令启动，不需要编写配置文件，相对其他的HTTP服务器要简单不少。 Gunicorn + Flask部署部署过程均在Linux环境下进行 安装12345678910111213# pip install gunicornCollecting gunicorn Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 539 kB/sRequirement already satisfied: setuptools&gt;=3.0 in ./venv/lib/python3.8/site-packages (from gunicorn) (59.7.0)Installing collected packages: gunicornSuccessfully installed gunicorn-20.1.0 如果安装较慢可以加上清华源的后缀-i https://pypi.tuna.tsinghua.edu.cn/simple 全局配置安装完gunicorn后无法直接通过命令行执行其二进制文件，如下： 123# gunicorn -h-bash: gunicorn: command not found 因为安装完成后gunicorn可执行文件会存在于python的bin/文件夹下，如果是使用的系统Python环境，则通常会存在于/usr/local/python3/bin/gunicorn，如果是使用的Python的虚拟环境，则通常会存在于虚拟环境目录./venv/bin/gunicorn。需要通过软链接将其链接到/usr/bin目录下，如下： 1# ln -s /usr/local/python/bin/gunicorn /usr/bin/gunicorn 设置完成后，执行如下命令确认： 123# gunicorn -vgunicorn (version 20.1.0) 启动Flask应用将已经完成的Flask上传到Linux环境的机器下； 先执行python app.py启动Flask应用，然后执行curl 127.0.0.1:5000/get确认应用程序可以正常运行； 执行gunicorn app:app，通过gunicorn启动Flask应用，默认会监听127.0.0.1:8000，输出如下： 12345678910111213[2022-03-27 15:53:53 +0800] [76948] [INFO] Starting gunicorn 20.1.0[2022-03-27 15:53:53 +0800] [76948] [INFO] Listening at: http://127.0.0.1:8000 (76948)[2022-03-27 15:53:53 +0800] [76948] [INFO] Using worker: sync[2022-03-27 15:53:53 +0800] [76954] [INFO] Booting worker with pid: 76954[2022-03-27 15:53:55 +0800] [76948] [INFO] Handling signal: int[2022-03-27 15:53:55 +0800] [76954] [INFO] Worker exiting (pid: 76954)[2022-03-27 15:53:55 +0800] [76948] [INFO] Shutting down: Master 这时再次执行curl 127.0.0.1:8000/get确认应用可以正常访问 基本配置gunicorn可以配置一些额外参数，格式如下： 1gunicorn -w 进程数量 -b 监听地址:监听端口 运行文件名称:Flask程序实例名 例如： 1gunicorn -w 4 -b 0.0.0.0:8080 app:app -D -D表示将gunicorn置于后台运行 执行ps -ef | grep gunicorn可以查看gunicorn进程信息 1234567891011# ps -ef | grep gunicorn 501 79794 1 0 3:58PM ?? 0:00.04 PycharmProjects/flaskProject2/venv/bin/gunicorn -w 4 -b 0.0.0.0:8080 openapi:app -D 501 79806 79794 0 3:58PM ?? 0:00.51 PycharmProjects/flaskProject2/venv/bin/gunicorn -w 4 -b 0.0.0.0:8080 openapi:app -D 501 79808 79794 0 3:58PM ?? 0:00.51 PycharmProjects/flaskProject2/venv/bin/gunicorn -w 4 -b 0.0.0.0:8080 openapi:app -D 501 79809 79794 0 3:58PM ?? 0:00.50 PycharmProjects/flaskProject2/venv/bin/gunicorn -w 4 -b 0.0.0.0:8080 openapi:app -D 501 79810 79794 0 3:58PM ?? 0:00.50 PycharmProjects/flaskProject2/venv/bin/gunicorn -w 4 -b 0.0.0.0:8080 openapi:app -D 除此之外如果是在生产环境，必不可少还需要配置日志信息，如下： 1gunicorn -w 4 -b 0.0.0.0:8080 --access-logfile access.log --error-logfile error.log app:app -D 可以通过tail -f access.log或者tail -f error.log查看记录的日志信息。 通用配置执行pip install gevent安装依赖包。 gunicorn可以通过执行配置文件来完成启动，配置文件如下： 12345678910111213141516171819202122232425262728293031# gun.pyimport multiprocessingimport gevent.monkeygevent.monkey.patch_all()bind = '0.0.0.0:8080' # 绑定的ip以及端口号chdir = '/home/flaskProject' # gunicorn要切换到的目的工作目录timeout = 60 # 超时worker_class = 'gevent' # 使用gevent模式，还可以使用sync 模式，默认的是sync模式workers = multiprocessing.cpu_count() * 2 + 1 # 启动的进程数loglevel = &quot;info&quot; # 日志级别，这个日志级别指的是错误日志的级别，而访问日志的级别无法设置access_log_format = '%(t)s %(p)s %(h)s &quot;%(r)s&quot; %(s)s %(L)s %(b)s %(f)s&quot; &quot;%(a)s&quot;' # 设置gunicorn访问日志格式，错误日志无法设置pidfile = &quot;gunicorn.pid&quot;accesslog = &quot;access.log&quot;errorlog = &quot;error.log&quot;daemon = True # 是否后台运行 执行gunicorn -c gun.py app:app启动应用程序， 启动后项目的目录下会生成access.log，error.log和gunicorn.pid三个文件，gunicorn.pid中保存了gunicorn的主进程PID号，可以通过cat gunicorn.pid查看，当想要停止gunicorn时，直接kill 进程号即可杀死所有gunicorn进程。 总结单纯的Flask 自带的Web服务器做下测试，在压力大的时候出现socket的问题，因为他是单进程单线程的。而使用gunicorn来启动，响应速度和能力提升显著。配置中workers指定启动的进程数。cpu的损耗是平均到各个进程。workers的值一定不要过大，毕竟多进程对于系统的调度消耗比较大。 这一章节的结束，就标志着自动化运维新手村系列正式完结了，希望大家可以通过这一系列的学习，都可以快速上手Python，并结合自己的场景完成特定的自动化运维小工具，敬请期待自动化运维初级村的更多内容。","link":"/posts/44bbbe77.html"},{"title":"1.1 自动化运维新手村-Python基础-1","text":"摘要首先说明，以下几类读者朋友们请自行对号入座： 对CMDB很了解但对于Python还没有上手的读者，强烈建议阅读此篇； 了解过Python基本的数据结构，但又没有经常在实践中运用的读者，建议阅读此篇； 已经可以熟练写出Python脚本，但对CMDB不是很了解的读者，建议阅读此篇； CMDB简介简单赘述以下，CMDB的英文全称是Configuration Management Database，中文名叫配置管理数据库，它几乎贯穿了运维的每个环节。在实际的项目中，CMDB常常被认为是构建其它ITIL(Information Technology Infrastructure Library，IT基础架构库)流程的基础，ITIL项目的成败与是否成功建立CMDB有非常大的关系。 对于一些中大型的互联网公司必然都有自研的CMDB系统，而一些初创公司可能采用开源的CMDB工具或者部分运维工程师日常使用Excel表格充当简易的CMDB功能，我们的目的并不是让大家去构建一个新的CMDB去推翻原有系统，也不是说用Excel表格就不如用Python来的高级，而是能让大家从CMDB自顶向下的拆解，来更生动的体会到Python基础数据结构的运用 CMDBv1.0实现完整的CMDB十分复杂，但作为讲解Python基本数据结构的范例，我们先实现一个简易的1.0版本。 CMDBv1.0只需要通过一个Python脚本就可以做到资产数据的增删改查。 可能会有一些有CMDB使用经验的朋友会有质疑，觉得这也叫CMDB，还请大家稍安勿躁，任何大型系统的构建都是经过日积月累的迭代，但我可以保证，在项目冷启动阶段，这样一个稍微简陋的1.0版本，就可以起到基本的资产管理的作用。 下面假设我们已经有了一个Python脚本，名叫 cmdb-v1.0.py ，我们简单的演示以下它的操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141root&gt; # python3 cmdb-v1.0.py init beijing // 初始化beijing IDC{ &quot;beijing&quot;: { &quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: {}, &quot;router&quot;: {} }}root&gt; # python3 cmdb-v1.0.py add /beijing/switch // 添加beijing IDC的IP地址是10.0.0.1的交换机信息{ &quot;beijing&quot;: { &quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: { &quot;10.0.0.1&quot;: { &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-00-01&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;asw&quot;, &quot;port&quot;: [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;], &quot;stack&quot;: true } } }, &quot;router&quot;: {}}root&gt; # python3 cmdb-v1.0.py get /beijing/switch/10.0.0.1 // 读取beijing IDC的IP地址是10.0.0.1的交换机信息{ &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-00-01&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;asw&quot;, &quot;port&quot;: [ &quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot; ], &quot;stack&quot;: true}root&gt; # python .\\cmdb-v1.0.py update /beijing/switch/10.0.0.1/hostname '\\&quot;test\\&quot;' // 将 beijing IDC的IP地址是10.0.0.1的交换机主机名修改为 testroot&gt; # python .\\cmdb-v1.0.py get /beijing/switch/10.0.0.1 // 读取验证相关信息{ &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;test&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;asw&quot;, &quot;port&quot;: [ &quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot; ], &quot;stack&quot;: true}root&gt; # python .\\cmdb-v1.0.py delete /beijing/switch/10.0.0.1/role // 删除beijing IDC的IP地址是10.0.0.1的交换机的角色属性root&gt; # python .\\cmdb-v1.0.py delete /beijing/switch/10.0.0.1/port '[\\&quot;Eth1/1/0\\&quot;]' // 删除beijing IDC的IP地址是10.0.0.1的交换机端口属性中的 Eth1/1/0root&gt; # python .\\cmdb-v1.0.py get /beijing/switch/10.0.0.1 // 读取验证相关信息{ &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;test&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;port&quot;: [ &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot; ], &quot;stack&quot;: true} 上面演示的几个步骤包括了地域的初始化，资产信息的增删改查，大家可以发现整个1.0版本中，数据源的结构是比较清晰的，几乎涉及到了Python中最常用的数据类型，以及不同数据类型的常用操作，所以这也是我想以CMDB为例切入Python的原因。 Python 从笔者个人经历来说，写过Python，Java，Golang，至今仍然觉得Python是一门十分优秀的语言，能够持续霸占最热门语言的前三甲，确实有其独到之处 Python优缺点 优点 简单 免费、开源 高层语言面向对象 可扩展性 丰富的库 缺点 性能，虽然有一部分网友还是对Python颇有微词，但如果非要从Python的众多缺点中挑一个最重要的一点的话，那就是性能问题，但性能问题绝对不是我们弃用Python的原因，目前仍然有诸多方法可以保证Python支持企业级应用平稳运行迭代，而且就连字节如此大体量的公司很多地方都仍然使用Python进行开发 CMDB拆解及Python基本数据类型CMDB 拆解根据上面的演示大家应该已经大概了解CMDBv1.0版本的数据源大概长什么样子了，它的层级的划分其实是根据每个公司不同的实际场景决定的，我们这里就暂且先考虑普适情况，即idc为顶层，其包含了switch和router，然后再包含具体的设备信息和属性，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161{ &quot;beijing&quot;: { &quot;idc&quot;: &quot;beijing&quot;, &quot;switch&quot;: { &quot;10.0.0.1&quot;: { &quot;ip&quot;: &quot;10.0.0.1&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-00-01&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;asw&quot;, &quot;port&quot;: [ &quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot; ], &quot;stack&quot;: true }, &quot;10.0.0.2&quot;: { &quot;ip&quot;: &quot;10.0.0.2&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-00-02&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;dsw&quot;, &quot;port&quot;: [ &quot;GEth1/1/0&quot;, &quot;GEth1/1/1&quot;, &quot;GEth1/1/2&quot; ], &quot;stack&quot;: true } }, &quot;router&quot;: { &quot;10.0.0.3&quot;: { &quot;ip&quot;: &quot;10.0.0.3&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-00-01&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;br&quot;, &quot;port&quot;: [ &quot;TGEth1/0/0/1&quot;, &quot;TGEth1/0/0/2&quot;, &quot;TGEth1/0/0/3&quot; ], &quot;bgp_as&quot;: 64512 } } }, &quot;shanghai&quot;: { &quot;idc&quot;: &quot;shanghai&quot;, &quot;switch&quot;: { &quot;10.0.1.1&quot;: { &quot;ip&quot;: &quot;10.0.1.1&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-01-01&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;asw&quot;, &quot;port&quot;: [ &quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot; ], &quot;stack&quot;: false } }, &quot;router&quot;: { &quot;10.0.1.3&quot;: { &quot;ip&quot;: &quot;10.0.1.3&quot;, &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-01-01&quot;, &quot;hardware&quot;: &quot;nexus9500&quot;, &quot;role&quot;: &quot;br&quot;, &quot;port&quot;: [ &quot;TGEth1/0/0/1&quot;, &quot;TGEth1/0/0/2&quot;, &quot;TGEth1/0/0/3&quot; ], &quot;bgp_as&quot;: 64512 } } }} 想必很多读者都听过数据结构，构建一个可扩展的CMDB非常需要一个合适的数据结构，当然很多计算机专业的同学应该了解，数据结构是一门十分复杂的学科，无法在短时间内将其讲解清楚，感兴趣的同学可以阅读番外篇详细了解，此处我们先简短的介绍一下需要用到的一些概念： 数据结构（英语：data structure）是计算机中存储、组织数据的方式，不同种类的数据结构适合于不同种类的应用；常见的数据结构有，栈，队列，数组，链表，树，图，堆，散列表=其实我们目前只需要用到数组和散列表(又称哈希表)两种数据结构，我先通俗易懂的讲解一下这两种数据结构 数组， 可以将其理解为一个容器，里面可以装很多元素，只不过这些元素必须是相同类型的, 他们可以用下标的位置进行存取，如 123|a | b | c | d | e | f | g | h | i | j | 0 1 2 3 4 5 6 7 8 9 值得注意的是数组的下标永远都是从0开始，这个对于初期接触编程的读者朋友来说可能会有点儿不适应 散列表，可以将其理解为通讯录，通讯录里的人不可以重名，每个人的名字都对应他的个人信息，个人信息可以存储任何数据，如 123456789101112131415161718192021{ &quot;jack&quot;: &quot;19098090000&quot;, &quot;allen&quot;: { &quot;age&quot;: 20, &quot;gender&quot;: &quot;male&quot; }, &quot;john&quot;: { &quot;city&quot;: &quot;shanghai&quot;, &quot;family&quot;: [&quot;father&quot;, &quot;mother&quot;, &quot;sister&quot;] }} Python基础数据操作 通过上述的介绍，我们了解到了CMDB-v1.0的数据源长什么样子，以及它使用了什么样的数据结构，那么接下来就是如何用Python来表示它，这就涉及到了Python的几大数据类型 字符串: 上述数据源中用到最多的类型就是字符串，如&quot;ip&quot;, &quot;cisco&quot;, &quot;role&quot;等 整数：Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，如 1, 64512, -100 等 浮点数：浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，如1.23，3.14，-9.01，1.5e11，1.5e-21 布尔值：布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值 列表：Python中列表即为数据结构中的数组，一种有序的集合，可以随时添加和删除其中的元素 元组：另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改 集合：也是一组key的集合，但在set中，没有重复的key 字典：dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度，dict即为数据结构中的散列表 Tips 上述说列表和元组为有序序列，并不是说列表和元组中的元素会按大小顺序排列，而是说列表和元组中的每个元素的排列是固定的，即不管print多少次，显示的结果是一样的；但字典和集合中的元素不是有序的，print出的结果可能会不一样；这种现象其实是由于不同的数据结构在计算机内存中不同的存储和表示方法造成的，后续会在番外篇中详细解释。 下面我们就结合CMDB-v1.0的数据源逐一讲解涉及到的数据类型和其操作： 第一个Python程序学习任何一门编程语言第一个程序都是如何打印出Hello World，Python对此的实现十分简单 1# 在命令行模式下，输入python，进入Python的交互模式&gt;&gt;&gt; print(&quot;Hello, World!!!&quot;)# 输出结果为 Hello World# 输入exit()退出Python交互模式，或者可以直接输入ctrl-D直接退出 Tips 从Python实现打印一行字符串其实可以看出很多这门语言的特点，首先给人的第一感觉就是简洁，代码阅读起来和阅读英文十分相似，其次就是Python程序的运行不需要编译，诸如C++，JAVA，Golang运行前都需要进行编译，这是因为Python是一门解释型语言，具体关于解释型语言和编译型语言的区别，后续会在番外篇中详细解释。 变量变量的概念基本上和初中代数的方程变量是一致的，只是在计算机程序中，变量不仅可以是数字，还可以是任意数据类型。 变量名必须是大小写英文、数字和_的组合，且不能用数字开头，比如: 123port_num = 40 # 变量port_num是一个整数hostname = &quot;cisco-test&quot; # 变量hostname是一个字符串。stack = True # 变量stack是一个布尔值True Tips 有过其他语言学习经历的同学可能会了解，程序中定义一个变量时，需要指定这个变量的数据类型，比如 int a = 123;，当把变量a指定为整型时，就无法把字符串再赋值给它，如a='ABC'，这样会出发报错，但Python并没有这样的限制，这也是Python的另一大特点，即Python是一门动态类型语言，动态类型语言的一大好处就是灵活，这也是Python易上手的原因之一，但同时，由于在运行时才确定变量的数据类型，相较于静态类型语言，动态类型语言更容易出错，但我们享受其优点的同时，就必须要接受其弊病。更多关于静态语言与动态语言类型的区别，后续会在番外篇中详细解释。 注释上面的示例代码中我们有使用到注释，注释可以帮我们很好的对代码进行解释说明，利于我们及他人后续阅读 Python的注释一般分为两种 单行注释，可以跟在某行代码的后面，或者写在一个代码块的上面，没有强制的规定， 如 1port_num = 40 # 变量port_num是一个整数 或者 1# 变量port_num是一个整数port_num = 40 多行注释，顾名思义，可以在多行注释内写多行文本 123&quot;&quot;&quot;变量port_num是一个整数这是一个十分复杂的代码&quot;&quot;&quot;port_num = 40 Tips 程序员之间比较流行的一句话是：今天的代码没写注释，别说其他人以后不认识，明天我自己就不认识了。 数组在CMDB-v1.0中端口属性的数据类型就是数组，与之相对应的数据结构是列表。 该数组中存储了某台设备上所有的端口号，我们以此为例看看Python中的数组都有哪些常用操作： 如果我们想知道一共有多少端口，可以使用len()方法，len即为length的简称，很多方法名其实是可以根据名称推断出其作用 len()方法即为求某个可迭代对象的长度，此处我们的可迭代对象为数组，何为可迭代对象，我们会在番外篇中提到 123&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; len(port)# 输出 3 如果我们想获取某一个端口，可以使用数组下标索引进行访问，下标索引默认从0开始，最大为数组长度-1，如果超过数组长度，则会报错 1234567891011&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; port[0]# 输出 Eth1/1/0&gt;&gt;&gt; port[2]# 输出 Eth1/1/2&gt;&gt; port[-1]# 等同于上一个，Eth1/1/2，以此类推，-2即为倒数第二个元素，同样不可以超出数组长度&gt;&gt;&gt; port[len(port)-1]# 输出 Eth1/1/2&gt;&gt;&gt; port[3]# 会产生 IndexError 错误 如果我们想在端口列表中增加一个端口，可以使用append()方法 append()方法为在数组末尾追加一个元素 12345&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; port.append(&quot;Eth1/1/3&quot;)&gt;&gt;&gt; port# 输出 [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;, &quot;Eth1/1/3&quot;] insert()方法可以在数组任意位置插入一个元素 123&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; port.insert(1, &quot;Eth1/1/1/1&quot;)# 输出 [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1/1&quot;, Eth1/1/1&quot;, &quot;Eth1/1/2&quot;] 如果我们想将两个端口列表合并，可以使用extend方法 12345&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; port.extend([&quot;Eth1/1/3&quot;, &quot;Eth1/1/4&quot;])&gt;&gt;&gt; port# 输出 [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;, &quot;Eth1/1/3&quot;, &quot;Eth1/1/4&quot;] 如果我们想修改数组中某个元素，可以直接使用下标索引并对其赋值 12345&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; port[1] = &quot;GEth1/1/1&quot;&gt;&gt;&gt; port# 输出 [&quot;Eth1/1/0&quot;, &quot;GEth1/1/1&quot;, &quot;Eth1/1/2&quot;] 如果我们想删除端口列表中的最后一个端口，可以使用pop()方法 pop()方法会返回弹出数组的最后一个元素，并将其返回 12345&gt;&gt;&gt; port = [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;, &quot;Eth1/1/2&quot;]&gt;&gt;&gt; port.pop()# 输出 Eth1/1/2&gt;&gt;&gt; port# 输出 [&quot;Eth1/1/0&quot;, &quot;Eth1/1/1&quot;] pop(i)可以弹出数组中任意位置的元素 更多数组相关的操作我们可以在以后的实践中慢慢学习 字典CMDB-v1.0中的核心数据类型是字典，对应的数据结构是散列表。 字典中存储了某个IDC的名称和其设备信息，我们以此为例看看Python中的字典都有哪些常用操作： 如果我们想知道这个字典是存储的IDC的信息，可以使用键对其进行查找 字典具有一个性质就是不管存储的数据有多大，根据某个键对其进行查找的速度都会非常快，不会随着字典数据的增加而变慢，这是数据结构中散列表的一个特性，并且字典要求键必须是不可变对象，相关知识我们后续会在番外篇中提到，此处我们暂且以字符串作为字典的键 1234567&gt;&gt;&gt; data = {{...}, {...}}&gt;&gt;&gt; bj_info= data[&quot;beijing&quot;] # 获取beijing IDC的数据&gt;&gt;&gt; data.get(&quot;beijing&quot;) # 同样为根据键进行查找，当字典中不存在 &quot;beijing&quot; 这个键时会返回 None&gt;&gt;&gt; idc_info.get(&quot;xiamen&quot;, {}) # dict.get() 方法可以接收另外一个参数，作为查找的键值不存在时的默认返回值# 输出 {} 如果我们想修改IDC的值，可以通过键对其进行赋值 字典中键和值是一一对应的，一个键只能存储一个值 123&gt;&gt;&gt; device_info = data[&quot;beijing&quot;][&quot;switch&quot;][&quot;10.0.0.1&quot;]&gt;&gt;&gt; device_info[&quot;hostname&quot;] = &quot;test&quot; # 将device_info设备的hostname修改为test 如果我们想知道switch下有哪些设备IP，可以使用dict.keys()方法 12345&gt;&gt;&gt; bj_switches = data[&quot;beijing&quot;][&quot;switch&quot;]&gt;&gt;&gt; bj_switches.keys()# 输出 [&quot;10.0.0.1&quot;, &quot;10.0.0.2&quot;]&gt;&gt;&gt; bj_switches.values() # 该方法可以获取字典中的所有值，得到beijing IDC的所有switch的详情# 输出 [{...}, {...}] 如果我们想给某个设备新增属性信息，可以直接用键去赋值 123456789&gt;&gt;&gt; switch_info_10_1 = bj_switches.get(&quot;10.0.0.1&quot;, {}) # 赋值时必须保证变量是字典，所以如果此处不用dict.get() 默认返回空字典，那么当不存在查询的数据时就会返回None，给None通过键赋值就会报错&gt;&gt;&gt; switch_info_10_1[&quot;label&quot;] = &quot;test_label&quot;&gt;&gt;&gt; switch_info_10_1# 输出 { &quot;label&quot;: &quot;test_label&quot;, manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;cisco-nx95-00-00-01&quot;, ...} 如果我们想用某个新的设备信息覆盖原有设备的属性信息，可以使用dict.update()方法 dict.update()方法接收一个字典，用来更新在原有的字典上 1234567&gt;&gt;&gt; new_dict = { &quot;hostname&quot;：&quot;test-00-00-01&quot;, &quot;role&quot;: &quot;csw&quot; }&gt;&gt;&gt; switch_info_10_1.update(new_dict)&gt;&gt;&gt; switch_info_10_1# 输出 { &quot;manufacturer&quot;: &quot;cisco&quot;, &quot;hostname&quot;: &quot;test-00-00-01&quot;, ...} 如果我们想删除设备的某个属性，可以使用dict.pop()方法 dict.pop()方法接收一个键，将该键和其对应的值从字典中删除 1&gt;&gt;&gt; bj_switches[&quot;10.0.0.1&quot;].pop(&quot;label&quot;) # 删除beijing IDC下10.0.0.1设备的label属性 更多字典相关的操作我们可以在以后的实践中慢慢学习 字符串我们CMDB-v1.0中最多使用到的就是字符串这一数据类型，如&quot;idc&quot;, &quot;beijing&quot;,&quot;ip&quot;等，在Python中使用引号将一串字符引住，即为字符串，引号可以是双引号或者单引号并没有强制要求，但具体如何使用更加规范我们会在番外篇中提到。 下面我们以设备的主机名为例，看看对于字符串有哪些具体的操作方法需要用到， 如&quot;cisco-nx95-00-00-01&quot; 如果我们想查看主机名的长度，可以使用len()方法，上文中提到len()可以获取数组的长度，因为字符串同样也为可迭代对象，所以len()同样可以获取字符串的长度 123&gt;&gt;&gt; hostname = &quot;cisco-nx95-00-00-01&quot;&gt;&gt;&gt; len(hostname)# 输出 19 如果我们将主机名以**-**分隔，可以使用split()方法，该方法需要传入分隔符，并且返回一个数组 1&gt;&gt;&gt; hostname.split(&quot;-&quot;)# 输出 [&quot;cisco&quot;, &quot;nx95&quot;, &quot;00&quot;, &quot;00&quot;, &quot;01&quot;] 如果我们想获取字符串的某一段，可以使用切片的方式，因为Python中字符串的存储与数组十分类似，所以切片的方式同时适用于数组和字符串 123456789101112131415&gt;&gt;&gt; hostname[0:5] # 0可以省略，故等价于 hostname[:5]，Python中的切片是一个左闭右开区间，0-5的切片范围不包括下标5# 输出 cisco&gt;&gt;&gt; hostname[6:len(hostname)] # 等价于 hostname[6:]，切片的区间右侧数字大于等于字符串长度时，都不会报错，此时相当于一直取到字符串末尾# 输出 nx95-00-00-01&gt;&gt;&gt; hostname[0:len(hostname):2] # 切片操作可以接受第三个参数，用于表示步长# 输出 cson9-00-1&gt;&gt;&gt; hostname[::-1] # 第三个操作为负数时可以将字符串或数组倒置# 输出 10-00-00-59xn-ocsic 如果我们想获取某个字符所在的位置，可以使用index()方法，该方法接收字符参数，并且返回该字符在字符串中的第一个出现的下标 1&gt;&gt;&gt; hostname.index(&quot;-&quot;)# 输出 5 更多字符串相关的操作我们可以在以后的实践中慢慢学习 Tips 字符串是一种十分常见的数据类型，但由于字符串是文本，既然是文本就涉及到不同国家之间的编码问题，关于编码相关的内容我们会在番外篇中详细解释，大家暂时只需要知道目前国际通用的是UTF-8编码即可。 知识总结 介绍了CMDB在自动化运维中的重要性 演示了v1.0版本的CMDB的增删改查操作 讲解了Python常用的数据类型：字符串，数组，字典，以及对它们的常用操作方法 CMDB系列第一节我们就暂且讲解到这里，其实Python的数据类型和其操作还没有全部涉及到，我们先只掌握最常用的即可，更多的类型和内置操作方法可以慢慢积累。 第二节我们就会进入到CMDBv1.0版本的具体代码，为大家讲解Python的基础语句以及函数和面向对象相关的知识。 篇后语 文中我们多次提到部分内容会在番外篇中详细解释，最大的原因是某个知识点如果详细展开，都足以单独写一篇文章，但对于初学者来说，我们完全必要花时间在一些细枝末节上，因为当我们学习一门新知识时，我们最好的方法就是自顶向下逐步拆解，如果一头扎进知识的海洋中，那极有可能“溺亡”。 所以如果一些职场朋友，没有多余的精力去深究细节，就没有必要去看番外篇，当然如果对某个知识点十分感兴趣也可以多做了解； 但对于计算机专业的同学，不管已经毕业或者还未毕业，我都强烈大家建议阅读番外篇，只有基础打的足够牢，才能做到触类旁通。","link":"/posts/71625180.html"},{"title":"1.16 自动化运维新手村-Flask-ORM集成","text":"摘要在上一章节「自动化运维新手村-ORM」入门中，我们已经了解了基本的数据库的概念，以及常用的数据表操作，并且我希望大家意识到，如果在后端应用不必须依赖数据库时，最好不要引入；但如果引入，那么就最好在使用之前了解一些数据库的基础知识，比如：从手动写SQL语句建数据表开始。 在建好表后，今天这一章节，就开始讲解如何在后端应用中集成对数据库操作的能力。 ORM全称叫做对象关系映射（Object Relational Mapping，简称ORM） ORM的名称上就已经十分清晰的表明其功能和用途： Object：对象，这里是指编程语言中的对象，例如Python/Java中的Class，或者Golang中的struct Relational：关系，表示特指关系型数据库，如MySQL，Oracle，PostgreSQL Mapping：映射，指将编程语言的对象和关系型数据库之间进行相互关联，例如对象名表示数据表名，对象属性表示数据表列名，以及数据类型等等。 映射关系如图所示 正是由于ORM具有的以上特点，所以在继承了ORM框架的后端应用，如Django，Flask中，才可以实现定义好Model类（对象模型）后，可以一键将其在数据库中创建出对应的数据表结构。 Flask-SQLAlchemy框架安装Flask作为微框架，其集成的几乎所有的能力都来自于插件，ORM框架就是提供数据库操作能力的一种插件。 SQLAlchemy，就是一个Python中十分常用的ORM框架，它提供了高层的ORM对底层的原生数据库的操作，让开发者不用直接和 SQL 语句打交道，而是通过 Python 对象来操作数据库。 而Flask-SQLAlchemy 是SQLAlchemy在Flask应用中的扩展，它旨在通过提供有用的默认值和额外的帮助程序来简化SQLAlchemy在Flask应用中的使用，从而更轻松地完成常见任务。 执行以下命令安装扩展包 123pip install pymysqlpip install flask-sqlalchemy 如果安装速度较慢，可以尝试指定镜像源地址 1pip install flask-sqlalchemy==2.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple 引入将Flask-SQLAlchemy引入到Flask应用中十分简单，如下： 1234567891011121314151617from flask import Flaskfrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)# 如果flask-sqlchemy是3.0.0版本及以上，需要增加Flask上下文。# app.app_context().push()app.config['SQLALCHEMY_DATABASE_URI'] = &quot;mysql+pymysql://root:Yfy98333498@127.0.0.1:3306/ops?charset=utf8&quot;app.config['SQLALCHEMY_ECHO'] = Truedb = SQLAlchemy(app) 使用Flask-SQLAlchemy扩展到简易性显而易见，只需要设置一个SQLALCHEMY_DATABASE_URI，该变量的格式如下： 1mysql+pymysql://username:password@host:port/db_name?charset=utf8 大家根据自己数据库的设置进行替换即可。 除了数据库URI的设置，我这里还十分推荐大家设置一个变量，那就是SQLALCHEMY_ECHO，该变量为True时，可以打印出ORM框架操作对应的数据库SQL语句，一方面对于刚接触数据库的朋友来说，可以更熟悉ORM框架与数据库之间的映射，另一方面也利于排查问题。 Model在引入db之后，就可以开始定义Model模型了，根据上一章节中的数据表结构，我们可以对应的定义出对象模型，如下： 12345678910111213141516171819202122232425class Devices(db.Model): __tablename__ = 'devices' id = db.Column(db.Integer, primary_key=True, autoincrement=True, comment=&quot;自增主键&quot;) ip = db.Column(db.String(16), nullable=False, comment=&quot;IP地址&quot;) hostname = db.Column(db.String(128), nullable=False, comment=&quot;主机名&quot;) idc = db.Column(db.String(32), comment=&quot;机房&quot;) row = db.Column(db.String(8), comment=&quot;机柜行&quot;) column = db.Column(db.String(8), comment=&quot;机柜列&quot;) vendor = db.Column(db.String(16), comment=&quot;厂商&quot;) model = db.Column(db.String(16), comment=&quot;型号&quot;) role = db.Column(db.String(8), comment=&quot;角色&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;) 这里与上节相比新增了两列，分别是created_at, updated_at，在新增一行数据或者某一行数据修改时，会更新数据创建或修改的时间。 大家可以参照数据表的SQL语句来对比一下，ORM框架中模型与数据库的映射。 123456789101112131415161718192021222324252627CREATE TABLE IF NOT EXISTS `devices` ( `id` INT AUTO_INCREMENT COMMENT '自增主键', `ip` VARCHAR(16) NOT NULL COMMENT 'IP地址', `hostname` VARCHAR(128) COMMENT '主机名', `idc` VARCHAR(32) COMMENT '机房', `row` VARCHAR(8) COMMENT '机柜行', `column` VARCHAR(8) COMMENT '机柜列', `vendor` VARCHAR(16) COMMENT '厂商', `model` VARCHAR(16) COMMENT '型号', `role` VARCHAR(8) COMMENT '角色', `created_at` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `updated_at` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 数据类型及列选项从上面的Model创建和SQL语句的对比来看，Flask-SQLAlchemy与数据库之间的数据类型存在特定的映射关系，由于Flask-SQLAlchemy是基于SQLAlchemy实现的，所以其数据类型与SQLAlchemy相同。 SQLAlchemy中常用数据类型： 除了常用的数据类型之外，我们SQL语句中还在定义列的时候，指定了很多属性，这些在SQLAlchemy中也有同样的定义。 SQLAlchemy中常用的列选项 增删改查在定义好Model之后，就进入了最为重要的一步，那就是通过对Model的操作，实现数据表的增删改查。 代码整理如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# /models.pyfrom flask import Flaskfrom flask_sqlalchemy import SQLAlchemyfrom sqlalchemy import textapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = &quot;mysql+pymysql://root:YfyH98333498.@127.0.0.1:3306/ops?charset=utf8&quot;app.config['SQLALCHEMY_ECHO'] = Truedb = SQLAlchemy(app)class Devices(db.Model): __tablename__ = 'devices' id = db.Column(db.Integer, primary_key=True, autoincrement=True, comment=&quot;自增主键&quot;) ip = db.Column(db.String(16), nullable=False, comment=&quot;IP地址&quot;) hostname = db.Column(db.String(128), nullable=False, comment=&quot;主机名&quot;) idc = db.Column(db.String(32), comment=&quot;机房&quot;) row = db.Column(db.String(8), comment=&quot;机柜行&quot;) column = db.Column(db.String(8), comment=&quot;机柜列&quot;) vendor = db.Column(db.String(16), comment=&quot;厂商&quot;) model = db.Column(db.String(16), comment=&quot;型号&quot;) role = db.Column(db.String(8), comment=&quot;角色&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;)if __name__ == '__main__': # 增 # 删 # 改 # 查 增1234567891011# /models.pyif __name__ == '__main__': # 增 device = Devices(ip=&quot;10.0.0.1&quot;, hostname=&quot;BJ-R01-C01-N9K-00-00-01&quot;, idc=&quot;Beijing&quot;, row=&quot;R01&quot;, column=&quot;C01&quot;, vendor=&quot;Cisco&quot;, model=&quot;Nexus9000&quot;, role=&quot;CSW&quot;) db.session.add(device) db.session.commit() 这里在操作数据时会通过db.session来进行操作，并且执行完语句之后需要执行db.session.commit()来提交操作，这是由于Flask-SQLAlchemy通过session机制保证了在多线程操作数据库时互不影响，具体的原理我们会单独在番外篇中提到。 执行python models.py后可以看到控制台输出如下： 123456789101112132022-03-06 20:03:35,775 INFO sqlalchemy.engine.Engine SELECT @@sql_mode2022-03-06 20:03:35,776 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names2022-03-06 20:03:35,777 INFO sqlalchemy.engine.Engine SELECT DATABASE()2022-03-06 20:03:35,778 INFO sqlalchemy.engine.Engine BEGIN (implicit)2022-03-06 20:03:35,780 INFO sqlalchemy.engine.Engine INSERT INTO devices (ip, hostname, idc, `row`, `column`, vendor, model, `role`) VALUES (%(ip)s, %(hostname)s, %(idc)s, %(row)s, %(column)s, %(vendor)s, %(model)s, %(role)s)2022-03-06 20:03:35,781 INFO sqlalchemy.engine.Engine [generated in 0.00023s] {'ip': '10.0.0.1', 'hostname': 'BJ-R01-C01-N9K-00-00-01', 'idc': 'Beijing', 'row': 'R01', 'column': 'C01', 'vendor': 'Cisco', 'model': 'Nexus9000', 'role': 'CSW'}2022-03-06 20:03:35,783 INFO sqlalchemy.engine.Engine COMMIT 执行select * from devices\\G;查询数据库结果如下： 123456789101112131415161718192021222324252627mysql&gt; select * from devices\\G;*************************** 1. row ***************************id: 1ip: 10.0.0.1hostname: BJ-R01-C01-N9K-00-00-01idc: Beijingrow: R01column: C01vendor: Ciscomodel: Nexus9000role: CSWcreated_at: 2022-03-06 20:03:35updated_at: 2022-03-06 20:03:351 row in set (0.00 sec) 可以看到已经成功插入了一条数据，并且数据库自动插入了id, created_at, updated_at字段。 批量新增如下： 123456789101112131415# /models.pyif __name__ == '__main__':# 增device1 = Devices(ip=&quot;10.0.0.1&quot;, hostname=&quot;BJ-R01-C01-N9K-00-00-01&quot;, idc=&quot;Beijing&quot;, row=&quot;R01&quot;, column=&quot;C01&quot;, vendor=&quot;Cisco&quot;, model=&quot;Nexus9000&quot;, role=&quot;CSW&quot;)device2 = Devices(ip=&quot;10.0.0.2&quot;, hostname=&quot;BJ-R01-C01-N9K-00-00-02&quot;, idc=&quot;Beijing&quot;, row=&quot;R01&quot;, column=&quot;C02&quot;, vendor=&quot;Cisco&quot;, model=&quot;Nexus9000&quot;, role=&quot;CSW&quot;)device3 = Devices(ip=&quot;10.0.0.3&quot;, hostname=&quot;BJ-R01-C01-N9K-00-00-03&quot;, idc=&quot;Beijing&quot;, row=&quot;R01&quot;, column=&quot;C03&quot;, vendor=&quot;Cisco&quot;, model=&quot;Nexus9000&quot;, role=&quot;CSW&quot;)db.session.add_all([device1, device2, device3])db.session.commit() 查询数据库结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475mysql&gt; select * from devices\\G;*************************** 1. row ***************************id: 5ip: 10.0.0.1hostname: BJ-R01-C01-N9K-00-00-01idc: Beijingrow: R01column: C01vendor: Ciscomodel: Nexus9000role: CSWcreated_at: 2022-03-06 20:18:52updated_at: 2022-03-06 20:18:52*************************** 2. row ***************************id: 6ip: 10.0.0.2hostname: BJ-R01-C01-N9K-00-00-02idc: Beijingrow: R01column: C02vendor: Ciscomodel: Nexus9000role: CSWcreated_at: 2022-03-06 20:18:52updated_at: 2022-03-06 20:18:52*************************** 3. row ***************************id: 7ip: 10.0.0.3hostname: BJ-R01-C01-N9K-00-00-03idc: Beijingrow: R01column: C03vendor: Ciscomodel: Nexus9000role: CSWcreated_at: 2022-03-06 20:18:52updated_at: 2022-03-06 20:18:523 rows in set (0.00 sec) 删、改、查删除和修改都需要基于查询，由于查询的篇幅较多，我们都统一放到下一章节再讲解。 总结这一章节是比较重要的一节，我们首次在后端应用中引入数据库，并通过ORM框架实现对数据库的操作，所以希望大家务必要亲自实践操作，否则后面的学习内容就无法顺利地开展。","link":"/posts/760ef160.html"},{"title":"2.1 自动化运维初级村-Paramiko vs Netmiko","text":"前言秉承着“最小化上手范围”的原则，相信大家经过新手村二十多个章节的学习，都已经对Python有了初步的了解，基础的学习必然是略显枯燥无聊的，但希望大家可以在“新手村”系列视频和文章的指导下有自己阅读程序，分析需求，编写程序，调试程序的基本技能。 以上也是进入初级村的最低门槛，如果还没有具备上述能力的朋友，衷心的希望你可以返回新手村进行学习，切勿囫囵吞枣，盲目求快。 初级村包含的内容Paramiko和Netmiko 两者的使用方式及优劣 Python 正则解析 将通过SSH收集回来的各种信息进行正则解析 番外：Text-FSM解析Python SNMP 如何使用Python调用SNMP采集指令，并讲解OID使用方式 NetConf 如何通过NetConf对网络设备进行操作 Crontab并回调CMDB 如何设置定时执行SSH任务进行定期巡检，并更新回写CMDB 番外：Python SchedulerFlask Web框架 将SSH和NetConf与Web框架相结合，通过API方式或前端调用其执行 摘要 在自动化运维领域里面，单从服务器运维的角度来讲，由于可以提前对服务器做系统的定制安装，所以一些大厂会选择在装机时植入特定的Agent，以此实现远程控制服务器的目的。 不过对于大部分的公司，服务器运维或者网络设备运维都仍然依靠远程SSH这一方法，所以我们使用Python作为自动化运维的编程语言的话，就非常需要一个第三方包来实现这一功能，而Paramiko和Netmiko可以说是扛把子的角色。 相信看这个教程的朋友肯定都听过这两个工具包，但可能并不是所有的人都了解这两个包之间的关系。除此之外，网上五花八门的对于用哪个更好的争论也难以分辨。 那么这个章节就先给大家介绍一下这两个包的来龙去脉，以及我个人在工作中对两个包都深度使用过之后的体验。 OpenSSH介绍第三方库之前有必要先科普一下关于SSH的知识。 我们经常谈论的SSH是一个传输层协议，相比Telnet来说可以与远端设备建立更安全的连接通道，对传输的内容进行加解密处理，所以基于安全考虑，企业中几乎所有的对远程设备的连接都要求使用SSH连接。 SSH协议有两种不兼容的版本：SSHv1和SSHv2。 那么设备想要支持SSH协议，就需要在设备上安装一个与之相匹配的服务端/客户端的应用程序，而OpenSSH就是目前使用最为广泛的SSH协议的开源实现。 对于较老的SSHv1，由于存在加密算法的专利问题和数据完整性的缺陷，OpenSSH已经删除了对其的支持。所以目前OpenSSH主要支持SSHv2。 OpenSSH 套件包含以下工具： 远程操作使用 ssh、 scp和 sftp完成。 使用ssh-add、 ssh-keysign、 ssh-keyscan和 ssh-keygen 进行密钥管理 。 服务端 sshd、 sftp-server和 ssh-agent组成。 整体的结构图如下所示： 暂时无法在飞书文档外展示此内容 Paramiko/Netmiko简介ParamikoParamiko遵循SSH2协议，支持以加密和认证的方式，进行远程服务器的连接；模块本身使用Python语言编写和开发，只有像crypto这样的核心函数才会用到C语言。 Paramiko目前是Python中应用最广的SSH模块，大家耳熟能详的Ansible， Netmiko，Nornir，NAPALM其实都是用到了Paramiko来做SSH的实现，所以也可以从中看出，其实Paramiko的角色其实是Python语言里实现SSH功能的底层工具包。 那么既然是底层工具包就必然为了具备完备的通用性而损失了易用性，而Paramiko过于底层的方法调用也是被网友最为诟病的原因。 但我们已经提到，Python想要实现SSH远程连接，就逃不开Paramiko，那么我们就非常有必要了解一下Paramiko的基本实现原理和主要的组成类 源码核心架构图 Paramiko中几个大的概念和相互之间的关系基本就是如上图所示了（一些身份认证类和其他杂类并没有包含其中，在学习初期也没有深究这些的必要）。 总体来说Paramiko的源码核心架构并不复杂，但对于使用该包的编程人员来说，暴露了太多底层细节。因为其最High-Level的类就是一个SSHClient对象，而一个SSHClient对象又必须通过创建Channel来完成数据的收发。 所以说Paramiko其实就是一个实现了SSH功能的底层工具包，它可以连接任何兼容SSHv2的设备，包括：服务器，网络设备，打印机，甚至是监控摄像头，并且它最核心的功能就是：建立SSH连接 -&gt; 发数据 -&gt; 收数据。 Netmiko但看本系列的教程的朋友应该大多数都是具备网络运维背景的朋友，想实现一些网络运维中的自动化能力，那么平时所要面对的就都是网络设备，我这里推荐大家使用Netmiko。 我们可以通过一个流程图来表示一下除了Paramiko本身的底层能力外，还需要哪些步骤才能完成一次与网络设备的交互。 交互流程图 从上述的流程图中可以看出，除了Paramiko支持的底层SSH连接与数据发送接收功能之外，我们想要与网络设备进行交互还需要诸多地方需要处理，而Netmiko正好可以提供以上这些功能，这也是我们为什么选择用Netmiko的原因。 总结其实网上有很多文章都有对Paramiko和Netmiko的对比，但大多数的解释都不够准确，并且还有很多直接通过代码来演示区别的，更是让一些编程基础不太好的朋友十分头疼，所以我希望大家可以通过这一章节，非常清晰的知道，两者之间的区别究竟有哪些。 这一章节并没有提到关于使用Paramiko或者Netmiko来进行连接设备的代码，但对Paramiko的源码核心架构做了介绍，这也是大家后续使用Netmiko必须要掌握的基础知识，并且对于Netmiko基于Paramiko的改进也通过流程图展示给大家，对后续的深入理解及二次开发会非常有帮助。","link":"/posts/7a08fdf6.html"},{"title":"2.2 自动化运维初级村-Netmiko-入门","text":"摘要经过上一章节的分析，Netmiko已经确定是SSH连接网络设备工具包的不二之选。那么这一章节我们就正式进入Netmiko的讲解，由浅入深的学会使用Netmiko。 用途首先我们再次总结一下Netmiko能够提供的能力 成功建立到设备的 SSH 连接。 封装掉与设备交互的许多低级机制。 抽象出与设备交互的统一API。 在广泛的网络供应商和平台上执行上述操作。 简化 show 命令的执行、检索和格式化。 简化配置命令的执行。 抽象网络设备架构图在开始讲解如何写代码之前，仍然需要一个架构图来了解Netmiko究竟做了哪些事情，通过架构图的直观展示，可以让第一次接触该包，或者了解不够深入的朋友，对其整体的逻辑有一个清晰的认识。 单纯从架构图可以发现，Netmiko中核心的几个概念都已经较难发现Paramiko的影子，因为虽然Netmiko底层仍然依赖了Paramiko的SSH连接能力，但都对其进行的一定的封装，并且将Telnet，Serial的连接一并进行了High-Level的抽象，这对于后续的使用来说就会非常方便。 支持厂商从上一章节的交互图中可以看出，在连接到网络设备之后，我们需要做诸多的预处理操作，那么这些操作都是与设备厂商强相关的，也就是说Netmiko必须对不同的设备类型做一定的适配。 究竟支持哪些设备厂商，在工具包的开源项目里就有提供，netmiko/PLATFORMS.md。 除此之外我们也可以在使用过程中通过源码检索到，检索方式可以参照视频讲解。 连接设备使用Netmiko来连接设备的代码非常的简短，如下： 123456789from netmiko import ConnectHandlerparams = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot;}net_connect = ConnectHandler(**params) 上述代码是通过用户名和密码进行SSH连接的方式，这种方式最为普遍，所以我们的讲解都按照这种方式来，另外还有使用证书认证的方式，我也会在视频中提到相关参数的使用方法。 ConnectHandler这个函数就是Netmiko中的最上层的工厂函数，它的作用就是根据传入的device_type来选择对应的连接对象，并把工厂函数接收到的所有参数都传递到连接对象中去。 “工厂模式”也是在实际工作中较为常用的一个设计模式，这里的ConnectHandler就是使用了这一思想；将创建方式相同的连接对象通过工厂函数的封装来实现对上层的屏蔽。 该函数会把params参数原封不动的透传到连接对象中去（用到了之前番外篇提到的可变参数自动化运维番外篇-Python参数 ），返回一个初始化好的连接对象Connection。 Connection这个对象就是真正的连接对象了，不同厂商设备的连接对象继承自基类连接对象(BaseConnection) 在这个对象初始化的过程中会打开SSH连接，由上一节内容的介绍可以知道，Netmiko在创建连接的时候是会很多准备工作，这些准备工作如下： test_channel_read：尝试读取通道中的数据。 set_base_prompt：设置分隔符，用来去除输出内容结尾的设备提示符，通常为设备名 set_terminal_width：设置终端宽度，这个值与输出内容的行宽度有关，会影响到结果的换行。 disable_paging：关闭输出分页。 上述是连接基类BaseConnection中提供的函数，不同设备厂商的连接对象会有各自不同的实现。 正是因为Netmiko在登录之后做了上述几个操作，所以会让人感觉Netmiko连接的速度要比Paramiko慢很多，Paramiko只需要协商SSH成功就可以直接收发数据了，但Netmiko的四个准备动作就需要和设备交互四次以上，才算建立连接成功。 可能部分朋友会觉得我讲的有点儿复杂了，但其实我认为这些内容都是非常有必要了解的，实际Netmiko源码中的处理要更为复杂，我这里只把最为核心的操作给大家介绍了一下。 如果像其他博客一样，直接贴几行Netmiko连接设备执行命令的代码，那么我这个教程就毫无意义了；而且如果不了解这些内容，在后续执行命令出现任何问题的时候，因为在最开始根本就没有了解到Netmiko的实现逻辑，所以会排查起来毫无头绪。 因此我希望大家可以耐心一点，慢即是快。 执行命令Netmiko的执行命令操作分为几种，我们先以最常用到的一种来举例 123456789101112from netmiko import ConnectHandlerparams = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot;}net_connect = ConnectHandler(**params)cmd = &quot;show interface brief&quot;output = net_connect.send_command(cmd)print(output) send_command这个函数通常用来执行show类型的命令，默认情况下，该方法将一直等待接收数据，直到检测到设备提示符。 设备提示符默认在执行命令前会自动确定，自动确定的方法为向设备发送一个换行符，然后读取输出内容的最后一行作为设备提示符，后续执行指定命令的时候就会等待输出结果中出现这个提示符，视为命令执行完成。 该函数的主要逻辑如下所示： 从流程图可以看出，调用一个简单的send_command函数后，Netmiko默默帮我们做了非常多的事情，尤其是在“读取数据”的环节，做了很多特殊的处理，并且大家在执行命令的时候遇到问题，大多数都是在“读取数据”的环节出了问题，所以只有了解清楚Netmiko究竟做了哪些事情，才能在出问题的时候快速解决。 读取数据在向通道发送命令之后，会进入到读取通道的数据的逻辑，流程图如下所示： 以上流程图中两个高亮的节点就是读取数据中比较核心的地方； read_timeout超时时间的设置直接决定读取通道数据是否抛出异常，很多情况下获取不到返回结果都是由于超时导致。 Netmiko4.x/5.x在Netmiko5.x中我们可以直接在调用send_command时传入read_timeout参数来指定此次命令执行超时时间，也可以在调用ConnectHandler创建连接的时候传入read_timeout_override参数来将其作为每次执行命令的超时时间。 Netmiko3.x在创建连接的时候如果将delay_factor_compat置为True，则可以将超时时间的设置切换为Netmiko3.x的模式，而在Netmiko3.x中超时时间有不同的计算方法。read_timeout的值是由几个不同的变量共同决定的，公示如下： 1read_timeout = max_loops * loop_delay * delay_factor 如果除了要执行的命令，没传任何其他参数的话，那么这里的超时时间默认为100秒，计算方式如下： 1read_timeout = 500 * 0.2 * 1 = 100s 同样也可以在调用send_command的时候通过传入max_loops|loop_delay|delay_factor来决定执行命令的超时时间。 past_n_readssend_command函数是通过判断是否返回内容中存在指定pattern来决定是否读取结束，但这个Netmiko做了一个特殊的机制来保护性能。 通过流程图可以知道，输出结果是通过循环读取通道来进行累加的，那么就会存在输出内容特别长的情况，所以Netmiko设置了一个MAX_CHARS变量，针对输出内容长度是否超过MAX_CHARS来做不同的处理，流程如下： 上面的流程看起来复杂，其实简单来说就是通过队列先进先出的特性，当output结果过长时，只对最近几次结果进行pattern匹配。 总结这一章节主要讲解了如何使用Netmiko进行设备连接，并执行show命令，但我想强调的是，这一章节的几行代码是次要的，最重要的是对于Netmiko收发数据机制的理解，只有理解了这部分逻辑，才能够对各种异常情况得心应手。","link":"/posts/248441c7.html"},{"title":"2.3 自动化运维初级村-Netmiko-进阶","text":"摘要上一章节中已经介绍了如何使用Netmiko创建连接，发送命令，并了解了Netmiko如何发送命令并接收数据，但只掌握这些还是远远不够的，实际的场景中会遇到各种各样的问题，诸如：设备类型问题，连接超时问题，执行超时问题等，这些问题其实Netmiko都有办法通过参数的调整来解决。 先回顾一下基础的代码，如下所示： 123456789101112from netmiko import ConnectHandlerparams = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot;}net_connect = ConnectHandler(**params)cmd = &quot;show interface brief&quot;output = net_connect.send_command(cmd)print(output) 连接参数Netmiko中支持的不同厂商的连接对象ConnectionClass都是继承自一个基类BaseConnection，并且常见的几种厂商初始化连接对象的方式（即__init__方法）几乎都没有进行重写，也就是说我们目前只需要着重了解基类的初始化即可。 源码中的连接对象初始化参数如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def __init__( self, ip: str = &quot;&quot;, host: str = &quot;&quot;, username: str = &quot;&quot;, password: Optional[str] = None, secret: str = &quot;&quot;, port: Optional[int] = None, device_type: str = &quot;&quot;, verbose: bool = False, global_delay_factor: float = 1.0, global_cmd_verify: Optional[bool] = None, use_keys: bool = False, key_file: Optional[str] = None, pkey: Optional[paramiko.PKey] = None, passphrase: Optional[str] = None, disabled_algorithms: Optional[Dict[str, Any]] = None, allow_agent: bool = False, ssh_strict: bool = False, system_host_keys: bool = False, alt_host_keys: bool = False, alt_key_file: str = &quot;&quot;, ssh_config_file: Optional[str] = None, # # Connect timeouts # ssh-connect --&gt; TCP conn (conn_timeout) --&gt; SSH-banner (banner_timeout) # --&gt; Auth response (auth_timeout) conn_timeout: int = 10, # Timeout to wait for authentication response auth_timeout: Optional[int] = None, banner_timeout: int = 15, # Timeout to wait for the banner to be presented # Other timeouts blocking_timeout: int = 20, # Read blocking timeout timeout: int = 100, # TCP connect timeout | overloaded to read-loop timeout session_timeout: int = 60, # Used for locking/sharing the connection read_timeout_override: Optional[float] = None, keepalive: int = 0, default_enter: Optional[str] = None, response_return: Optional[str] = None, serial_settings: Optional[Dict[str, Any]] = None, fast_cli: bool = True, _legacy_mode: bool = False, session_log: Optional[SessionLog] = None, session_log_record_writes: bool = False, session_log_file_mode: str = &quot;write&quot;, allow_auto_change: bool = False, encoding: str = &quot;utf-8&quot;, sock: Optional[socket.socket] = None, auto_connect: bool = True, delay_factor_compat: bool = False,) -&gt; None: 大部分朋友可能看到这么多参数会非常头疼，我这里给大家做一个归类，就可以很好的掌握这些参数的作用了。 参数分类 类别 参数名 参数类型 默认值 含义 基本参数 ip/host string “” 两者必须填一个，作为远程设备的地址 device_type string “” 根据设备类型映射到对应的连接对象 认证参数 username string “” 对目标设备进行身份验证的用户名 password string None 对目标设备进行身份验证的密码 secret string “” 如果目标设备需要，则设置为enable的密码 use_keys bool False 使用SSH密钥连接目标设备 key_file string None 要使用的SSH密钥文件的文件名路径 pkey paramiko.PKey None 要使用的SSH密钥对象 passphrase string None 用于密钥的密码短语 disabled_algorithms dict None 要禁用的SSH算法字典 allow_agent bool False 启用SSH key-agent ssh_strict bool False 自动拒绝未知的SSH主机密钥(默认值:False表示将接受未知的SSH主机密钥)。 system_host_keys bool False 从users known_hosts文件加载主机密钥 alt_host_keys bool False 如果’ True ‘主机密钥将从指定的文件加载alt_key_file。 alt_key_file string “” 要使用的SSH主机密钥文件(如果alt_host_keys=True) ssh_config_file string “” OpenSSH配置文件文件名 serial_settings Dict None 串口连接的设置 超时参数(单位为秒) conn_timeout int 10 建立TCP连接的超时时间 auth_timeout int 10 设置等待认证响应的超时时间 banner_timeout int 15 等待banner出现的超时时间 blocking_timeout int 20 阻塞读写操作的超时时间，如果有值则会在读写channel阻塞超时后抛出异常 timeout int 20 可以用来调整改变读取通道的max_loops session_timeout int 60 设置并发请求的超时时间 read_timeout_override float None 用来覆盖读取超时的设置 global_delay_factor float 1.0 影响读取通道间隔延迟的乘法因子(默认值:1) delay_factor_compat bool False 设置send_command和send_command_timing使用Netmiko3.x中delay_factor/global_delay_factor/max_loops的行为。这在Netmiko 5中被淘汰x(默认值:False) keepalive int 0 按指定的时间间隔发送SSH keepalive报文，单位为秒 与读写通道相关的参数 global_cmd_verify bool None 控制是否启用或禁用命令回显验证(默认值:没有)。全局属性优先于函数“cmd_verify”论点。“None”的值表示使用函数“cmd_verify”参数 default_enter string “” 默认换行符(\\n) response_return string “” 在规范化返回数据中要表示的字符舒付(默认:\\n) fast_cli bool True 提供一种优化性能的方法。转换select_delay_factor选择最小的全局和特定。设置默认的global_delay_factor为0.1，这个值在Netmiko3.x中会影响到read_timeout的值 _legacy_mode bool False 逐行执行多条命令后，一起收集输出级过 session_log SessionLog None 用来写入会话日志的文件路径或BufferedIOBase子类对象。 session_log_record_writes bool False 会话日志通常只记录通道读操作，以消除命令回传造成的命令重复。如果你想在日志中都记录通道读和通道写，则需要将该值设为True session_log_file_mode string write session_log文件模式为write或append allow_auto_change bool False 允许自动更改终端宽度的设置，部分继承了基类的连接对象会用到该值。 encoding string utf-8 向输出通道写入字节时使用的编码。 sock socket None 一个开放的套接字或类似套接字的对象(如’ . channel ‘)，用于与目标主机通信(默认:None)。 auto_connect bool True 是否在初始化连接对象后自动建立连接 通过上面表格的分类，大家应该都对连接对象的参数有了清晰的认识，虽然每个分类里的参数都比较多，但并不是都需要全部用到，我这里只把常用的几个参数通过高亮标识里出来。 参数调整连接设备必须的基本参数和认证参数就不做赘述了，大家可以根据自己的实际情况选择是用户名/密码方式连接，还是密钥连接。 但超时参数里有几个参数是比较重要的。 conn_timeout auth_timeout banner_timeout 整个连接的建立过程存在几个可能会超时的节点，如下图所示： 所以大家可以根据连接设备时具体的报错日志分析，超时异常出现在哪一个阶段，然后相应的调整连接超时参数即可。 命令参数Netmiko发送命令的函数有很多个，分别适用于不同的场景，但今天我们以最为常用的send_command做为例子来讲解，因为其他的执行函数大多也都是基于该函数进行了封装，或者其参数与该函数的参数也基本类似。 源码中函数的参数如下： 123456789101112131415161718def send_command( self, command_string: str, expect_string: Optional[str] = None, read_timeout: float = 10.0, delay_factor: Optional[float] = None, max_loops: Optional[int] = None, auto_find_prompt: bool = True, strip_prompt: bool = True, strip_command: bool = True, normalize: bool = True, use_textfsm: bool = False, textfsm_template: Optional[str] = None, use_ttp: bool = False, ttp_template: Optional[str] = None, use_genie: bool = False, cmd_verify: bool = True,) -&gt; Union[str, List[Any], Dict[str, Any]]: 其实源码中的类，函数，参数的作用都和实际的命名相关，所以大家可以多去翻译一下，这样不仅可以在后续编程的过程中养成良好的命名习惯，也可以慢慢的面对陌生的函数或参数大概猜到其用法。 参数分类 类别 参数名 参数类型 默认值 含义 基本参数 command_string string “” 在设备上执行的命令 expect_string string None 用于判定输出结束的正则表达式/字符串，如果不传则默认自动获取设备标识符 超时参数 read_timeout float 10.0 循环读取通道的超时时间 delay_factor float None 确定超时时间延迟乘数，只有delay_factor_compat设为True时才使用，通常在Netmiko4.x/5.x中不使用 max_loops int None 确定超时时间的最大循环次数，只有delay_factor_compat设为True时才使用，通常在Netmiko4.x/5.x中不使用 格式化参数 auto_find_prompt bool True 自动获取设备标识 strip_command bool True 删除执行命令的回显信息，通常为删除结果的首行并处理特殊字符 strp_prompt bool True 从输出中删除末尾的路由器提示符 normalize bool True 去除command_string的多余空格并末尾添加换行符 cmd_verify bool True 在获取命令结果之前，验证命令回显，确保不会读取到无用的通道buffer数据 解析参数 use_textfsm bool False 通过TextFSM模板处理命令回显信息 textfsm_template string None TextFSM模版的文件名 use_ttp bool False 通过TTP模板处理命令输出 ttp_template string None TTP模版的文件名 use_genie bool False 通过PyATS/Genie解析器处理命令输出 参数调整执行的命令的参数中大多数都是默认设置，并且都设为了True，也就是说大多数情况下都不需要修改；而需要根据实际情况修改的参数我也高亮标注了出来。 read_timeout这个函数之前已经提到了一次，在Netmiko5.x中可以直接用这个参数来调整循环读取通道的超时时间，不过通常的做法是设置一个较大的值，使其可以覆盖大多数的命令延迟，比如设为100。 如果实际确实有延迟很高的命令的话，再根据具体的命令适当修改read_timeout的值。 可能部分朋友会觉得直接设置一个5分钟或者10分钟，这样就可以覆盖所有命令了，没必要单独处理。 但如果真的某台设备出现故障，所有的命令都延迟极高，或者网络链路出现很大抖动，那么执行的命令会在一段时间内都获取不到结果，这种情况下超时时间都设为5分钟的话，就无法及时抛出超时异常，而是会陷入长时间的循环等待，这当然不是我们想要看到的。 所以对于异常情况，或者边界case的考虑也是需要大家在学习和运用的过程中锻炼提高的。 expect_string由于auto_find_prompt参数默认设为了True，所以即使不传递expect_string，也可以自动获取到设备标识符做为search_pattern。 对于特殊的命令的话，我们可以修改该参数，使其作为判定循环读取结束的正则表达式，但大多数命令其实都是用的设备标识符做的search_pattern。 所以我的建议是这里直接将expect_string设置为self.base_prompt，或者将auto_find_prompt设置为False，因为在连接建立后的session_preparation函数里获取了设备标识符作为base_prompt，所以send_command函数自动获取的标识符同样也是发送换行符后获取的设备标识符，设置调用的函数都是同一个find_prompt，所以我们大多数情况下可以直接穿入self.base_prompt作为search_pattern，这样就可以每条命令都减少一次自动获取标识符时产生的与设备的交互，对于提高性能来说会有不小的帮助。 基于时间的命令执行上面提到send_command是使用最为普遍的一个执行命令函数，它是基于字符串匹配决定是否输出结束，但对于部分命令，我们无法准确的定义它的expect_string，这时候我们就需要另一种方式来获取它的执行结果，那就是send_command_timing，这个函数是一种基于时间机制的命令执行函数。 参数这个函数的参数与send_command中的参数几乎完全一致，只额外多了一个last_read参数，这个参数会对是否结束循环起到关键性的作用。 除此之外，read_timeout时间也从默认的100s改为了默认120s。 流程图 基于时间的机制中，循环读取通道数据后，已经不需要设置last_n_reads队列来提高性能来，因为时间机制下，完全没有做任何的字符匹配，而是单纯通过last_read这一变量来判断是否输出结束。 最为关键的一个节点我已经高亮标注了出来，其实核心的逻辑就是： 循环读取通道时，当此次没有读取到任何数据，那么就说明有可能已经输出结束了，那么此时休眠last_read秒之后再读取一次，如果还没有数据，就判定为结束，如果又读到了数据，那么上一次没读到数据就有可能是设备或网络延迟造成的，则再次进入循环。 调参send_command_timing是基于时间机制来判定输出结束的，所以我们能调整的就是两个参数： read_timeout当执行的命令返回内容输出较长时，则需要循环更多次才能读取到全部结果，这时候就需要调大read_timeout，避免抛出超时异常。 last_read当执行的命令需要较长时间延迟才能输出回显结果时，可能会造成某次循环在last_read时间内从通道中读取不到任何数据，导致提前结束循环，获取不到全部的结果，所以这时候需要调大last_read参数。 执行命令（配置）对于网络设备来说，在不同的模式下可以执行不同的命令，show类型的命令通常都在用户模式或特权模式执行，而配置命令的话必须要在配置模式执行； 那么对于不同的设备进入配置模式的方式也都不尽相同，既然Netmiko能够支持不同厂商的设备，就理所应当也适配了该类型设备进入配置模式的方法。 关于配置模式相关的函数如下： self.check_config_mode: 调用该函数可以检查当前是否在配置模式下，返回一个bool值 self.config_mode：调用函数可以进入设备的配置模式 self.exit_config_mode: 调用该函数可以退出配置模式 通常执行配置类型的命令需要：进入配置模式 -&gt; 执行命令 -&gt; 退出配置模式，而Netmiko就为我们提供了一个封装好的方法： send_config_set源码中的参数如下： 1234567891011121314151617def send_config_set( self, config_commands: Union[str, Sequence[str], Iterator[str], TextIO, None] = None, *, exit_config_mode: bool = True, read_timeout: Optional[float] = None, delay_factor: Optional[float] = None, max_loops: Optional[int] = None, strip_prompt: bool = False, strip_command: bool = False, config_mode_command: Optional[str] = None, cmd_verify: bool = True, enter_config_mode: bool = True, error_pattern: str = &quot;&quot;, terminator: str = r&quot;#&quot;, bypass_commands: Optional[str] = None,) -&gt; str: 参数 参数名 参数类型 默认值 含义 config_commands 多种 None 要执行的配置命令 exit_config_mode bool True 执行完配置命令之后是否退出配置模式 read_timeout float None 如果不传该参数会默认设置为15s， config_mode_command string None 进入配置模式的命令，各厂商会在自己的连接类里定义 cmd_verify bool True 每条配置命令都验证是否输出命令echo enter_config_mode bool True 是否进入配置模式，默认进入 error_pattern string “” 输出结果出现匹配字符串时视为执行异常 terminator string r’#’ 配置模式下判定命令输出结束的正则字符串 bypass_commands string None 与要执行的命令匹配，如果匹配成功则关闭cmd_verify功能 上述参数虽然比较多，但都较好理解，部分参数我也会在视频中更通俗易懂的讲解一下，但这个函数封装的比较完善，绝大多数情况都只需要传递config_command即可，该参数的值可以是字符串，或数组。 总结这一章节主要是对Netmiko中的核心函数的讲解，这些函数的参数也都给大家分类展示了出来，更容易理解和使用，但还是需要从实践中加深理解；而且Netmiko的封装过程都是基于网络设备的交互逻辑，所以大家也不需要死记硬背，只要结合网络运维的经验即可理解掌握。 下一章节我会给大家代入一个简单的巡检场景，讲解如何真正编写一个健壮的与网络设备交互的代码。","link":"/posts/f6ffcca1.html"},{"title":"2.4 自动化运维初级村-Netmiko-巡检-1","text":"摘要人工定期巡检，应该是大多数做运维工作的朋友的一大心病，面对几十上百台机器，执行相同的命令，要仔细得从输出里提取信息，然后把最终的结果整理到Excel里，发一份邮件抄送老板。 我刚开始工作时巡检也是平台规划中优先级较高的一个模块，需要提供灵活的设备筛选，执行用户指定的命令，应用相应的解析规则，最终把结果导出成Excel。 大多数系统的巡检模块也都具备这些功能，那么今天我们就一起来做一个简易的巡检工具，带着大家边实践边趟一趟用Netmiko巡检的坑。 巡检工具这里要说一点的是，不管做什么需求，无论是大到系统的设计，还是一个小的脚本，在动手做之前都需要提前做好充分的思考，思考究竟要实现什么功能，这些功能都会涉及到哪些东西，是否需要引入中间件，比如数据库，缓存库等。 如果盲目的开始动手，那么后果就是一直修修改改，既想要这个功能，又想新加那个功能，而由于没有做好充分的设计，而导致扩展性很差，最后堆成了“屎山”；这也是在做功能模块之前需要编写方案设计文档和技术评审的主要原因。 模块设计图 上述流程图就是我们要实现的巡检工具的大概功能设计。 模块概述设备筛选在新手村中我们曾经讲解过一个简易的CMDB，即支持从json文件中读写设备信息，也支持从MySQL数据库中读写设备信息，那么这个CMDB功能就可以为我们提供设备筛选的能力，而恰好实际工作中，CMDB同样是会承担这个作用。 命令筛选由于不同厂商的设备执行的巡检命令会不一样，例如同一个查看镜像版本的操作，可能会对应不同的具体指令；那么我们就需要先根据不同厂商录入好相应的命令，然后巡检的时候可以根据具体要检查的指标项来筛选命令。 执行器整个巡检工具中最为关键的就是执行器，我们需要实现一个封装好的执行器对象，这个对象对外暴露参数以供其他模块传入；并且它是一个较为抽象的对象，并不是单指通过SSH方式来执行操作，也可以是以API的形式调用厂商API，或者以Netconf的方式对设备进行相应的操作。 解析规则筛选执行器进行相应的操作之后的回显通常都需要提取关键信息，尤其是以SSH的方式执行完命令后，因此需要针对不同的命令录入其解析规则，这个规则可以是正则表达式，也可以是TextFSM模版，这样就可以在执行完命令之后直接根据相应的规则解析出结构化的数据。 保存结果巡检的结果需要进行持久化存储，我们可以选择将其保存到MySQL数据库中，并且保存的内容除了处理完的结构化数据之外，也需要将原始输出进行存储，方便后续追溯。 导出当需要将结果展示或者发送的时候，需要具有导出的功能，具体的细节就是在一定的时间范围内，根据设备和指标项来把之前持久化的数据导出成Excel或CSV。 方案细节设备筛选暂且忽略，后续补充 执行器由于最近的几个章节都在讲解关于Netmiko相关的知识，并且执行器中最为重要的远程执行命令行这一动作，但前期为了便于理解，我们不对执行器做过多的设计，单纯只实现SSH类型的执行器， 执行器必须要具备的几个功能如下： 初始化执行器 获取目标对象 获取执行动作 执行 解析执行结果 保存执行结果 基本代码框架如下 123456789101112131415161718192021222324Executor_Mapping = { &quot;SSH&quot;: SSHExecutor, # SSH执行器对象} class Executor: def __init__(self, executor_type, *args, **kwargs): if not Executor_Mapping.get(executor_type): raise Exception(&quot;has no %s executor&quot; % executor_type) self.executor = Executor_Mapping[executor_type](*args, **kwargs) def fetch_object(self): pass def fetch_action(self): pass def execute(self): pass def parse_result(self): pass def save(self): pass SSH执行器SSH执行器必须要具备的几个功能如上述执行器对象所需的相同。 初始化执行器（建立连接）上一章节中的SSH连接代码如下： 123456789from netmiko import ConnectHandlerparams = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot;}net_connect = ConnectHandler(**params) 首先我们需要对连接时的可能存在的异常进行处理，一般的做法如下： 123456789101112try: conn = ConnectHandler(**device)except NetmikoAuthenticationException: # TODO returnexcept NetmikoTimeoutException: # TODO returnexcept Exception: # TODO returnprint(conn.find_prompt()) 大家可以发现上述代码中的异常分别为NetmikoAuthenticationException、NetmikoTimeoutException、Exception；其中Exception类是Python中的最底层Exception，所有的自定义Exception都需要继承Exception基类，Netmiko库中也会自定义一些异常类，这里的前两个异常类就分别是认证异常类和超时异常类。 自定义异常类必须写在异常基类的前面才能被捕获。 通常情况下会在捕获到异常之后，打印堆栈及错误信息，或者将其记录到日志文件中。 这一点Netmiko显然也有考虑，所以专门提供了针对设备连接阶段的异常捕获，如下： 12345678910111213141516import sysfrom netmiko import ConnUnifyparams = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot;}conn = ConnUnify(**params)if conn is None: sys.exit(&quot;Logging in failed&quot;)print(conn.find_prompt())conn.disconnect() ConnUnify函数的作用就是捕获设备连接过程中各种异常； 该函数的核心代码如下所示： 1234567891011121314151617181920212223def ConnUnify( **kwargs: Any,) -&gt; &quot;BaseConnection&quot;: try: kwargs[&quot;auto_connect&quot;] = False net_connect = ConnectHandler(**kwargs) hostname = net_connect.host port = net_connect.port device_type = net_connect.device_type general_msg = f&quot;Connection failure to {hostname}:{port} ({device_type})\\n\\n&quot; net_connect._open() return net_connect except NetmikoAuthenticationException as e: msg = general_msg + str(e) raise ConnectionException(msg) except NetmikoTimeoutException as e: msg = general_msg + str(e) raise ConnectionException(msg) except Exception as e: msg = f&quot;An unknown exception occurred during connection:\\n\\n{str(e)}&quot; raise ConnectionException(msg) 在try...except中提取了关键字参数中的主机信息，以便打印日志时使用，除此之外，可能有朋友会对高亮处有疑惑： 明明在BaseConnection中创建连接对象的__init__函数中，auto_connect参数默认为True，会默认调用_open()函数；为什么在ConnLogOnly中要专门把auto_connect设为False，然后在下面单独的调用net_connect._open()函数呢？ 我个人的理解是因为ConnectionHandler其实是一个工厂函数，它的作用就是保证根据设备类型返回创建好的连接对象，这个连接对象只要继承了BaseConnection就可以，但子类继承父类时，可以重写其__init__方法，所以可能会存在（或者不保证永远不存在）某个连接对象，在初始化连接对象时不进行_open()，而是在后续再额外发起连接； 如果一旦出现上述情况，那么在对ConnectHandler进行异常捕获时就会无法捕获到建立连接时的异常现象，导致程序中断，这也是为什么这里会显示地调用_open()来建立连接的原因，目的就是为了保证这个try...except一定可以捕获到连接异常。 SSHExecutor的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class SSHExecutor: def __init__( self, host: str = &quot;&quot;, username: str = &quot;&quot;, password: str = None, secret: str = &quot;&quot;, port: str = None, device_type: str = &quot;&quot;, conn_timeout: int = 10, auth_timeout: int = None, banner_timeout: int = 15, log_file: str = &quot;ssh_executor.log&quot;, log_level: str = None, log_format: str = None, ) -&gt; None: self.host = host self.port = port self.username = username self.password = password self.secret = secret self.device_type = device_type self.conn_timeout = conn_timeout self.auth_timeout = auth_timeout self.banner_timeout = banner_timeout import logging if log_level is None: log_level = logging.ERROR if log_format is None: log_format = &quot;%(asctime)s %(levelname)s %(name)s %(message)s&quot; logging.basicConfig(filename=log_file, level=log_level, format=log_format) self.logger = logging.getLogger(__name__) self.conn = self.connect() def connect(self): try: conn = ConnUnify(**self.__dict__) msg = f&quot;Netmiko connection successful to {self.host}:{self.port}&quot; self.logger.info(msg) return conn except Exception as e: self.logger.error(str(e)) 初始化函数冲需要传入一个关键参数log_file，该参数表示连接过程中的日志输出文件，包括Info日志或者Error日志；如果不传的话默认会保存在同级目录下的ssh_executor.log文件中。 如果有需要还可以传入log_level修改日志打印等级，默认为只打印Error等级，或者传入log_format来修改日志格式 总结这一章节主要给大家讲解了如何设计一个巡检模块，并对其中的基本组成做了初步的介绍，尤其是针对初始化SSH执行器做了较为深入的讲解，下一章节我们继续巡检模块的讲解，带领大家实现设备和命令的获取。","link":"/posts/937e7568.html"},{"title":"2.5 自动化运维初级村-Netmiko-巡检-2","text":"摘要上一章节中讲解了巡检的设计方案，并实现了SSH执行器的连接功能，不过更为重要的还是希望培养大家一些基础的编程思想和良好的习惯；那么这一章节会着重讲解获取命令和设备的功能，并逐步完善SSH执行器。 一方面由于部分朋友第一次接触较为复杂的模块设计，其次这几个章节中还包含一些Python中较高级的用法，为了让大家便于理解，就先以文件为存储介质，实现命令筛选和设备筛选，后续的话会扩展到与Flask结合，并使用ORM来操作MySQL实现这些功能。 命令筛选由于是巡检的场景，所以必然是有不同的巡检项的，这些巡检项不需要使用实际的命令行表示，而是统一使用通俗易懂的文字代替，比如：fans_check、power_check等，这样不同厂商实际的命令行就可以对外屏蔽了。 获取命令可以分为两种实现方式，分别是JSON文件和MySQL存储下的数据读写（本章节以JSON文件为主）；虽然是两种实现方式，但理论上它们都应该具备“增删改查”的功能，这就恰好可以应用之前讲到的面向对象中的继承的概念。 首先定义一个ActionHandler表示存储命令的抽象类，该类具有增删改查数据的方法，另外再分别实现ActionJSONHandler和ActionDBHandler继承自ActionHandler。这样的话后续进行命令筛选就不需要区分JSON还是DB类型，因为它们两个都是ActionHandler类型. ActionHandler：定义一个具备读写方法的抽象类如下： 123456789101112131415class ActionHandler: def __init__(self, *args, **kwargs): pass def add(self, data): pass def delete(self, data): pass def update(self, data): pass def get(self, condition): pass 上述代码定义了ActionHandler的初始化及增删改查方法，我们想要的效果是：不管什么存储介质，只要继承了ActionHandler并且实现它所有的实例方法即可。但怎么可以保证所有子类都实现这些方法呢？万一不小心忘了怎么办，或者别人接手代码不知道这个规则又怎么办？可能会导致代码异常，并且还较难察觉，而且基于Python解释型语言的特性，运行时才会抛出异常，这种情况显然不是我们想看到的。 但这个问题有办法可以解决，只需要对上述代码加一点点细节即可。 代码重构12345678910111213141516171819202122232425262728293031323334353637# action.pyimport jsonimport abcfrom typing import List, Dict, Optionalclass Action: name = &quot;&quot; description = &quot;&quot; vendor = &quot;&quot; model = &quot;&quot; cmd = &quot;&quot; type = &quot;&quot; parse_type = &quot;&quot; parse_content = &quot;&quot;class ActionHandler(abc.ABC): @abc.abstractmethod def __init__(self, *args, **kwargs) -&gt; None: pass @abc.abstractmethod def add(self, data: List[Dict]) -&gt; None: pass @abc.abstractmethod def delete(self, data: Dict) -&gt; None: pass @abc.abstractmethod def update(self, data: Dict) -&gt; None: pass @abc.abstractmethod def get(self, condition: Optional[Dict] = None) -&gt; List[Action]: pass 上面的代码看起来好像跟之前章节中写的Python代码有些不一样，多了很多陌生的东西，下面我们就依次来给大家进行讲解。 抽象类上述代码将ActionHandler定义为了抽象类，并且初始化及增删改查方法都定义成了抽象方法，这时候如果某个类继承了它，但没有实现任意一个方法，那么就会IDE会有提示，并且还会及时抛出异常； Python中有关于抽象类的工具包——abc（Abstract Base Classes），继承了abc.ABC的类不可被实例化（实际上也不需要被实例化，因为它只是我们规范命令存储对象的抽象类），同时添加了abc.abstractmethod装饰器的方法必须被子类实现，如果编写代码时某个子类未实现其中某个方法，IDE则会出现如下飘黄报错： 1Class ActionJSONMarket must implement all abstract methods 抽象类的使用在很多第三方库的源码中比较常见，Netmiko中就有用到，netmiko.Channel就是一个抽象基类，要求子类必须实现初始化和读写通道的方法，netmiko.SSHChannel和netmiko.TelnetChannel都继承了Channel基类。 所以在阅读源码的过程中也可以不断的学习到Python的一些高阶技巧，让我们能够把自己的代码编写的更为健壮和可扩展。 函数和变量注解由于Python是动态语言类型，变量的命名和实际变量指向的对象保存在内存的不同地方，所以在3.5版本之前，变量只是一个名字，它并没有类型，但变量指向的实际对象是有类型的： 12var1 = 1 # 整型var1 = &quot;1&quot; # 字符串 上述代码var1先后从整数变成了字符串，但并不是改变的var1的类型，而是改变了var1的引用对象。 基于上述原因Python会给人一种上手很简单，代码写起来很快的错觉；但应用在大型项目中的时候，就会阅读和维护起来很抓狂，尤其是多人协作时，由于类型信息丢失，看到一个函数或方法，都不知道如何传递参数，该函数会返回什么结果。所以使得代码并不健壮，也不易维护。 在Python3.5和3.6版本先后加入了函数注解和类型注解，不过注解也只是给IDE和人看的，实际运行中并不会进行强制校验；但可以结合pylint对代码做检查，这样对代码的可靠性起到不小的帮助。 后续由于模块的代码量变多，非常有必要引入注解，但注解也并不复杂，所有的类型都在typing内置库中可以找到，而且常用的也不多，大家可以慢慢熟悉（我也会在视频中提到）。 ActionJSONHandler可以利用JSON文件作为存储介质，实现命令的读写，数据格式如下： 12345678// action.json[ { &quot;name&quot;: &quot;fans_check&quot;, &quot;description&quot;: &quot;风扇检查&quot;, &quot;vendor&quot;: &quot;h3c&quot;, &quot;model&quot;: &quot;&quot;, &quot;cmd&quot;: &quot;display fans&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;regexp&quot;, &quot;parse_content&quot;: &quot;&quot; }, { &quot;name&quot;: &quot;fans_check&quot;, &quot;description&quot;: &quot;风扇检查&quot;, &quot;vendor&quot;: &quot;h3c&quot;, &quot;model&quot;: &quot;&quot;, &quot;cmd&quot;: &quot;display fans&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;textfsm&quot;, &quot;parse_content&quot;: &quot;&quot; }, { &quot;name&quot;: &quot;fans_check&quot;, &quot;description&quot;: &quot;风扇检查&quot;, &quot;vendor&quot;: &quot;cisco&quot;, &quot;model&quot;: &quot;nexus&quot;, &quot;cmd&quot;: &quot;show fans&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;regexp&quot;, &quot;parse_content&quot;: &quot;&quot; }, { &quot;name&quot;: &quot;fans_check&quot;, &quot;description&quot;: &quot;风扇检查&quot;, &quot;vendor&quot;: &quot;cisco&quot;, &quot;model&quot;: &quot;ios&quot;, &quot;cmd&quot;: &quot;show fans&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;regexp&quot;, &quot;parse_content&quot;: &quot;&quot; }, { &quot;name&quot;: &quot;fans_check&quot;, &quot;description&quot;: &quot;风扇检查&quot;, &quot;vendor&quot;: &quot;huawei&quot;, &quot;model&quot;: &quot;&quot;, &quot;cmd&quot;: &quot;display fans&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;regexp&quot;, &quot;parse_content&quot;: &quot;&quot; }] 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121import jsonimport abcfrom typing import List, Dict, Optionalclass Action: name = &quot;&quot; description = &quot;&quot; vendor = &quot;&quot; model = &quot;&quot; cmd = &quot;&quot; type = &quot;&quot; parse_type = &quot;&quot; parse_content = &quot;&quot; @classmethod def to_model(cls, **kwargs): action = Action() for k, v in kwargs.items(): if hasattr(action, k): setattr(action, k, v) return action def __str__(self): return json.dumps(self.__dict__)class ActionHandler(abc.ABC): @abc.abstractmethod def __init__(self, *args, **kwargs) -&gt; None: pass @abc.abstractmethod def add(self, data: List[Dict]) -&gt; None: pass @abc.abstractmethod def delete(self, data: Dict) -&gt; None: pass @abc.abstractmethod def update(self, data: Dict) -&gt; None: pass @abc.abstractmethod def get(self, condition: Optional[Dict] = None) -&gt; List[Action]: passclass ActionJSONHandler(ActionHandler): def __init__(self, location: str) -&gt; None: &quot;&quot;&quot; :param location: 文件的路径 &quot;&quot;&quot; import os if not os.path.exists(location): raise Exception(&quot;%s path has no exists&quot; % location) self.path = location def add(self, data: List[Dict]) -&gt; None: &quot;&quot;&quot; :param data: List[Dict] 保存的数据 &quot;&quot;&quot; try: with open(self.path, &quot;r+&quot;, encoding=&quot;utf-8&quot;) as f: _data = json.load(f) _data.extend(data) with open(self.path, &quot;w+&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(_data, f, ensure_ascii=False) except Exception as e: print(&quot;save action failed, error: %s&quot; % str(e)) def delete(self, condition: Dict) -&gt; None: &quot;&quot;&quot; :param condition: List[str] 删除的命令 &quot;&quot;&quot; try: with open(self.path, &quot;r+&quot;, encoding=&quot;utf-8&quot;) as f: _data = json.load(f) _data: List[Dict] with open(self.path, &quot;w+&quot;, encoding=&quot;utf-8&quot;) as f: result = [] for idx, item in enumerate(_data): flag = True for k, v in condition.items(): if not v or item[k] != v: flag = False if not flag: result.append(item) json.dump(result, f, ensure_ascii=False) except Exception as e: print(&quot;delete action failed, error: %s&quot; % str(e)) def update(self, data: Dict) -&gt; None: &quot;&quot;&quot; :param data: List[Dict] 更新的数据 &quot;&quot;&quot; pass def get(self, condition: Optional[Dict] = None) -&gt; List[Action]: &quot;&quot;&quot; :param condition: Dict[Str, Any] 筛选条件 :return: List[Dict] &quot;&quot;&quot; result = [] try: with open(self.path, &quot;r+&quot;, encoding=&quot;utf-8&quot;) as f: data = json.load(f) if not condition: return [Action.to_model(**item) for item in data] for item in data: for k, v in condition.items(): if not v: continue if item[k] != v: break else: result.append(Action.to_model(**item)) except Exception as e: print(&quot;search action by condition failed, error: %s&quot; % str(e)) return result 上述代码就是通过JSON文件来实现命令的增删改查，可以通过以下方法测试： 12345678# action.pyif __name__ == '__main__': action_json = ActionJSONHandler(&quot;action.json&quot;) # res = action_json.get({&quot;vendor&quot;: &quot;cisco&quot;, &quot;model&quot;: &quot;nexus&quot;}) # get by conditions # action_json.add([{ &quot;name&quot;: &quot;fans_check&quot;, &quot;description&quot;: &quot;风扇检查&quot;, &quot;vendor&quot;: &quot;huawei&quot;, &quot;model&quot;: &quot;&quot;, &quot;cmd&quot;: &quot;display fans&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;regexp&quot;, &quot;parse_content&quot;: &quot;&quot; }]) # action_json.delete({&quot;cmd&quot;: &quot;display fans&quot;, &quot;vendor&quot;: &quot;h3c&quot;}) res = action_json.get() print(res[0]) 设备筛选之前的章节中已经将设备的数据保存到了MySQL数据库中，并使用ORM来进行了增删改查的操作；有朋友向我咨询问题的过程中谈到觉得ORM要比SQL更复杂，那么这一章节，我就用原生SQL来实现一下设备筛选，大家也可以自行判断一下究竟是哪个更为简单易用。 DeviceHandler同样定义一个具备增删改查方法的抽象类 123456789101112131415161718192021222324import abcfrom typing import List, Dict, Optional# device.pyclass DeviceHandler(abc.ABC): @abc.abstractmethod def __init__(self, *args, **kwargs) -&gt; None: pass @abc.abstractmethod def add(self, data: List[Dict]) -&gt; None: pass @abc.abstractmethod def delete(self, data: Dict) -&gt; None: pass @abc.abstractmethod def update(self, data: Dict) -&gt; None: pass @abc.abstractmethod def get(self, condition: Optional[Dict] = None) -&gt; List[Device]: pass DeviceDBHandler原先存在两张设备表，分别是devices和device_detail，现在需要修改一下device_detail，增加两列： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394alter table device_detail add column (device_type varchar(32), channel varchar(8));# device.pyimport pymysqlfrom pymysql.cursors import Cursor, DictCursorclass Device: ip = &quot;&quot; hostname = &quot;&quot; vendor = &quot;&quot; model = &quot;&quot; hardware = &quot;&quot; channel = &quot;ssh&quot; channel_port = 22 device_type = &quot;&quot; @classmethod def to_model(cls, **kwargs): device = Device() for k, v in kwargs.items(): if hasattr(device, k): setattr(device, k, v) if device.channel == &quot;telnet&quot;: device.channel_port = 23 return device def __str__(self): return json.dumps(self.__dict__) class DeviceDBHandler(DeviceHandler): def __init__(self, user: str, password: str, host: str, database: str, port: int = 3306) -&gt; None: self.conn = pymysql.connect(user=user, password=password, host=host, port=port, database=database, cursorclass=DictCursor) self.conn: pymysql.connections.Connection def get_conn(self) -&gt; Cursor: if self.conn is None: raise Exception(&quot;mysql is lost connection&quot;) return self.conn.cursor() def close_db(self): self.conn.close() def add(self, data: List[Dict]) -&gt; None: cursor = self.get_conn() device_sql = &quot;insert into devices (sn, ip, hostname, idc, vendor, model, role) values (%s, %s, %s, %s, %s, %s, %s);&quot; device_detail_sql = &quot;insert into device_detail values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);&quot; device_data = [] device_detail_data = [] for item in data: device_data.append([ item.get(&quot;sn&quot;, &quot;&quot;), item.get(&quot;ip&quot;, &quot;&quot;), item.get(&quot;hostname&quot;, &quot;&quot;), item.get(&quot;idc&quot;, &quot;&quot;), item.get(&quot;vendor&quot;, &quot;&quot;), item.get(&quot;model&quot;, &quot;&quot;), item.get(&quot;role&quot;, &quot;&quot;) ]) device_detail_data.append([ item.get(&quot;sn&quot;, &quot;&quot;), item.get(&quot;ipv6&quot;, &quot;&quot;), item.get(&quot;console_ip&quot;, &quot;&quot;), item.get(&quot;row&quot;, &quot;&quot;), item.get(&quot;column&quot;, &quot;&quot;), item.get(&quot;last_start&quot;, &quot;&quot;), item.get(&quot;runtime&quot;, &quot;&quot;), item.get(&quot;image_version&quot;, &quot;&quot;), item.get(&quot;over_warrant&quot;), item.get(&quot;warrant_time&quot;) ]) try: cursor.executemany(device_sql, device_data) cursor.executemany(device_detail_sql, device_detail_data) self.conn.commit() except Exception as e: self.conn.rollback() raise Exception(&quot;db insert failed, error: %s&quot; % str(e)) finally: cursor.close() def delete(self, data: Dict) -&gt; None: pass def update(self, data: Dict) -&gt; None: pass def get(self, condition: Optional[Dict] = None) -&gt; List[Device]: cursor = self.get_conn() sql = &quot;select ip, hostname, vendor, model, hardware, channel, device_type from devices &quot; \\ &quot;join device_detail on devices.sn = device_detail.sn&quot; where_str = [] if condition is not None: for k, v in condition.items(): if isinstance(v, int): where_str.append(&quot;%s=%d&quot; % (k, v)) else: where_str.append(&quot;%s='%s'&quot; % (k, v)) if len(where_str) &gt; 0: sql += (&quot; where %s&quot; % &quot;,&quot;.join(where_str)) cursor.execute(sql) result = cursor.fetchall() devices = [] for item in result: devices.append(Device().to_model(**item)) return devices 上述代码中呢，我引入的一个新的第三方库pymysql，这个库就是Python中用来连接MySQL数据库的最常用的轮子，大多数兼容MySQL的ORM框架也是用pymysql作为底层驱动。 我们可以直接使用这个库执行原生SQL，虽然我之前有长期使用过这个库，但在编写上述代码的时候仍然会有诸多细节需要仔细确认（我会在视频中详细提到）； 相比前面章节使用ORM来操作增删改查，易用性的差距简直不是一点半点；除此之外，由于定义了devices和device_detail两张表进行关联，所以大家可以发现，在做数据库操作时会格外的复杂繁琐，如果使用ORM的关联查询的话，处理起这些问题来就非常得心应手。 总结经过这一章节的讲解，我们已经基本准备好了执行命令必须的几个模块：执行器初始化连接，命令筛选，设备筛选，关于保存和解析部分会在后面章节提到，在阅读完这一章节之后，大家便可以自己尝试一下编写SSHExecutor的代码了。我会在下一章节中手把手带领大家来写SSHExecutor。","link":"/posts/4d64d229.html"},{"title":"2.6 自动化运维初级村-Netmiko-巡检-3","text":"摘要上一章节已经完成了命令筛选和设备筛选部分的代码，接下来就是将其整合到执行器中，今天的章节中，我会在视频里带领大家完善SSHExecutor，大家也可以体会一下写代码的过程，以及其中体现的编程思想，对后续的学习和发展会有很大帮助。 SSHExecutor这里再次巩固一下整体巡检的设计图，如下所示： 初始化连接之前章节已经实现的SSHExecutor初始化代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class SSHExecutor: def __init__( self, host: str = &quot;&quot;, username: str = &quot;&quot;, password: str = None, secret: str = &quot;&quot;, port: str = None, device_type: str = &quot;&quot;, conn_timeout: int = 10, auth_timeout: int = None, banner_timeout: int = 15, log_file: str = &quot;ssh_executor.log&quot;, log_level: str = None, log_format: str = None, ) -&gt; None: self.host = host self.port = port self.username = username self.password = password self.secret = secret self.device_type = device_type self.conn_timeout = conn_timeout self.auth_timeout = auth_timeout self.banner_timeout = banner_timeout import logging if log_level is None: log_level = logging.ERROR if log_format is None: log_format = &quot;%(asctime)s %(levelname)s %(name)s %(message)s&quot; logging.basicConfig(filename=log_file, level=log_level, format=log_format) self.logger = logging.getLogger(__name__) self.conn = self.connect() def connect(self): try: conn = ConnUnify(**self.__dict__) msg = f&quot;Netmiko connection successful to {self.host}:{self.port}&quot; self.logger.info(msg) return conn except Exception as e: self.logger.error(str(e)) def execute(self): pass def fetch_object(self): pass def fetch_action(self): pass 获取执行设备/命令上述代码中的执行、获取执行对象、获取执行动作这三个方法都还没有实现，基于设计图中所示，如果想要实现执行，首先需要获取到执行的对象和动作，那么我们上一章节中又分别实现了ActionHandler和DeviceHandler这两个类，并且具有增删改查的功能，那直接将这两个对象传入到执行器中不就可以实现fetch_object()和fetch_action了吗？ 所以SSHExecutor的初始化函数改造如下 12345678910111213141516from action import ActionHandler, Actionfrom device import DeviceHandler, Deviceclass SSHExecutor: def __init__( self, ... # action action_handler: Optional[ActionHandler] = None, # object object_handler: Optional[DeviceHandler] = None, ) -&gt; None: ... self.action_handler = action_handler self.object_handler = object_handler 获取执行对象和执行动作的函数实现如下： 1234567891011121314151617181920class SSHExecutor: ... def fetch_action(self, condition: Dict) -&gt; Action: if not self.action_handler: raise Exception(&quot;has no action handler instance&quot;) filter_actions = self.action_handler.get(condition) if not filter_actions: raise Exception(f&quot;not found action by {condition}&quot;) if len(filter_actions) &gt; 1: self.logger.warning(f&quot;{condition} has more than one action&quot;) return filter_actions[0] def fetch_object(self, condition: Dict) -&gt; Device: if not self.device_handler: raise Exception(&quot;has no object handler instance&quot;) devices = self.device_handler.get(condition) if len(devices) == 0: raise Exception(&quot;not found device by %s&quot; % str(condition)) return devices[0] 既然已经具备了根据条件筛选设备的函数，那么初始化函数中就不需要传入关于device的全部参数，只需要传入设备筛选条件即可，因此初始化函数进一步改造如下： 1234567891011121314151617181920212223242526272829class SSHExecutor: def __init__( self, # device # ... device_condition: Optional[Dict] = None, # logger ... # action action_handler: Optional[ActionHandler] = None, # object object_handler: Optional[DeviceHandler] = None, ) -&gt; None: # device params # ... self.action_handler = action_handler self.object_handler = object_handler if not self.host or not self.port: device = self.fetch_object(device_condition) self.host = device.ip self.port = device.channel_port self.device_type = device.device_type # logging # ... self.conn = self.connect() 执行最后就是执行的代码，这一部分需要格外注意。 执行的前提是要有执行的动作，所以需要将执行的动作当作参数传入execute函数中，如果没有指定Action的话就需要传入动作的筛选条件进行筛选，然后再根据Action是查看类型还是配置类型去调用Netmiko不同的方法。 1234567891011121314151617181920212223from typing import Dict, Optional, Listfrom netmiko import ConnUnifyfrom action import ActionHandler, CommandType, Actionfrom device import DeviceHandler, Deviceclass SSHExecutor: def execute(self, action: Action, action_condition: Optional[Dict] = None, read_timeout: float = 100.0) -&gt; str: if not action: # 根据条件筛选action action = self.fetch_action(action_condition) if not action.cmd: raise Exception(f&quot;action has cmd attribute, action: {action}&quot;) if self.conn is None: self.conn = self.connect() # 根据action类型执行 if action.type == CommandType.Config: output = self.conn.send_config_set(action.cmd, read_timeout=read_timeout) else: output = self.conn.send_command(action.cmd, expect_string=self.conn.base_prompt, read_timeout=read_timeout) self.result[action.cmd] = output return output 这里首先要注意的就是异常前置，将对命令的判断逻辑放在最前面；其次就是对Action类型的判断，这里引入了一个自定义类，如下所示： 1234# action.pyclass CommandType: Show = &quot;show&quot; Config = &quot;config&quot; 在生产环境的代码中，原则上不要出现字面变量（也就是数字或者字符串），尽量将字面量替换为常量，比如时间长了你可能忘了配置类型是config还是configuration，或者在多处用到Action的类型的时候会很难统一； 所以这里定义了一个自定义类来表示两种不同的类型，这样对于代码的可读性和可维护性有很大帮助。 保存到目前为止还没有讲到保存结果的功能，因为保存需要在完成解析之后再做，所以这一部分我们后面再讲，但有个地方大家需要仔细思考一下：在self.execute函数中直接把output返回了出来，那岂不是执行变成了单个Action粒度，如果我想多执行几个Action应该怎么做呢？ 有一种做法是调用多次execute并将结果保存下来，然后解析或者保存的时候再把结果当作参数传入到函数中，这种做法理论上也可以，但就让执行结果这一对象脱离了执行器的上下文；可能有的朋友听起来有些模糊，我们对代码稍加改动大家再看： 1234567891011121314151617181920212223242526272829# executor.pyimport timeclass SSHExecutor: def __init__( self, # device # ... device_condition: Optional[Dict] = None, # logger ... # action action_handler: Optional[ActionHandler] = None, # object object_handler: Optional[DeviceHandler] = None, ) -&gt; None: # ... self.result = [] def execute(self, action: Action, action_condition: Optional[Dict] = None, read_timeout: float = 100.0) -&gt; str: ... # TODO parse_result = self.parse() parse_result = &quot;&quot; self.save(action.cmd, output, parse_result) return output def save(self, cmd: str, output: str, parse_result: str): self.result.append({&quot;cmd&quot;: cmd, &quot;output&quot;: output, &quot;timestamp&quot;: time.time(), &quot;parse_result&quot;: parse_result}) 经过上面的修改，就把执行的结果作为了执行器对象的一个属性，一方面更符合面向对象的封装原则，另一方面也更有利于我们后续对结果进行保存。 关闭连接123456class SSHExecutor: ... def close(self): if self.conn is not None: self.conn.disconnect() 测试下面我们就直接测试写好的SSHExecutor，让他连接到我局域网内的一台模拟器设备上完成执行命令的动作，测试代码如下： 12345678910111213141516171819202122232425import loggingfrom device import DeviceDBHandlerfrom action import ActionJSONHandler, Actionfrom executor import SSHExecutorSSH_USERNAME = &quot;cisco&quot;SSH_PASSWORD = &quot;cisco&quot;device_filter = {&quot;ip&quot;: &quot;192.168.31.149&quot;}action_filter = {&quot;name&quot;: &quot;power_check&quot;}if __name__ == '__main__': logging.basicConfig(filename=&quot;ssh_executor.log&quot;, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) try: device_handler = DeviceDBHandler(&quot;root&quot;, &quot;Yfy98333498&quot;, &quot;localhost&quot;, &quot;python_ops&quot;) action_handler = ActionJSONHandler(&quot;action.json&quot;) executor = SSHExecutor(username=SSH_USERNAME, password=SSH_PASSWORD, device_condition=device_filter, device_handler=device_handler, action_handler=action_handler, logger=logger) output = executor.execute(action=Action.to_model(cmd=&quot;show ip interface brief&quot;)) logger.info(output) executor.close() except Exception as e: logger.error(str(e)) 执行后日志如下： 123456789101112131415161718192021222324# ssh_executor.log2022-12-01 20:10:18,466 - __main__ - INFO - Netmiko connection successful to 192.168.31.149:222022-12-01 20:10:18,617 - __main__ - INFO - Interface IP-Address OK? Method Status ProtocolEthernet0/0 192.168.31.149 YES DHCP up up Ethernet0/1 unassigned YES unset administratively down down Ethernet0/2 unassigned YES unset administratively down down Ethernet0/3 unassigned YES unset administratively down down # netmiko.log###################################################### Welcome to Cisco, Please quit if not administrator######################################################r1&gt;r1&gt;terminal width 511r1&gt;terminal length 0r1&gt;r1&gt;show ip interface briefInterface IP-Address OK? Method Status ProtocolEthernet0/0 192.168.31.149 YES DHCP up up Ethernet0/1 unassigned YES unset administratively down down Ethernet0/2 unassigned YES unset administratively down down Ethernet0/3 unassigned YES unset administratively down down r1&gt;r1&gt;exit 总结这一章节给大家演示如何逐步完善SSH执行器的代码，其中涉及到面向对象的特性、Python中的函数注解、异常处理和日志记录等诸多知识点，希望大家可以反复观看视频进行深入理解。","link":"/posts/f357f9b2.html"},{"title":"2.7 自动化运维初级村-巡检-Flask大型应用-上","text":"摘要经过前面几个章节的学习，大家应该已经对巡检模块的整体设计以及各个部分的实现都有了清晰的了解，但之前的代码其实只是小试牛刀，主要是为了让大家能够更方便的理解各个部分的功能，并且让刚接触较为复杂的程序设计的朋友更容易上手。 今天的章节中，我们会把巡检的代码和新手村中的CMDB结合起来，将巡检集成到Flask后端应用中。 设备/命令Handler实现之前的章节中DeviceHandler和ActionHandler都各自实现了一个具备增删改查功能的子类，今天我们就用ORM将其改造一下，使其能结合到Flask应用中。 ActionORMHandler首先创建Action的数据表 123456789101112CREATE TABLE `action` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(64) NOT NULL COMMENT '动作名称', `description` varchar(256) DEFAULT NULL COMMENT '动作描述', `vendor` varchar(64) DEFAULT NULL COMMENT '厂商', `model` varchar(64) DEFAULT NULL COMMENT '型号', `cmd` varchar(256) NOT NULL COMMENT '命令行', `type` varchar(8) DEFAULT NULL COMMENT '命令类型[show|config]', `parse_type` varchar(8) DEFAULT NULL COMMENT '解析类型[regexp|textfsm]', `parse_content` varchar(1024) DEFAULT NULL COMMENT '解析内容', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 然后定义两个与数据模型相关的工具方法，负责从数据模型转成字典，或者从字典转成数据模型 1234567891011121314151617181920212223242526# utils.pyfrom typing import Dict, Anydef to_model(cls: Any, **kwargs: Dict) -&gt; Any: &quot;&quot;&quot; 根据关键字参数生成对象 :param cls: ClassVar 目标对象 :param kwargs: Dict 关键字字典 :return: ClassVar &quot;&quot;&quot; device = cls() # 实例化一个对象 columns = [c.name for c in cls.__table__.columns] # 获取模型定义的所有列属性的名字 for k, v in kwargs.items(): # 遍历传入kwargs的键值 if k in columns: # 如果键包含在列名中，则为该对象赋加对应的属性值 setattr(device, k, v) return devicedef to_dict(self: Any) -&gt; Dict: &quot;&quot;&quot; 将实例对象的属性生成字典 :param self: ClassVar 对象实例 :return: Dict &quot;&quot;&quot; return {c.name: getattr(self, c.name) for c in self.__table__.columns} 下面在原先的app.py文件中定义Action的数据模型 12345678910111213141516171819202122# app.pyfrom utils import to_model, to_dictclass Action(db.Model): __tablename__ = &quot;action&quot; id = db.Column(db.Integer, primary_key=True, autoincrement=True) name = db.Column(db.String(64), nullable=False, comment=&quot;动作名称&quot;) description = db.Column(db.String(256), comment=&quot;动作描述&quot;) vendor = db.Column(db.String(64), comment=&quot;厂商&quot;) model = db.Column(db.String(64), comment=&quot;型号&quot;) cmd = db.Column(db.String(256), nullable=False, comment=&quot;命令行&quot;) type = db.Column(db.String(8), comment=&quot;命令类型[show|config]&quot;) parse_type = db.Column(db.String(8), comment=&quot;解析类型[regexp|textfsm]&quot;) parse_content = db.Column(db.String(1024), comment=&quot;解析内容&quot;) @classmethod def to_model(cls, **kwargs) -&gt; Dict: return to_model(cls, **kwargs) def to_dict(self) -&gt; db.Model: return to_dict(self) 有了数据模型之后，就可以定义ActionORMHandler类，该类初始化的时候接收一个handler参数，该参数就是SQLAlchemy的db实例，可以通过这个handler来操作数据库，实现增删改查，代码如下： 12345678910111213141516171819202122232425262728293031323334# action.pyfrom app import Actionclass ActionORMHandler(ActionHandler): def __init__(self, handler): self.handler = handler def add(self, args: List[Dict]): if self.handler is None: raise Exception(&quot;has no active db handler&quot;) actions = [] for item in args: actions.append(Action.to_model(**item)) self.handler.add_all(actions) self.handler.commit() def delete(self, args: List[int]): if self.handler is None: raise Exception(&quot;has no active db handler&quot;) Action.query.filter(Action.id.in_(args)).delete() self.handler.commit() def update(self, args: List[Dict]): if self.handler is None: raise Exception(&quot;has no active db handler&quot;) for item in args: if &quot;id&quot; not in item: continue Action.query.filter_by(id=item.pop(&quot;id&quot;)).update(item) self.handler.commit() def get(self, filters: Optional[Dict] = None): return Action.query.filter_by(**(filters or {})).all() 利用orm进行增删改动作之后，均需要调用commit完成提交，否则操作会被回滚。 DeviceORMHandler由于之前我们已经在app.py中定义过Device和DeviceDetail的数据模型，但之前的章节中给device_detail表增加了两列，所以现在需要修改DeviceDetail模型的属性，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# app.pyfrom sqlalchemy import text, DateTime, Numericfrom ..utils import to_model as tm, to_dict as tdclass Devices(db.Model): __tablename__ = &quot;devices&quot; sn = db.Column(db.String(128), primary_key=True, comment=&quot;资产号&quot;) ip = db.Column(db.String(16), nullable=False, comment=&quot;IP地址&quot;) hostname = db.Column(db.String(128), nullable=False, comment=&quot;主机名&quot;) idc = db.Column(db.String(32), comment=&quot;机房&quot;) vendor = db.Column(db.String(16), comment=&quot;厂商&quot;) model = db.Column(db.String(16), comment=&quot;型号&quot;) role = db.Column(db.String(8), comment=&quot;角色&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;) detail = db.relationship(&quot;DeviceDetail&quot;, uselist=False, backref=&quot;device&quot;) ports = db.relationship(&quot;Ports&quot;, uselist=True, backref=&quot;device&quot;) @classmethod def to_model(cls, **kwargs): return tm(**kwargs) def to_dict(self): res = {} for col in self.__table__.columns: val = getattr(self, col.name) if isinstance(col.type, DateTime): # 判断类型是否为DateTime if not val: # 判断实例中该字段是否有值 value = &quot;&quot; else: # 进行格式转换 value = val.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) elif isinstance(col.type, Numeric): # 判断类型是否为Numeric value = float(val) # 进行格式转换 else: # 剩余的直接取值 value = val res[col.name] = value return resclass DeviceDetail(db.Model): __tablename = &quot;device_detail&quot; sn = db.Column(db.String(128), db.ForeignKey(Devices.sn, ondelete=&quot;cascade&quot;), primary_key=True, comment=&quot;资产号&quot;) ipv6 = db.Column(db.String(16), nullable=False, comment=&quot;IPv6地址&quot;) console_ip = db.Column(db.String(16), nullable=False, comment=&quot;console地址&quot;) row = db.Column(db.String(8), comment=&quot;机柜行&quot;) column = db.Column(db.String(8), comment=&quot;机柜列&quot;) last_start = db.Column(db.DateTime(), comment=&quot;最近启动时间&quot;) runtime = db.Column(db.Integer, comment=&quot;运行时长&quot;) image_version = db.Column(db.String(128), comment=&quot;镜像版本&quot;) over_warrant = db.Column(db.BOOLEAN, comment=&quot;是否过保&quot;) warrant_time = db.Column(db.DateTime(), comment=&quot;过保时间&quot;) device_type = db.Column(db.String(32), comment=&quot;远程连接设备类型&quot;) channel = db.Column(db.String(8), comment=&quot;远程连接方式[ssh|telnet]&quot;) created_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), comment=&quot;创建时间&quot;) updated_at = db.Column(db.DateTime(), nullable=False, server_default=text('NOW()'), server_onupdate=text('NOW()'), comment=&quot;修改时间&quot;) @classmethod def to_model(cls, **kwargs): return tm(**kwargs) def to_dict(self): return td(self) 修改好数据模型后就可以利用其实现DeviceORMHandler，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839# device.pyfrom app import Devices, DeviceDetailclass DeviceORMHandler(DeviceHandler): def __init__(self, handler: scoped_session): self.handler = handler def add(self, args: List[Dict]): if self.handler is None: raise Exception(&quot;has no active db handler&quot;) devices = [] for item in args: device = Devices.to_model(**item) device.device = DeviceDetail.to_model(**item) devices.append(device) self.handler.add_all(devices) self.handler.commit() def delete(self, args: List[int]): if self.handler is None: raise Exception(&quot;has no active db handler&quot;) Devices.query.filter(Devices.sn.in_(args)).delete() self.handler.commit() def update(self, args: List[Dict]): if self.db_handler is None: raise Exception(&quot;has no active db handler&quot;) for item in args: if &quot;sn&quot; not in item: continue sn = item.pop(&quot;sn&quot;) if &quot;detail&quot; in item: DeviceDetail.query.filter_by(sn=sn).update(item.pop(&quot;detail&quot;)) Devices.query.filter_by(sn=sn).update(item) self.db_handler.commit() def get(self, filters: Optional[Dict] = None) -&gt; List[Devices]: return Devices.query.filter_by(**(filters or {})).all() 文件拆分在定义了很多个数据模型之后大家应该会发现，app.py文件已经变得非常冗长，Model和Route混在一起，让代码开始变得难以阅读和维护，这时候就需要进行适当的文件拆分了。 大部分朋友第一反应应该就是，将Model拆出去写在另一个文件不就好了？就像如下： 12345678910111213# model.pyfrom app import dbclass Action(db.Model): passclass Devices(db.Model): pass class DeviceDetail(db.Model): pass 拆完后model.py中需要引用ORM的db对象，这个对象是在app.py中创建的。 device.py和action.py文件中的ORMHandler，需要引用到Action和Devices的Model类，app.py中的路由函数如果需要提供命令和设备的增删改查接口的话，就需要从device.py或action.py中引入ORMHandler，这时候就出现了让人非常头疼的circular import，也叫循环引用；如下图所示： 当一个项目或者应用的体积初步变大的时候，出现循环引用的情况是必然现象，这时候就需要对项目的整体布局进行合理的规划。 Flask大型应用下面我会详细给大家讲解如何进行Flask大型应用的布局规划，并把巡检模块集成到后端中。 工厂函数创建应用根据上面的图示可知，循环引用的最源头其实就是app对象，所以现在首先就是要把app拆分出来，使用“应用程序工厂”来创建app，代码如下： 12345678910111213141516# __init__.pyfrom flask import Flaskdef create_app() -&gt; Flask: app = Flask(__name__) # 挂载各种中间件 # ... return app# run.pyfrom . import create_appapp = create_app()if __name__ == &quot;__main__&quot;: app.run() “应用程序工厂”的意思就是在一个函数里把app给“生产”出来，在工厂里把数据库初始化完成，或者在app上挂载其他的对象，这些下面会提到。 ORM集成我们需要使用到SQLAlchemy作为ORM的第三方库，并将其注册到app上，除此之外将model进行拆分，代码如下： 12345678910111213141516171819202122232425262728293031# /models/__init__.pyfrom flask_sqlalchemy import SQLAlchemydb = SQLAlchemy()# /models/action.pyfrom . import dbclass Action(db.Model): pass# /models/device.pyfrom . import dbclass Devices(db.Model): pass class DeviceDetail(db.Model): pass# /__init__.pyfrom config import configfrom model import dbdef create_app(env: str = &quot;dev&quot;) -&gt; Flask: app = Flask(__name__) # register db db.init_app(app) return app 这里大家可能发现，在models/init.py中的db没有传入app参数，是不是没有注册在Flask的app上呢？ 其实并不是，我们在create_app的工厂里面引入了models中的db对象，并使用的db.init_app(app)进行了注册，在程序启动一开始创建app的过程里已经把db这个对象注册过了，因为models/init.py是一个全局对象，所以后续用到的db都会是已经注册过的db。 路由蓝图之前的章节中我们实现了对设备数据的增删改查，分别对应四个路由函数，那么现在又增加了命令数据的增删改查，后续又会有解析规则的增删改查，那么就有必要对路由文件进行拆分。 原先直接使用app.route()进行路由的注册，这种方式也会导致我们的路由文件依赖app，导致了循环引用的风险。 所以结合上述两点原因，把路由文件进行拆分，将路由函数进行分类，同一类的称作一个Blueprint（蓝图），代码如下： 123456789101112131415161718# cmdb_view.pyfrom flask import Blueprintcmdb_blueprint = Blueprint(&quot;cmdb&quot;, __name__, url_prefix=&quot;/cmdb&quot;)@cmdb_blueprint.route(&quot;/get&quot;)def get(): return &quot;cmdb&quot;# action_view.pyfrom flask import Blueprintaction_blueprint = Blueprint(&quot;action&quot;, __name__, url_prefix=&quot;/action&quot;)@action_blueprint.route(&quot;/get&quot;)def get(): return &quot;action&quot; 如上述代码所示，从Flask中引入Blueprint对象，创建一个蓝图实例； 参数nameBlueprint的第一个参数是name，这个参数起到了相当重要的作用，应用程序想要区分不用的蓝图模块就是靠这个参数，现在cmdb和action蓝图模块里都存在get函数，而且都把这个函数注册成了路由函数，如果实例化cmdb_blueprint和action_blueprint的时候没有传name参数或者传了相同的值（最新版Flask该参数要求必传），那程序启动就会报错，因为Flask不允许两个路由函数的endpoints重名（函数上添加了路由装饰器，就会将该函数名称作为路由的endpoints，例如”/get”路由对应的endpoints就是”get”）； 但是如果两个函数上加的是不同的蓝图装饰器，并且两个蓝图实例传入了不同name，那就会在路由注册的时候进行区分，大家可以理解成将两个函数分别注册成了“action_get”和“cmdb_get”，这样就可以避免endpoints重复。 import_name这个参数通常传__name__，通过该参数定位还蓝图的根路径，不需要深究 url_prefix这个参数也很重要，该参数的值会附加在路由的URL前面，比如action_blueprint注册的“/get”路由，当传入了url_prefix=”/action”之后，这个函数的路由就变成了“/action/get”。 注册有一点大家不知道发现没有，只有一个app.py的时候，创建了app，直接使用app.route进行路由注册，那现在拆分之后，Blueprint是从Flask直接引入的，貌似跟我们“工厂”中的app没有任何关系。 实际上到目前为止确实还没有关系，因为创建完蓝图之后还需要挂载到app，代码如下： 12345678910111213# __init__.pyfrom action_view import action_blueprintfrom cmdb_view import cmdb_blueprintdef create_app() -&gt; Flask: app = Flask(__name__) ... # register blueprint blueprints = [cmdb_blueprint, action_blueprint] for blueprint in blueprints: app.register_blueprint(blueprint) return app 挂载的形式其实在我看来是大型应用解决循环引用的一个非常好的方式。 配置在任何的项目中配置管理都是非常重要的一部分，通常一个应用会存在多种环境，至少会有“开发环境”和“生产环境”两个，不同环境的配置是不一样的。 Flask管理配置也有不同的方式，有通过文件管理的，也有直接通过变量来管理的，我们采用一个通过类的管理方式，代码如下： 12345# config.pyclass Config: DEBUG = False LOG_LEVEL = &quot;info&quot; SQLALCHEMY_ECHO = False 首先定义一个配置基类，其中包含三项基本配置，下面再定义两个不同环境的配置类，继承自基类，代码如下： 1234567891011121314151617181920212223242526272829303132# config.pyclass Config: DEBUG = False LOG_LEVEL = &quot;INFO&quot; SQLALCHEMY_ECHO = Falseclass DevelopmentConfig(Config): DEBUG = True LOG_LEVEL = &quot;DEBUG&quot; # 查询时会显示原始SQL语句 SQLALCHEMY_ECHO = True # 数据库连接格式 SQLALCHEMY_DATABASE_URI = &quot;mysql+pymysql://root:YfyH98333498.@localhost:3306/python_ops?charset=utf8&quot; # 动态追踪修改设置，如未设置只会提示警告 SQLALCHEMY_TRACK_MODIFICATIONS = False # 数据库连接池的大小 SQLALCHEMY_POOL_SIZE = 10 # 指定数据库连接池的超时时间 SQLALCHEMY_POOL_TIMEOUT = 10 # 控制在连接池达到最大值后可以创建的连接数。当这些额外的 连接回收到连接池后将会被断开和抛弃。 SQLALCHEMY_MAX_OVERFLOW = 2class ProductionConfig(Config): passconfig_mapper = { &quot;dev&quot;: DevelopmentConfig, &quot;prod&quot;: ProductionConfig,} 在“工厂应用”中给app做配置，通过环境变量区分应该使用哪一个环境的配置类，代码如下： 123456789101112131415161718192021# __init__.pyfrom config import config_mapperdef create_app(env: str = &quot;dev&quot;) -&gt; Flask: app = Flask(__name__) # register configuration app.config.from_object(config_mapper[env]) # register blueprint # ... return app# run.pyimport osfrom . import create_appenv = os.getenv(&quot;env&quot;, &quot;dev&quot;)app = create_app(env)if __name__ == '__main__': app.run() 到目前为止已经完成了Flask应用到初步改造，下面就需要对项目的目录稍作调整即可。 项目布局完整的项目布局如下： 1234567891011121314151617181920/application --------- 应用目录||models ------------ 数据表模型目录|| |__init__.py|| |action.py ------- 执行命令模型|| |cmdb.py --------- 设备模型|| | ||views ------------- 路由目录|| |__init__.py|| |action.py ------- 执行命令的路由|| |cmdb.py --------- 设备信息的路由|| |||services ---------- 业务逻辑目录|| |action.py ------- 获取命令的逻辑代码|| |cmdb.py --------- 获取设备的逻辑代码|| |executor.py ----- 执行器的逻辑代码|| |||__init__.py ------- 应用工厂目录|config.py ----------- 配置文件|utils.py ------------ 工具方法文件|run.py ------------- 启动文件 总结关于Flask构建大型应用的基本布局就讲解完了，有很多需要注意的细节和需要大家仔细体会的编程思想，希望大家可以结合代码仔细的阅读研究。下一章节我会继续完善Flask的后段应用，让它变得更加的健壮，一起期待吧。","link":"/posts/ccee86b7.html"},{"title":"2.8 自动化运维初级村-巡检-Flask大型应用-下","text":"摘要经过上一章节的重构之后，我们已经把设备和命令的增删改查成功的集成到了Flask应用中，并且后端应用也具备了大型项目的雏形。 今天的章节则会对应用做更进一步的完善和延伸处理，希望大家可以从中学到如何把自己的代码变得能够在生产环境中安全稳定的运行。 路由函数首先需要实现设备和命令的增删改查功能的路由函数，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# /application/views/action.pyfrom flask import Blueprint, requestfrom ..models import dbfrom ..services import ActionORMHandleraction_blueprint = Blueprint(&quot;action&quot;, __name__, url_prefix=&quot;/action&quot;)@action_blueprint.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() ActionORMHandler(db.session()).add(data) return &quot;success&quot;@action_blueprint.route(&quot;delete&quot;, methods=[&quot;POST&quot;])def delete(): data = request.get_json() ActionORMHandler(db.session()).delete(data) return &quot;success&quot;@action_blueprint.route(&quot;update&quot;, methods=[&quot;POST&quot;])def update(): data = request.get_json() ActionORMHandler(db.session()).update(data) return &quot;success&quot;@action_blueprint.route(&quot;/get&quot;)def get(): args = request.args.to_dict() res = ActionORMHandler(db.session()).get(args) return [item.to_dict() for item in res]# /application/views/cmdb.pyfrom flask import Blueprint, requestfrom ..models import dbfrom ..services import DeviceORMHandlercmdb_blueprint = Blueprint(&quot;cmdb&quot;, __name__, url_prefix=&quot;/cmdb&quot;)@cmdb_blueprint.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() DeviceORMHandler(db.session()).add(data) return &quot;success&quot;@cmdb_blueprint.route(&quot;/delete&quot;, methods=[&quot;POST&quot;])def delete(): data = request.get_json() DeviceORMHandler(db.session()).delete(data) return &quot;success&quot;@cmdb_blueprint.route(&quot;/update&quot;, methods=[&quot;POST&quot;])def update(): data = request.get_json() DeviceORMHandler(db.session()).update(data) return &quot;success&quot;@cmdb_blueprint.route(&quot;/get&quot;)def get(): res = DeviceORMHandler(db.session()).get() return [item.to_dict() for item in res] 设备和命令的相关路由函数非常简单，只需要调用services中提供的DeviceHandler和ActionHandler即可，这也是把代码合理分层的最重要原因——让代码变得更具有易读性和可维护性。 下面注册SSH执行器相关的路由函数，代码如下： 123456789101112131415161718192021222324# /application/views/executor.pyfrom flask import Blueprint, request, current_appfrom ..models import db, Actionfrom ..services import DeviceORMHandler, ActionORMHandler, SSHExecutorexecutor_blueprint = Blueprint(&quot;executor&quot;, __name__, url_prefix=&quot;/executor&quot;)@executor_blueprint.route(&quot;/prompt&quot;, methods=[&quot;POST&quot;])def get_prompt(): data = request.get_json() device_handler = DeviceORMHandler(db.session()) action_handler = ActionORMHandler(db.session()) ssh_executor = SSHExecutor( username=current_app.config.get(&quot;SSH_USERNAME&quot;), password=current_app.config.get(&quot;SSH_PASSWORD&quot;), secret=current_app.config.get(&quot;SSH_SECRET&quot;), device_condition=data.get(&quot;device_condition&quot;), device_handler=device_handler, action_handler=action_handler, logger=current_app.logger) prompt = ssh_executor.conn.base_prompt ssh_executor.close() return prompt 上述代码中，创建了新的蓝图，将与执行器有关的路由都注册到executor_blueprint上； 除此之外我将需要注意的地方高亮了出来： 路由函数允许接受的方法：methods=[“POST”] 获取JSON类型的body请求体：request.get_json() 获取配置信息：current_app.config.get(“”) 获取已注册的logger（下文提到）：current_app.logger 接口测试之前的章节中提到过可以使用postman类型的插件或扩展进行接口测试，这一章节我们用另外一种方式进行测试； 如果使用的是PyCharm的话，可以在目录里新建后缀为.http的文件，内容如下： 1234567# api.httpPOST http://127.0.0.1:5000/executor/promptContent-Type: application/json{&quot;device_condition&quot;: {&quot;ip&quot;: &quot;192.168.31.149&quot;}}### 可以点击左侧的绿色箭头直接出发http请求，其中Content-Type是http请求里body的类型，该类型需要和后端保持一致。 异常处理一个部署在生产环境的项目，就一定需要有完整的异常处理； Flask中的内部异常继承的是HTTPException这个异常类，这个异常类来自Werkzeug； Werkzeug不是一个框架，它是一个 WSGI 工具集的库，你可以通过它来创建你自己的框架或 Web 应用，之前的章节中提到Web框架必须符合WSGI标准协议，而Flask就是借助Werkzeug来作为实现WSGI标准的底层库，自己再此之上构建Web框架，关系图如下： HTTPException的源码大致如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class HTTPException(Exception): code: t.Optional[int] = None description: t.Optional[str] = None def __init__( self, description: t.Optional[str] = None, response: t.Optional[&quot;Response&quot;] = None, ) -&gt; None: super().__init__() if description is not None: self.description = description self.response = response def get_body( self, environ: t.Optional[&quot;WSGIEnvironment&quot;] = None, scope: t.Optional[dict] = None, ) -&gt; str: &quot;&quot;&quot;Get the HTML body.&quot;&quot;&quot; return ( &quot;&lt;!doctype html&gt;\\n&quot; &quot;&lt;html lang=en&gt;\\n&quot; f&quot;&lt;title&gt;{self.code} {escape(self.name)}&lt;/title&gt;\\n&quot; f&quot;&lt;h1&gt;{escape(self.name)}&lt;/h1&gt;\\n&quot; f&quot;{self.get_description(environ)}\\n&quot; ) def get_headers( self, environ: t.Optional[&quot;WSGIEnvironment&quot;] = None, scope: t.Optional[dict] = None, ) -&gt; t.List[t.Tuple[str, str]]: &quot;&quot;&quot;Get a list of headers.&quot;&quot;&quot; return [(&quot;Content-Type&quot;, &quot;text/html; charset=utf-8&quot;)] def get_response( self, environ: t.Optional[t.Union[&quot;WSGIEnvironment&quot;, &quot;WSGIRequest&quot;]] = None, scope: t.Optional[dict] = None, ) -&gt; &quot;Response&quot;: if self.response is not None: return self.response if environ is not None: environ = _get_environ(environ) headers = self.get_headers(environ, scope) return WSGIResponse(self.get_body(environ, scope), self.code, headers) 可以发现其中有几个最为重要的函数，分别是get_body，get_headers， get_response；get_response返回的是一个WSGIResponse的对象，它需要传入body，code， headers。 所以如果我们想实现一个自定义的异常类，那就只需要继承HTTPException，并且实现get_body，get_headers，并支持自定义code就可以了，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738# /application/exception.pyimport jsonfrom typing import List, Tuplefrom flask import requestfrom werkzeug.exceptions import HTTPExceptionclass APIException(HTTPException): code = 500 message = 'API Exception' data = None def __init__(self, code=None, message=None, data=None): if code is not None: self.code = code if message is not None: self.message = message if data is not None: self.data = data super(APIException, self).__init__(self.message, None) def get_body(self, environ=None, scope=None) -&gt; str: body = { &quot;data&quot;: self.data, &quot;status_code&quot;: self.code, &quot;message&quot;: self.message, &quot;request&quot;: request.method + ' ' + self.get_url_without_param() } return json.dumps(body) def get_headers(self, environ=None, scope=None) -&gt; List[Tuple[str, str]]: return [('Content-Type', 'application/json')] @staticmethod def get_url_without_param() -&gt; str: full_url = str(request.full_path) return full_url.split('?')[0] 原本的HTTPException中返回的是html文本，但我们的项目是以API的方式提供服务，所以后端的返回统一是JSON字符串，因此需要重写get_headers方法，将其返回类型改为application/json，并且重写get_body方法，返回自定义的JSON字符串即可；初始化方法则改为可以接收code，message，data三个参数。 经过上述的改造就实现了一个自定义的异常类，异常类具体的使用方法如下： 先定义几个常见的异常类继承自APIException 1234567891011121314151617# /application/exception.pyclass Success(APIException): code = 200 message = &quot;success&quot; def __init__(self, data=None): super().__init__(self.code, self.message, data)class ServerError(APIException): code = 500 message = &quot;server error&quot;class DBError(APIException): code = 510 message = &quot;db error&quot; 路由函数中抛出自定义异常类 12345678910111213141516171819202122# /application/views/action.pyfrom ..exception import Success, DBError@action_blueprint.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() try: ActionORMHandler(db.session()).add(data) return Success() except Exception as e: raise DBError(message=str(e)) @action_blueprint.route(&quot;/get&quot;)def get(): args = request.args.to_dict() try: res = ActionORMHandler(db.session()).get(args) return Success(data=[item.to_dict() for item in res]) except Exception as e: raise DBError(message=str(e)) 日志大型项目除了完善的异常处理，还有一个必不可少的就是日志记录，无论是在关键的逻辑处理地方主动打印的日志，还是意料之外的异常日志都需要记录下来 首先需要将日志的对象注册到app上，代码如下： 1234567891011121314151617181920212223242526272829303132# application/__init__.pyimport loggingfrom logging.handlers import RotatingFileHandlerdef create_app(env: str = &quot;dev&quot;) -&gt; Flask: app = Flask(__name__) # register configuration # ... # register db # ... # register blueprint # ... # register logging register_logging(app) return appdef register_logging(app): formatter = logging.Formatter( '%(asctime)s %(levelname)s P[%(process)d] T[%(thread)d] %(lineno)sL@%(filename)s:' ' %(message)s') handler = RotatingFileHandler(&quot;flask.log&quot;, maxBytes=1024000, backupCount=10) handler.setLevel(app.config.get(&quot;LOG_LEVEL&quot;)) handler.setFormatter(formatter) app.logger.addHandler(handler) @app.before_request def log_each_request(): app.logger.info(f&quot;[{request.method}]{request.path} from {request.remote_addr}, params {request.args.to_dict()}, body {request.get_data()}&quot;) 上述代码中的register_logging函数中对日志做了一定的配置，包括通过formatter定义日志格式，通过RotatingFileHandler定义了根据数据大小进行切分的日志文件，并且设置了日志的打印级别，最后将日志对象添加到了app.logger上。 除此之外我们还希望将每次请求的信息记录下，诸如：请求的方法，路径，参数等；上述代码中用@app.before_request装饰的log_each_request就可以实现这个功能，该装饰器会在执行路由函数之前执行被装饰的函数，具体细节会在视频讲解中提到。 注册完日志对象后，可以通过如下方式记录日志： 12345678910111213# /application/views/action.pyfrom flask import current_app@action_blueprint.route(&quot;/add&quot;, methods=[&quot;POST&quot;])def add(): data = request.get_json() try: ActionORMHandler(db.session()).add(data) current_app.logger.success(&quot;add success&quot;) return Success() except Exception as e: raise DBError(message=str(e)) 此时访问该路由函数后，日志文件中会多一条记录： 12022-11-29 15:08:24,829 SUCCESS P[87068] T[123145357492224] 35L@action.py: add success 除了主动记录的日志之外，还有程序意外抛出的异常需要记录，那么这就需要对整个后端应用做一个try…except，这一点Flask已经考虑到了，并且也已经做了，我们只需要在这个地方使用刚才自定义的日志对象记录错误信息即可。 1234567891011121314151617181920212223242526272829303132333435# application/exception.pyimport tracebackdef register_errors(app: Flask): @app.errorhandler(Exception) def framework_error(e): app.logger.error(str(e)) app.logger.error(traceback.format_exc()) if isinstance(e, APIException): # 手动触发的异常 return e elif isinstance(e, HTTPException): # 代码异常 return APIException(e.code, e.description, None) else: if app.config['DEBUG']: raise e else: return ServerError()# application/__init__.pyfrom application.exception import register_errordef create_app(env: str = &quot;dev&quot;) -&gt; Flask: app = Flask(__name__) # register configuration # ... # register db # ... # register blueprint # ... # register logging # ... register_errors(app) return app 上下文（Context）大家应该在最近的几个章节中可以频繁的看到，request、current_app这样的变量，这个就涉及到了Flask的上下文，也是Flask中比较难理解的部分，但对于初期的使用上来说，对这个概念是否理解并不会很大的影响应用的构建。 上下文顾名思义就是与某处相关的内容，那么代码中的上下文就是指与某处代码相关的变量或对象。 上下文管理器提起上下文可能很多朋友会想到上下文管理器，这个是Python中较高级的特性，理论上属于一种语法糖；但上下文管理器和Flask中的上下文并无关联，这里只是顺便讲解一下。 我们可以通过改造SSHExecutor来给大家讲解一下如何使用上下文管理器，原本使用SSHExecutor的代码如下所示： 123ssh_executor = SSHExecutor(...)output = ssh_executor.execute(...)ssh_executor.close() 通过上下文管理器来重构的话就可以变成如下所示： 12with SSHExecutor(...) as ssh: output = ssh.execute(...) 上述代码的作用就是可以在SSHExecutor创建和结束时执行相应的操作，比如结束时我们想自动关闭连接，而不是手动调用close()方法；通过这种方式就可以将SSHExecutor的上下文管理起来，故叫做上下文管理器。 很明显with…as…作为一种语法糖并不是可以想用就用的，因为Python不可能自动的识别某个对象在创建和结束时想执行的操作，所以需要我们基于要管理的对象来实现某些方法，代码如下： 1234567891011class SSHExecutor: ... def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): if exc_type: self.logger.error(exc_type) self.logger.error(exc_val) self.logger.error(exc_tb) self.close() 如上述代码所示，执行with … as …语句时，会初始化SSHExecutor并调用__enter__方法，将该方法的返回值赋值给as后的变量（这里其实相当于直接初始化了一个SSHExecutor实例，但如果有必要还可以自定义其他操作）； 当执行完with语句块内的内容后，就会自动调用__exit__方法结束上下文管理器，exc_type、exc_val、exc_tb变量的含义分别为：异常类型、异常值、异常堆栈；这样我们就可以在结束SSHExecutor的操作之后自动调用self.close()方法关闭连接，如果with语句块中有异常也可以进行相应的处理。 Python也有内置模块contextlib中提供一些装饰器实现相关的功能，但原理都是相同的。 Flask上下文在Flask中，应用从客户端收到请求的时候，视图函数如果要处理请求，可能就要访问一些相关的对象，比如有关此次请求的各种属性，或者有关app的相关变量，这些就统称为Flask的上下文。 如果要清晰的解释Flask上下文，就必须要涉及到源码的解读，但考虑到部分源码讲解起来较难理解，反而会导致刚接触Flask的朋友更为困惑，所以我这里将上下文的原理通俗易懂的解释一下 Flask中的上下文分为两种： 请求上下文：request，session 应用上下文：current_app，g 作为全局变量不管是请求上下文还是应用上下文都是全局变量，可以直接通过import的方式引入，代码如下： 1from flask import request, session, current_app, g 全局变量的好处就是可以不用将其作为参数传递，示例如下： 1234567891011121314151617# a.pyfrom flask import request, gfrom utils import test@app.route(&quot;/index&quot;)def index(): print(request.args) g.username = 'ethan' test()# b.pyfrom flask import request, gdef test(): print(request.method) print(g.username) 如上述代码所示，在路由函数中调用utils文件中的test()函数，两个函数中都想打印请求的method，但不需要将request作为参数传递，而是分别在两个文件中直接导入request即可；current_app、g、session同理。 线程隔离请求上下文或应用上下文在处理多个的请求时互相不会干扰，因为Flask内部将其处理为线程隔离的对象，大致实现的方式可以理解为将线程ID作为字典的键，存储的上下文内容作为值，跟路由区分不同蓝图的实现原理很类似。 生命周期和存储内容请求上下文和应用上下文都是有生命周期的，他们都伴随请求创建，并在请求结束后销毁。 request：接收到请求后创建，存储了此次请求的相关信息，如请求头，请求体等 session：与request同时创建，默认加载请求中的cookies内容，否则创建一个空的session current_app：接收到请求后创建，但稍晚于request，等同于此刻的app对象，可以完美解决对app的循环引用， g：与current_app同时创建，默认没有存储值，可以用来存储自定义内容，用法如上述代码中所示， 所有的上下文都会在请求结束后销毁，但session与其他的上下文有一个不同的就是，Flask会在销毁session前默认将session的内容写入到返回体的cookie中（可以新增一个配置项：SESSION_REFRESH_EACH_REQUEST，并将其设为True，则不会去更新返回体的cookie） 总结这一章节结束后Flask大型应用的构建就已经结束了，从文件拆分，到路由蓝图，再加上环境配置、异常处理，最后完成和SSH执行器的结合。现在已经具备了将远程CLI进行服务化的基本能力。 文章中所讲的内容只是我给大家提供一个基本的思路，以此来完成各个知识点的串联，以及对大家编程思维的培养，大家不必完全局限于文章中所讲，可以结合已有的编程知识尽情发挥。","link":"/posts/9e8f3190.html"},{"title":"2.9 自动化运维初级村-巡检-文本解析-正则","text":"摘要在初级村的巡检模块涉及的章节中，我带领着大家从零开始设计规划，并且针对SSH执行器，设备和命令的增删改查，以及Flask大型应用的布局与重构，都做了非常详细的讲解；到目前为止，巡检模块已经具备雏形，但还差一个十分重要的部分——文本解析，今天及之后的几个章节我都会围绕这一部分展开讲解。 正则 vs TextFSM文本解析看似不复杂的知识，但我在整个新手村中都没有提及，并且选择在巡检模块的最后才来讲解，这是因为我对这一部分非常重视，而较为原始的文本解析正是通过正则表达式来实现，但以我个人的经历来说，想要理解并且写好正则表达式并不是一件容易的事。 很多稍微了解过自动化运维的朋友可能会问为什么不直接讲解与Netmiko有很好结合的TextFSM？ 这里我就不得不提到我出教程的宗旨——最小化上手范围，虽然我多次提及到这一点，但很多朋友其实并不理解其重要性，或者是选择性忽视。 我在自己或同行的交流群中每天都会看到有人问各种各样的问题，其中不乏关于TextFSM的，但说实话某些问题很难解答，比如：“这段输出内容我想用TextFSM匹配怎么没有结果？”，或者“我想要匹配出xxx应该怎么写TextFSM？”等等。结果解释到最后，提问者连最基本的正则匹配都没有概念，正所谓“授人以鱼不如授人以渔”，别人是没有办法穷举解决所有模版的，只能将原理解释清楚，剩下靠“自悟”。 我所推荐的“快速上手”是指尽量压缩上手成本，绝不是copy一份代码，改改运行起来就叫“上手”，而且我觉得这样做也并没有什么意义。 正则表达式与TextFSM之间并没有孰优孰劣，而是相辅相成。 正则具有灵活的匹配方式，而且是TextFSM的底层原理；而TextFSM有更加系统的匹配模式，更适合作为解析模版。 综上，我想表达的是正则表达式的学习其实是TextFSM的上手前提，TextFSM其实是合理运用正则的一种高级封装，我们要学习如何使用他，更要了解其原理，才能更进一步的体会其思想；切不可囫囵吞枣，事倍功半。 正则表达式（Regexp）正则匹配是一种文本匹配的模式，正则表达式则是用来描述这种匹配的一串字符。 百度百科对正则表达式的定义是这样的： 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。 在不同的编程语言中，对于正则表达式的编写都会有些微的差别，但其基本的逻辑和常用的匹配字符都是通用的。 正则表达式的一大特点就是灵活，但最大的缺点就是晦涩，对于新手朋友来说学习正则表达式的过程都会相对比较痛苦，但我会尽量选择性的讲解必要的知识，并通过例子来让大家尽快熟悉。 在之前的章节中的我们有讲到Netmiko执行完命令后获取输出结果有两种模式（基于模式/时间），其中基于模式匹配的机制（send_command）就是运用了正则匹配的方法，通过判断输出的结果是否包含search_pattern来决定是否输出完成，这里的包含并不是指字面上的字符串是否包含，而是使用了正则匹配来判断，所以大家也可以在执行send_command的时候，将expect_string传成正则表达式。 讲到此处，大家对于正则表达式都只是有一个模糊的概念，下面我举一个例子： 假如在程序中想验证一下某个字符串是否为合法的IPv4地址格式，有很多种方法，可以通过纯代码的方式实现 12345678910def valid_ipv4(ip): segments = ip.split(&quot;.&quot;) if len(segments) != 4: # 如果不是四段，则非法 return False for seg in segments: if not seg.isdigit(): # 如果某一段不是0或正整数，则非法 return False if int(seg) &gt; 255: # 如果大于255，则非法 return False return True # 合法 也可以通过正则表达式来进行匹配，一般写正则表达式的思路都是循序渐进的； 首先IPv4地址分为四段，每一段都是0-255的数字，段和段之间由点号隔开，所以难点在于如何匹配0-255。 字符集合匹配表达式正则表达式中，可以通过字符集合来表示匹配集合中的任意一个字符，例如： “[abc]”可以匹配目标字符串中的“a”或者“b”或者“c”，“[0-9]”可以匹配目标字符串中任意一个0-9范围内的数字。 那么可不可以用“[0-255]”直接匹配呢？答案是不可以，因为字符集合的中括号内必须由一个一个的字符组成，短横线只是一种简写，比如“[a-z]”或者“[A-Z]”，假如想匹配全部的大小写字母，就必须是“[a-zA-Z]”。显然“[0-255]”只能匹配四个数字（0、1、2、5）。 特殊字符匹配表达式正则表达式中，存在一些特殊字符来表示匹配多个字符中的任意一个字符，可以认为是字符集合的特殊版，例如： “.”可以匹配除了\\r和\\n之外的任意字符。 “\\d”可以匹配任意一个数字，等价于“[0-9]”。 “\\w”可以匹配任意一个单词字符和下划线，类似于“[_a-zA-Z]”，但不完全相同，“\\w”可以匹配Unicode中的任意一个字符，也包括中文，大家可以简单的理解为可以匹配任意一个字符即可。 “\\s”可以匹配任何不可见字符，包括空格、制表符、换页符等等。等价于“[ \\f\\n\\r\\t\\v]”（注意中括号里有一个空格）。 另外还有“\\D”“\\W”“\\S”，均为以上小写特殊字符的反向匹配。 那么可不可以用特殊字符来进行匹配，比如“\\d\\d\\d”？答案是不完全可以。如果在确认此处一定会出现IPv4地址，只是想将其过滤出来的话可以用较为粗粒度（不怎么精确）的正则表达式对其进行匹配； 如果用不精确的匹配的话除了用“\\d”，还可以用“\\S\\S\\S”来表示三个任意可见字符。 表达式重复特殊字符里，我们用了三个相同的符号表示三个数字，在正则中可以用简单的方式来表示表达式的重复次数，例如： “?”可以匹配前面的子表达式零次或一次，比如“\\d?”，表示零个或一个数字；该字符还有非贪婪匹配的含义，此处暂且不提，感兴趣的朋友可以自行研究。 “”可以匹配前面的子表达式任意次，比如“\\d”，表示零个或任意多个连续的数字。 “+”可以匹配前面的子表达式至少一次，比如“\\d+”，表示一个及以上任意多个连续的数字。 “{n}”可以匹配前面的子表达式n次，比如“\\d{3}”，表示三个连续的数字。 “{n,}”可以匹配前面的子表达式至少n次，比如“\\d{3,}”，表示三个或更多个连续的数字。 “{n,m}”可以匹配前面的子表达式至少n次，至多m次，比如“\\d{1,2}”，表示一个或两个连续的数字。 其他除了上述几个分类外，还有几个匹配表达式平时会需要用到，例如： “|”表示或，比如“x|y”，表示匹配x或y字符 “\\”表示转义，比如“.”，表示匹配目标字符串中的点，而非代表正则表达式中的任意字符符号 “(pattern)”表示获取匹配，比如“(\\d{3}.){4}”，表示匹配连续三个数字加一个点号，并重复四次，并会在返回的group结果中获取到连续的三个数字，在不同的编程语言中有不同的获取方法。 “(?:pattern)”表示非获取匹配，比如“(?:\\d{3}.){4}”，表示匹配连续三个数字加一个点号，并重复四次。 “^”表示匹配行首，比如“^\\d”，表示目标字符串的开头必须是数字（但匹配的行为是否设置了多行匹配的模式有关，具体不做展开）。 “$”表示匹配行尾 匹配IPv4经过上述知识点的讲解，大家应该可以写出匹配IPv4的表达式了，我在下面列举几种从不精确到完全精确的写法： “(?:\\d+.){3}\\d+” “(?:\\d{0,3}.){3}\\d{0,3}” “(?:(?:[1-9]?\\d|1\\d\\d|2[0-4]\\d|25[0-5]).){3}(?:[1-9]?\\d|1\\d\\d|2[0-4]\\d|25[0-5])” 大多数情况下，在解析文本时，只需要使用第二条正则便可以过滤出IPv4，但在验证用户输入的时候，则有必要使用第三种最为精确的匹配。 具体上述三个表达式的讲解我会在视频中提到。 Python正则表达式上文中所讲的正则表达式都是针对所有编程语言都适用的，但一直有提到“匹配”这个词，究竟如何匹配，是在不同的编程语言中有不同方法的。 Python中进行正则匹配需要引入一个模块re，这是一个内置模块，不需要安装；该模块提供了很多的函数用于做正则匹配或正则替换，以及具备一些特殊的常量实现额外的匹配功能，但我们这一章节并不会讲解，我只把几个最常用的给大家解释一下，让大家先初步了解如何使用Python进行正则匹配。 表达式写法Python中的正则表达式通常都写为r''，大家可以先不用深究为什么这么写，只需要知道这样写可以避免一些不必要的转义即可。 正则匹配函数re模块中有几个常见的正则匹配函数，分别是match，search，findall，正则匹配函数会将匹配到的结果返回。 re.match该函数的功能是使用正则表达式从目标字符串的开头去匹配，例如： 1234567891011import reres = re.match(r'(?:\\d{0,3}\\.){3}\\d{0,3}', &quot;127.0.0.1&quot;)print(res)# &lt;re.Match object; span=(0, 9), match='127.0.0.1'&gt;print(res.group())# 127.0.0.1res = re.match(r'(?:\\d{0,3}\\.){3}\\d{0,3}', &quot;a127.0.0.1&quot;)print(res.group())# AttributeError: 'NoneType' object has no attribute 'group' 上述例子中re.match返回一个Match对象，通过.group()方法可以获取到匹配结果，由于第二个匹配中目标字符串的开头有一个字母，所以匹配的结果为None，调用.group()方法时就抛出了AttributeError异常。 re.Match对象有很多个方法和属性，其中最为常用的就是.group()，表示获取匹配结果的第一个分组，关于分组的详细原理大家可以自行了解，文章中先不做详细的解释。 re.search该函数的功能和re.match基本相同，但并不要求必须从目标字符串的开头匹配，例如： 123res = re.search(r'(?:\\d{0,3}\\.){3}\\d{0,3}', &quot;a127.0.0.1a127.0.0.3&quot;)print(res.group())# 127.0.0.1 re.fullmatch该函数的功能和re.match基本相同，但要求字符串必须完整匹配，即头尾也匹配，这里就不做举例了 re.findall上述三个函数都只会返回一个匹配项，但大多数情况我们想要匹配一个文本中的内容时，都会包含一个及以上的结果，比如show ip interface的输出结果，其中就会包含多个IP地址，这时候就要使用findall函数，例如： 123res = re.findall(r'(?:\\d{0,3}\\.){3}\\d{0,3}', &quot;a127.0.0.1a127.0.0.3&quot;)print(res)# ['127.0.0.1', '127.0.0.3'] 该函数匹配过程类似于re.search，但会返回多个结果，且以列表的形式返回。 总结这一章节主要是给大家做一下正则表达式的科普，本身正则表达式的编写并不会特别复杂，但需要掌握的知识点较为零碎，所以我希望尽可能的精简，并且通过匹配IP的例子将其串联起来，让大家尽快熟悉正则表达式，也能为后续学习TextFSM打好基础。","link":"/posts/82c9e913.html"},{"title":"2.10 自动化运维初级村-巡检-文本解析-从正则到TextFSM","text":"摘要上一章节中相信大家已经对正则表达式有了基本的概念，正如我之前提到的，单纯看完知识点的介绍只能算作“记忆”，如果没有实践，就算把正则表达式全部的规则都背会了，也不能算作“学会”。今天这一章节我会带领大家实践一下如何针对实际的文本编写正则表达式。 但为什么标题是“文本解析-从正则到TextFSM”呢，因为有很多朋友一开始就想从TextFSM上手，但TextFSM究竟比正则强在哪儿？TextFSM所遵循的状态转移和匹配机制究竟是如何产生又是如何运用的？ 在我带领大家实践正则表达式的过程中，大家会发现如此灵活的正则匹配的短板在哪里？而这些短板恰恰就是催生TextFSM的重要因素。 简单文本匹配我们以Cisco设备上执行“show clock”的输出为原始文本，由于输出文本足够简单，非常适合作为第一个例子来讲解，输出如下： 118:42:41.321 CST Sun Jan 1 2023 现在想根据上述输出内容匹配几个关键信息，分别是：时间、时区、月份、日、年。 通过正则表达式来匹配的话会有好几种方式，我这里提供一个示例如下： 1234567import restdout = &quot;18:42:41.321 CST Sun Jan 1 2023&quot;# 括号表示获取匹配，最终会将括号内结果返回regexp = r'(..:..:..\\....) (\\w+) \\w+ (\\w+) (\\d+) (\\d+)'result = re.findall(regexp, stdout)print(result)# 输出结果 [('18:42:41.321', 'CST', 'Jan', '1', '2023')] 对于从一串文本中想要匹配指定的几个关键信息，通过正则表达式更为方便。 但有一个非常显而易见的缺陷，那就是我们通过“获取匹配”的方式来筛选匹配关键信息，如果一串文本中的关键信息有很多个，那正则表达式就会变得非常冗长，且最终较难区分哪个匹配结果对应哪个信息。比如上面例子中想要返回可读的结构化数据通常还需要做如下处理： 1234567res_dict = { &quot;time&quot;: result[0][0], &quot;timezone&quot;: result[0][1], &quot;month&quot;: result[0][2], &quot;day&quot;: result[0][3], &quot;year&quot;: result[0][4],} 所以到目前为止，根据一个简单的示例可以延伸分析得出，我们需要一个机制能够标识匹配表达式所对应的信息字段名称；但光有这一点，还不足以让编程人员弃用表达式，因为关键信息再多也是有限的，再不济也就是多写几个下标索引罢了。 多行文本匹配如果输出内容是多行文本时，匹配的复杂度又会有一个新的提升，比如Cisco设备执行“show version”的输出如下： 12345678910111213141516171819202122232425262728293031323334353637383940Cisco IOS Software, Catalyst 4500 L3 Switch Software (cat4500-ENTSERVICESK9-M), Version 12.2(31)SGA1, RELEASE SOFTWARE (fc3)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2007 by Cisco Systems, Inc.Compiled Fri 26-Jan-07 14:28 by kellythwImage text-base: 0x10000000, data-base: 0x118AD800ROM: 12.2(31r)SGAPod Revision 0, Force Revision 34, Gill Revision 20router.abc uptime is 11 weeks, 4 days, 20 hours, 26 minutesSystem returned to ROM by reloadSystem restarted at 22:49:40 PST Tue Nov 18 2008System image file is &quot;bootflash:cat4500-entservicesk9-mz.122-31.SGA1.bin&quot;This product contains cryptographic features and is subject to UnitedStates and local country laws governing import, export, transfer anduse. Delivery of Cisco cryptographic products does not implythird-party authority to import, export, distribute or use encryption.Importers, exporters, distributors and users are responsible forcompliance with U.S. and local country laws. By using this product youagree to comply with applicable laws and regulations. If you are unableto comply with U.S. and local laws, return this product immediately.A summary of U.S. laws governing Cisco cryptographic products may be found at:http://www.cisco.com/wwl/export/crypto/tool/stqrg.htmlIf you require further assistance please contact us by sending email toexport@cisco.com.cisco WS-C4948-10GE (MPC8540) processor (revision 5) with 262144K bytes of memory.Processor board ID FOX111700ZNMPC8540 CPU at 667Mhz, Fixed ModuleLast reset from Reload2 Virtual Ethernet interfaces48 Gigabit Ethernet interfaces2 Ten Gigabit Ethernet interfaces511K bytes of non-volatile configuration memory.Configuration register is 0x2102 现在我们的目标仍然是获取几个关键信息，包括：版本号、启动时长、镜像文件、重置原因。 虽然需要从多行文本中去匹配信息，但仍然可以通过正则表达式来实现，代码如下： 1234567891011import restdout = &quot;output of show version&quot;regexp = r'Cisco IOS .*Version (\\S+),.*uptime is (.*?)\\n.*System image file is &quot;(.*)&quot;.*Last reset from (\\w+)'result = re.findall(regexp, stdout, re.DOTALL)print(result)&quot;&quot;&quot; 输出结果 [( '12.2(31)SGA1', '11 weeks, 4 days, 20 hours, 26 minutes', 'bootflash:cat4500-entservicesk9-mz.122-31.SGA1.bin', 'Reload')]&quot;&quot;&quot; 上述的代码有两个需要注意的地方： re.findall函数中多传了一个参数，re.DOTALL，表示“.”可以匹配任何字符，包括换行符（默认不包括）。 正则表达式中的“.*?”，因为加了re.DOTALL参数，所以“.*”可以匹配任意多个字符（贪婪匹配特性会导致尽可能多的匹配字符），这样会导致不能准确的匹配到uptime所在行尾的\\n，但加了?，会将贪婪匹配转为非贪婪匹配，此时就会尽可能少的匹配字符。 虽然可以成功匹配出结果，但大家应该已经发现，多行文本的匹配中会存在更多因素影响匹配结果的准确性，所以以我个人经验来看，尽量不用正则表达式来匹配多行文本。 那是否可以改变匹配方式来进行匹配呢，代码如下： 123456789101112131415161718import restdout = &quot;output of show version&quot;version_regexp = r'^Cisco IOS .*Version (\\S+),'uptime_regexp = r'.*uptime is (.*)'image_regexp = r'System image file is &quot;(.*)&quot;'reset_regexp = r'Last reset from (\\w+)'regexps = [version_regexp, uptime_regexp, image_regexp, reset_regexp]result = []for regexp in regexps: res = re.findall(regexp, stdout) result.append(res[0] if len(res) &gt; 0 else &quot;unknown&quot;)print(result)&quot;&quot;&quot; 输出结果 [( '12.2(31)SGA1', '11 weeks, 4 days, 20 hours, 26 minutes', 'bootflash:cat4500-entservicesk9-mz.122-31.SGA1.bin', 'Reload')]&quot;&quot;&quot; 通过稍加改造后，将匹配不同信息的正则表达式分开定义，然后每个表达式匹配一次完整的字符串，最终就可以得出所有的匹配结果；但理论上已经匹配过的内容实际上不需要再次被匹配了，而且仍存在准确性问题，因为某个关键信息的正则表达式是对某一行进行匹配的，所以如果用该正则去匹配全文，可能会出现相似内容导致结果不准确，所以可以将文本内容分成一行行再去进行匹配，如下： 12345678910111213141516import restdout = &quot;output of show version&quot;version_regexp = r'^Cisco IOS .*Version (\\S+),'uptime_regexp = r'.*uptime is (.*)'image_regexp = r'^System image file is &quot;(.*)&quot;'reset_regexp = r'^Last reset from (\\w+)'regexps = [version_regexp, uptime_regexp, image_regexp, reset_regexp]result = []for line in stdout.split(&quot;\\n&quot;): for regexp in regexps: res = re.findall(regexp, line) if not res: continue result.append(res[0]) breakprint(result) 此时就可以正则表达式前加一个“^”来表示匹配行首，这样可以大大增加匹配结果的准确性。 这里我希望大家可以着重理解上述代码的逻辑，因为这个逻辑其实已经和TextFSM的基本匹配逻辑非常相似，那就是：定义多个匹配规则，一行一行的读取文本，用该行去依次匹配每个正则，如果匹配到则读取新行，再次去和所有规则进行匹配。 复杂内容的匹配简单文本和多行文本都是针对关键字信息的提取，虽然可以总结出一些优化逻辑，但整体上匹配过程相对简单，所以仍没有办法说服我们弃用正则，而选择TextFSM。 因为一旦引入一个新的第三方包，就意味着增加了更多的学习成本，而且会由于对于使用方法和原理的理解不够清晰，而导致增加更多的不稳定因素。 那么现在除了上述的信息提取外，还有一种在网络设备非常常见的输出内容，那就是表格类型的输出，比如“show ip route”的输出内容，如下： 12345678910 Destination Gateway Dist/Metric Last Change ----------- ------- ----------- -----------B EX 0.0.0.0/0 via 192.0.2.73 20/100 4w0d via 192.0.2.201 via 192.0.2.202 via 192.0.2.74B IN 192.0.2.76/30 via 203.0.113.183 200/100 4w2dB IN 192.0.2.204/30 via 203.0.113.183 200/100 4w2dB IN 192.0.2.80/30 via 203.0.113.183 200/100 4w2dB IN 192.0.2.208/30 via 203.0.113.183 200/100 4w2d 上述的输出增加了一些难处理的内容，比如表头的多余内容，表格的结构，以及某个列中可能会包含多个值。 我们可以仍然尝试使用正则来进行处理，代码如下： 1234567891011121314151617import restdout = &quot;output of show ip route&quot;regexp = r'(\\w) (\\w+) (\\S+)\\s+via (\\S+)\\s+(\\d+)/(\\d+)\\s+(\\S+)'result = []for line in stdout.split(&quot;\\n&quot;): res = re.findall(regexp, line) if not res: continue result.append(res[0])print(result)&quot;&quot;&quot; 输出内容 [ ('B', 'EX', '0.0.0.0/0', '192.0.2.73', '20', '100', '4w0d'), ('B', 'IN', '192.0.2.76/30', '203.0.113.183', '200', '100', '4w2d'), ('B', 'IN', '192.0.2.204/30', '203.0.113.183', '200', '100', '4w2d'), ('B', 'IN', '192.0.2.80/30', '203.0.113.183', '200', '100', '4w2d'), ('B', 'IN', '192.0.2.208/30', '203.0.113.183', '200', '100', '4w2d')]&quot;&quot;&quot; 通过代码的处理貌似可以提取出部分的信息，但仍然有最为关键的内容被错过了，那就是Gateway列的多个值。 最终想要的结果应该是Gateway列最终匹配到的值是一个列表，可以包含多个值，那么代码改造如下： 12345678910111213141516171819202122232425import restdout = &quot;output of show ip route&quot;regexp = r'(\\w) (\\w+) (\\S+)\\s+via (\\S+)\\s+(\\d+)/(\\d+)\\s+(\\S+)'gateway_regexp = r'\\s+via (\\S+)'result = []for line in stdout.split(&quot;\\n&quot;): res = re.findall(regexp, line) if res: route = list(res[0]) route[3] = [route[3]] result.append(route) continue col = re.findall(gateway_regexp, line) if not col: continue if len(result) &gt; 0: result[-1][3].append(col[0])print(result)&quot;&quot;&quot;输出内容 [ ['B', 'EX', '0.0.0.0/0', ['192.0.2.73', '192.0.2.201', '192.0.2.202', '192.0.2.74'], '20', '100', '4w0d'], ['B', 'IN', '192.0.2.76/30', ['203.0.113.183'], '200', '100', '4w2d'], ['B', 'IN', '192.0.2.204/30', ['203.0.113.183'], '200', '100', '4w2d'], ['B', 'IN', '192.0.2.80/30', ['203.0.113.183'], '200', '100', '4w2d'], ['B', 'IN', '192.0.2.208/30', ['203.0.113.183'], '200', '100', '4w2d']]&quot;&quot;&quot; 上述代码中定义了两个正则语句，并且在匹配过程中进行了额外的处理，虽然最终得到了正确的结果，但实现的过程也是非常“hack”，这种情况显然不是我们想要的。 总结虽然上述三个例子都通过正则表达式+代码的逻辑完成了输出内容的结构化处理，但有几个地方是不符合预期的： 匹配规则和结果缺少字段标识 处理多行文本+多个正则表达式时引入的额外代码逻辑 处理表格类型文本+多值字段时引入的额外代码逻辑 讲到这里我仍然想说的是，上述的三个缺点在处理有限的内容时也并不足以让我们弃用正则表达式，而引入TextFSM，比如某个简单的需求只想要解析指定几个输出内容，那么解析的逻辑和规则时可以穷举的，只需要将对应输出内容的解析方法封装成相应的函数，然后直接的调用即可。 但在我们的巡检场景或者更为大型的系统中，要处理的输出内容是无法穷举的，随时会有可能新增某个巡检项，如果所有的解析逻辑都通过封装函数来一一对应处理，那么这种方式并不符合程序设计的原则。 我们的预期是想要通过某种方式来实现解析逻辑的抽象化，只需要维护解析模版即可，将解析模版以文件的形式保存或者存储在数据库中，这样就可以随时新增某个模版来应对需要处理的内容，而没有必要改动代码逻辑。而TextFSM正好可以帮助我们实现，而在我看来这才是需要引入TextFSM的真正原因。 如果大家只是想要写某个脚本来处理有限的输出内容而引入TextFSM，那显然是脱离了“最小化上手原则”，并且我希望大家能够学会根据需求合理的选择工具的使用。","link":"/posts/94cbed9d.html"},{"title":"2.12 自动化运维初级村-巡检-TextFSM-2","text":"摘要经过前面两个章节的学习，我们已经初步掌握了正则表达式的原理，并从正则解析一步一步过渡到了 TextFSM，而且 TextFSM 在匹配机制上和对于大型项目的重要性上都让我们非常有必要将他集成到巡检模块中。 那么这一章节我们就主要讲解一下如何在 Python 中使用 TextFSM，以及如何将其集成到巡检框架中。 Python+TextFSMPython 中使用 TextFSM 需要安装一个第三方包，执行下列命令即可： 1pip install textfsm 模板解析上一个章节中我们已经举了三个使用 TextFSM 模板进行匹配的例子，但并没有演示在 Python 中如何使用模板进行文本解析。 而且大家最为熟知的 TextFSM 使用方法都是直接将其和 Netmiko 等模块结合使用，这种方式也很方便，我们后续同样会使用；但在此之前，我想说的是，TextFSM 本质就是一个模板解析的第三方库，所以肯定是可以用来解析任何文本的，不光局限于网络设备的输出；所以我先给大家讲解一下 Python 中是如何直接使用 TextFSM 进行解析的。 用模版文件解析首先将模版内容保存在单独的文件中，如下： 12345678910111213141516# route.tmplValue Protocol (\\S)Value Type (\\S\\S)Value Required Prefix (\\S+)Value List Gateway (\\S+)Value Distance (\\d+)Value Metric (\\d+)Value LastChange (\\S+)Start ^.*----- -&gt; RoutesRoutes ^\\s\\s\\S\\s\\S\\S -&gt; Continue.Record ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s+via ${Gateway} 这里需要说明的是，我经过测试模版文件的后缀并没有强制要求，之所以保存为 .tmpl 后缀，只是为了更为方便的识别该文件为模板文件。 Python 代码如下： 123456789101112131415161718192021222324import jsonfrom textfsm import TextFSMdef parse_from_file(): stdout = &quot;&quot;&quot; Destination Gateway Dist/Metric Last Change ----------- ------- ----------- ----------- B EX 0.0.0.0/0 via 192.0.2.73 20/100 4w0d via 192.0.2.201 via 192.0.2.202 via 192.0.2.74 B IN 192.0.2.76/30 via 203.0.113.183 200/100 4w2d B IN 192.0.2.204/30 via 203.0.113.183 200/100 4w2d B IN 192.0.2.80/30 via 203.0.113.183 200/100 4w2d B IN 192.0.2.208/30 via 203.0.113.183 200/100 4w2d&quot;&quot;&quot; with open(&quot;route.tmpl&quot;, &quot;r+&quot;) as f: fsm = TextFSM(f) res = fsm.ParseTextToDicts(stdout) print(json.dumps(res, indent=2))if __name__ == '__main__': parse_from_file() 上述代码中从 textfsm 库中引入了 TextFSM 类，之后将 “route.tmpl” 模板经过 open() 打开后得到一个文件对象，将其传入 TextFSM 类中，得到一个 TextFSM 的实例化对象。 该对象有两个方法是最常使用的，分别是 ParseText() 和 ParseTextToDicts()，这两个方法解析的过程都是一样的，不同的就是呈现结果的结构不一样，前者是以列表的形式返回解析结果，后者是以字典的形式。 通常使用的是后者，因为可以清晰的知道哪个值对应哪个字段，更有利于解析结果处理的准确性和代码的健壮性。 这里我将解析结果用 json 模块处理了一下，更方便打印输出展示，上述代码的输出结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[ { &quot;Protocol&quot;: &quot;B&quot;, &quot;Type&quot;: &quot;EX&quot;, &quot;Prefix&quot;: &quot;0.0.0.0/0&quot;, &quot;Gateway&quot;: [ &quot;192.0.2.73&quot;, &quot;192.0.2.201&quot;, &quot;192.0.2.202&quot;, &quot;192.0.2.74&quot; ], &quot;Distance&quot;: &quot;20&quot;, &quot;Metric&quot;: &quot;100&quot;, &quot;LastChange&quot;: &quot;4w0d&quot; }, { &quot;Protocol&quot;: &quot;B&quot;, &quot;Type&quot;: &quot;IN&quot;, &quot;Prefix&quot;: &quot;192.0.2.76/30&quot;, &quot;Gateway&quot;: [ &quot;203.0.113.183&quot; ], &quot;Distance&quot;: &quot;200&quot;, &quot;Metric&quot;: &quot;100&quot;, &quot;LastChange&quot;: &quot;4w2d&quot; }, { &quot;Protocol&quot;: &quot;B&quot;, &quot;Type&quot;: &quot;IN&quot;, &quot;Prefix&quot;: &quot;192.0.2.204/30&quot;, &quot;Gateway&quot;: [ &quot;203.0.113.183&quot; ], &quot;Distance&quot;: &quot;200&quot;, &quot;Metric&quot;: &quot;100&quot;, &quot;LastChange&quot;: &quot;4w2d&quot; }, { &quot;Protocol&quot;: &quot;B&quot;, &quot;Type&quot;: &quot;IN&quot;, &quot;Prefix&quot;: &quot;192.0.2.80/30&quot;, &quot;Gateway&quot;: [ &quot;203.0.113.183&quot; ], &quot;Distance&quot;: &quot;200&quot;, &quot;Metric&quot;: &quot;100&quot;, &quot;LastChange&quot;: &quot;4w2d&quot; }, { &quot;Protocol&quot;: &quot;B&quot;, &quot;Type&quot;: &quot;IN&quot;, &quot;Prefix&quot;: &quot;192.0.2.208/30&quot;, &quot;Gateway&quot;: [ &quot;203.0.113.183&quot; ], &quot;Distance&quot;: &quot;200&quot;, &quot;Metric&quot;: &quot;100&quot;, &quot;LastChange&quot;: &quot;4w2d&quot; }] 用模板字符串解析目前大家常见的都是上述将模板保存到文件中，然后去解析，但实际项目中模板的内容并不一定是在文件中，因为如果要将解析过程集成到巡检功能中的话，就需要将模板的管理线上化，而通过服务器文件形式进行管理并不是很好的选择，在多进程的情况下可能会出现多个解析线程都去使用同一个模板解析内容，这时候还需要考虑文件锁。 那么既然 TextFSM 模板是字符串，那理所当然我们可以将它保存到数据库中进行管理，这样也更有利于更删改查。 所以当从数据库中读取到 TextFSM 模板后，模板内容在程序中实际是一个字符串，那这时候怎么进行解析呢？ 从模板文件中解析的时候，初始化 TextFSM 对象需要传入一个打开的文件对象，那么我们可以从这里做文章。 计算机读取一个文件后，实际上是将该文件放在了内存中，供后续的程序使用，那么我们只需要传入一个内存中的文件流，就可以让 TextFSM 解析了。 在 Python 中，StringIO 可以将字符串转化为内存中的 IO 流，具备和使用 open() 方法打开文件后获取到的文件对象几乎相同的属性和方法，所以代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940import jsonfrom io import StringIOfrom textfsm import TextFSMtemplate = &quot;&quot;&quot;Value Protocol (\\S)Value Type (\\S\\S)Value Required Prefix (\\S+)Value List Gateway (\\S+)Value Distance (\\d+)Value Metric (\\d+)Value LastChange (\\S+)Start ^.*----- -&gt; RoutesRoutes ^\\s\\s\\S\\s\\S\\S -&gt; Continue.Record ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s+via ${Gateway}&quot;&quot;&quot;stdout = &quot;&quot;&quot; Destination Gateway Dist/Metric Last Change ----------- ------- ----------- ----------- B EX 0.0.0.0/0 via 192.0.2.73 20/100 4w0d via 192.0.2.201 via 192.0.2.202 via 192.0.2.74 B IN 192.0.2.76/30 via 203.0.113.183 200/100 4w2d B IN 192.0.2.204/30 via 203.0.113.183 200/100 4w2d B IN 192.0.2.80/30 via 203.0.113.183 200/100 4w2d B IN 192.0.2.208/30 via 203.0.113.183 200/100 4w2d&quot;&quot;&quot;def parse_from_str(): fsm = TextFSM(StringIO(template)) res = fsm.ParseTextToDicts(stdout) print(json.dumps(res, indent=2))if __name__ == '__main__': parse_from_str() 上述代码中，我们把模板定义成了一个变量，并且在初始化 TextFSM 类的时候传入了经过 StringIO() 处理的文件流对象，这样同样可以完成解析，而且调试起来也非常方便。 输出结果与刚才完全相同。 ntc-template的使用在开源项目中，ntc-template 这个项目将知名厂商的最为常用的交换机命令的输出结果进行了解析，并开源了对应的解析模版（值得大家致敬一下）。 这个库的好处就是只需要传入（厂商、命令、输出）就可以返回解析的结果，但如果要解析的命令在库中还没有对应的模板，那就需要做一些定制化的处理，具体的流程如下： 将 ntc-templates 库中的 templates 目录拷贝到你的项目中 在环境变量 NTC_TEMPLATES_DIR 改为你上一步的自定义目录（可以在系统中更改或者在代码中更改） 在第一步的目录中添加自己的解析模板，并且添加相应规则到 index 文件中，格式如下： 1“{解析模板名称}, .*, {设备的平台}, {执行的命令}” 但我个人并不推荐这种方式，首先是文件形式存储模板的缺点，我们在上文中已经提到了，另外就是每次添加自己的模版都要修改 index 文件并不是很友好；但考虑到内容的完整性，所以有必要进行上述步骤的介绍，对以上方式更为认可的朋友可以采用。 Netmiko中结合 ntc-template在Netmiko中同样集成了解析功能，包括 TTP、TextFSM、Genie三种，并且在 TextFSM 方式里将 ntc-template 集成了进去，所以我们可以直接将执行命令完输出的结果进行解析，并返回结构化的内容，示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import jsonfrom netmiko import ConnectHandlerdef connect(params, command): conn = ConnectHandler(**params) output = conn.send_command(command, use_textfsm=True) print(json.dumps(output, indent=2))if __name__ == '__main__': params = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot; } command = &quot;show version&quot; connect(params, command)&quot;&quot;&quot; output[ { &quot;version&quot;: &quot;&quot;, &quot;rommon&quot;: &quot;Bootstrap&quot;, &quot;hostname&quot;: &quot;r1&quot;, &quot;uptime&quot;: &quot;22 weeks, 23 hours, 23 minutes&quot;, &quot;uptime_years&quot;: &quot;&quot;, &quot;uptime_weeks&quot;: &quot;22&quot;, &quot;uptime_days&quot;: &quot;&quot;, &quot;uptime_hours&quot;: &quot;23&quot;, &quot;uptime_minutes&quot;: &quot;23&quot;, &quot;reload_reason&quot;: &quot;Unknown reason&quot;, &quot;running_image&quot;: &quot;/opt/unetlab/addons/iol/bin/i86bi-linux-l3-adventerprisek9-15.4&quot;, &quot;hardware&quot;: [], &quot;serial&quot;: [ &quot;67108896&quot; ], &quot;config_register&quot;: &quot;0x0&quot;, &quot;mac&quot;: [], &quot;restarted&quot;: &quot;&quot; }]&quot;&quot;&quot; 上述代码中我们只需要在调用 send_command 时传入 use_textfsm=True 即可开启TextFSM的解析功能，本质上就是在获取到命令输出之后调用 ntc-template 进行了解析。 巡检解析Netmiko + TextFSM上文中介绍了 Netmiko 中内置了 TextFSM 解析的功能，本质上是调用了 ntc-template 模块，但由于 ntc-template 的自定义方式不够灵活，所以并不适合集成到我们的巡检功能中，因此需要结合「模版字符串解析」的方式进行适当的改造，代码如下： 12345678910111213141516171819202122232425262728293031import jsonfrom io import StringIOfrom typing import List, Dictfrom textfsm import TextFSMfrom netmiko import ConnectHandlerdef parse_from_str(output: str, template: str) -&gt; List[Dict]: fsm = TextFSM(StringIO(template)) res = fsm.ParseTextToDicts(output) return resdef connect(params: Dict, command: str, template: str): conn = ConnectHandler(**params) output = conn.send_command(command) res = parse_from_str(output, template) print(json.dumps(res, indent=2))if __name__ == '__main__': params = { &quot;device_type&quot;: &quot;cisco_ios&quot;, &quot;host&quot;: &quot;192.168.31.149&quot;, &quot;username&quot;: &quot;cisco&quot;, &quot;password&quot;: &quot;cisco&quot; } template = &quot;&quot;&quot;模版内容从数据库中读取&quot;&quot;&quot; command = &quot;show version&quot; connect(params, command, template) 上述代码中，在执行完 send_command 后，将输出内容和模板变量一起传入到 parse_from_str 方法中即可；该方法与直接使用 send_command 方法的解析功能没有本质的区别，因为底层都是用了 TextFSM进行解析，但不同的是获取模板的方式，改为了可以被灵活处理，这里为了更好的与巡检功能结合，后续我们会从数据库中获取模板内容。 总结这一章节主要介绍了如何在 Python 中使用 TextFSM，并且根据逐步的过渡，已经可以初步看出如何将其集成到我们的巡检功能中了，下一章节就正式让巡检模块具备解析功能，并将其完整的运行起来。","link":"/posts/af22ed7b.html"},{"title":"2.11.0 自动化运维初级村-巡检-TextFSM","text":"摘要上一章节中，我已经给大家演示了如何使用正则来应对常见的几种文本解析场景，但使用正则解析的过程中发现了几个明显的痛点，如果要在我们初级村中设计的巡检模块中补齐文本解析的功能，那么正则是无法担此重任的，所以有必要引入新的文本解析方式，那就是-TextFSM。 TextFSM简介首先正则解析的痛点我们已经总结了出来，如下： 匹配规则和结果缺少字段标识 处理多行文本+多个正则表达式时引入的额外代码逻辑 处理表格类型文本+多值字段时引入的额外代码逻辑 TextFSM本质上就是一个文本解析的工具包，谷歌的工程师研发出了TextFSM这个工具包就是为了解决解析网络设备输出过程中的痛点，TextFSM的优势如下： 将待提取的信息定义为变量 内置匹配多行文本的逻辑，减轻编程负担 通过内置的状态和动作逻辑，灵活的解析表格类型文本 抽象匹配逻辑，让模版只做“模版” TextFSM详解这一章节同样以上个章节的示例进行演示，大家可以体会一下同样的场景使用TextFSM相比正则是否更为方便。 简单文本解析以匹配Cisco设备上执行“show clock”的输出为例子，使用TextFSM模版来匹配 118:42:41.321 CST Sun Jan 1 2023 现在想根据上述输出内容匹配几个关键信息，分别是：时间、时区、月份、日、年。 使用TextFSM进行解析需要定义一个模版，如下： 12345678Value Year (\\d+)Value MonthDay (\\d+)Value Month (\\w+)Value Timezone (\\S+)Value Time (..:..:..)Start ^${Time}.* ${Timezone} \\w+ ${Month} ${MonthDay} ${Year} -&gt; Record 上述的模版由两部分组成，分别是变量定义部分和状态转移部分；状态转移部分又包含状态定义和规则。 模版格式变量定义格式是Value 选项 变量名 (正则表达式)，其实就是将想要匹配的信息字段定义成一个变量，一个字段就被称为一个“Value”。 选项字段大家可以大概了解一下，但从定义看可能不太好理解，后续可以在例子中进行体会： Filldown：如果该值在这一行为匹配到，那么将该变量填充为上一次匹配到的值。 Fillup：与Filldown类似，但是填充为下一次匹配到的值。 List：如果该字段不是List，那么在记录的时候会记录最新的一次匹配值，如果是List则会把该字段处理为列表，把匹配到的值都加在列表里。 Required：表示该变量为必须，如果某一行文本没匹配到这个变量，那么其他匹配到的值也丢弃。 状态转移可以理解为从某个状态开始然后转换到另一个状态。 状态定义TextFSM中的状态转移是从Start状态开始，最终到EOF状态结束； Start是必须在模版里写上的，下面必须包含匹配规则，当逐行读取文本读取到EOF时，表示匹配结束，则执行 EOF 状态，这是一个隐式状态，不需要写在模版里。 像这种有始有终的状态机就叫做有限状态机。 规则在状态的下面一行可以写多个规则（Rule），但缩进必须是一个空格，且必须有**^**代表从行首进行匹配。 TextFSM匹配的逻辑是进入到一个状态后，读取当前的文本行，用该文本行去和该状态下的每个规则去匹配，“匹配”的过程其实就是正则匹配，不同的是“规则”中的变量是提前定义的。 ^${Time}.* ${Timezone} \\w+ ${Month} ${MonthDay} ${Year}规则 等同于^(..:..:..).* (\\S+) \\w+ (\\w+) (\\d+) (\\d+)。 所以说TextFSM本质上还是正则匹配，只不过是封装了很多逻辑来简化除正则匹配之外的代码逻辑。 如果单纯的看操作的定义可能会有些晦涩，不过可以结合我们上一章节的内容进行理解就会非常清晰，上一章节中我们用代码实现了逐行匹配的逻辑，如下： 12345678910111213version_regexp = r'^Cisco IOS .*Version (\\S+),'uptime_regexp = r'.*uptime is (.*)'image_regexp = r'^System image file is &quot;(.*)&quot;'reset_regexp = r'^Last reset from (\\w+)'regexps = [version_regexp, uptime_regexp, image_regexp, reset_regexp]result = []for line in stdout.split(&quot;\\n&quot;): for regexp in regexps: res = re.findall(regexp, line) if not res: continue result.append(res[0]) break 上述代码中的多个regexp变量就等同于TextFSM中的Value定义； regexps是一个包含多个正则的数组，那么它就相当于TextFSM某个状态下的多个规则，可以看做我们的代码中所有的规则都在Start状态下。 操作TextFSM的机制里，命中某个规则后会执行规则后衔接的操作。 每条规则后面可以使用-&gt;衔接一个或多个操作，多个操作的格式为-&gt; A.B.C，操作共分为四类： 行操作 Next：表示命中该规则后继续读取下一行，并从当前状态的第一个规则开始重新匹配； Continue：表示命中该规则后仍然使用这一行，继续进行下一个规则的匹配； 上述代码匹配的过程就是逐行读取文本，然后用regexps数组中的每个规则去匹配，如果匹配到，就读取下一行，然后再用regexps中的每个规则去匹配；这就相当于行操作里的“Next”； 而Continue就是把代码中的“break”去掉，在匹配到regexps中的某个规则后，继续用下一个规则去匹配，而不去读取新行。 记录操作“记录”大家可以理解为上述代码中的result.append(res[0])，就是把匹配的结果保存下来的意思 NonRecord：表示什么都不做。 Record：从上次执行记录之后匹配到的所有值进行记录，当指定了该规则中某个变量为Required，且该变量没匹配到值时，则全都不做记录。 还有另外两个操作很少用到，这里就不做解释。 状态转移操作 新状态：读取下一行，并进入到指定状态，进行规则匹配。 错误操作 Error 错误信息：表示匹配到该行后则抛出错误信息，终止匹配，并丢弃全部记录。 未指定任何action的时候相当于执行Next.NoRecord 匹配多行文本我们再以匹配多行文本为例子，看看如何写TextFSM的模版。 匹配的内容仍然用上一章节中的”show version”的输出，如下： 12345678910111213141516171819202122232425262728293031323334353637383940Cisco IOS Software, Catalyst 4500 L3 Switch Software (cat4500-ENTSERVICESK9-M), Version 12.2(31)SGA1, RELEASE SOFTWARE (fc3)Technical Support: http://www.cisco.com/techsupportCopyright (c) 1986-2007 by Cisco Systems, Inc.Compiled Fri 26-Jan-07 14:28 by kellythwImage text-base: 0x10000000, data-base: 0x118AD800ROM: 12.2(31r)SGAPod Revision 0, Force Revision 34, Gill Revision 20router.abc uptime is 11 weeks, 4 days, 20 hours, 26 minutesSystem returned to ROM by reloadSystem restarted at 22:49:40 PST Tue Nov 18 2008System image file is &quot;bootflash:cat4500-entservicesk9-mz.122-31.SGA1.bin&quot;This product contains cryptographic features and is subject to UnitedStates and local country laws governing import, export, transfer anduse. Delivery of Cisco cryptographic products does not implythird-party authority to import, export, distribute or use encryption.Importers, exporters, distributors and users are responsible forcompliance with U.S. and local country laws. By using this product youagree to comply with applicable laws and regulations. If you are unableto comply with U.S. and local laws, return this product immediately.A summary of U.S. laws governing Cisco cryptographic products may be found at:http://www.cisco.com/wwl/export/crypto/tool/stqrg.htmlIf you require further assistance please contact us by sending email toexport@cisco.com.cisco WS-C4948-10GE (MPC8540) processor (revision 5) with 262144K bytes of memory.Processor board ID FOX111700ZNMPC8540 CPU at 667Mhz, Fixed ModuleLast reset from Reload2 Virtual Ethernet interfaces48 Gigabit Ethernet interfaces2 Ten Gigabit Ethernet interfaces511K bytes of non-volatile configuration memory.Configuration register is 0x2102 现在我们的目标是获取几个关键信息，包括：版本号、启动时长、镜像文件、重置原因，模版如下： 12345678910Value Version (\\S+)Value Uptime (.*)Value Image (\\S+)Value ResetReason (\\w+)Start ^Cisco IOS .*Version ${Version}, ^.*uptime is ${Uptime} ^System image file is &quot;{Image}&quot; ^Last reset from ${ResetReason} -&gt; Record 这个模版整体上来看还是比较简单的，跟我们上述的代码逻辑也是几乎一致，但区别在于Record操作放在了最后，那么就是在匹配到最后一个ResetReason之后再把之前到几个变量一起保存下来，相当于把我们代码的append操作后移了一下，如下： 123456789101112result = []tmp = []for line in stdout.split(&quot;\\n&quot;): for index, regexp in enumerate(regexps): res = re.findall(regexp, line) if not res: continue tmp.append(res[0]) if index == len(regexps): result.append(tmp) tmp = [] # clear tmp break 先把规则中命中的值暂存在tmp中，如果出现Record操作呢，就把tmp添加到最终的结果里，这里index == len(regexps)表示当前规则是最后一个规则的时候，做Record。 匹配复杂格式文本同样以我们上一章节匹配路由表的输出为例，如下： 12345678910 Destination Gateway Dist/Metric Last Change ----------- ------- ----------- -----------B EX 0.0.0.0/0 via 192.0.2.73 20/100 4w0d via 192.0.2.201 via 192.0.2.202 via 192.0.2.74B IN 192.0.2.76/30 via 203.0.113.183 200/100 4w2dB IN 192.0.2.204/30 via 203.0.113.183 200/100 4w2dB IN 192.0.2.80/30 via 203.0.113.183 200/100 4w2dB IN 192.0.2.208/30 via 203.0.113.183 200/100 4w2d TextFSM模版如下： 123456789101112131415Value Protocol (\\S)Value Type (\\S\\S)Value Required Prefix (\\S+)Value List Gateway (\\S+)Value Distance (\\d+)Value Metric (\\d+)Value LastChange (\\S+)Start ^.*----- -&gt; RoutesRoutes ^\\s\\s\\S\\s\\S\\S -&gt; Continue.Record ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s+via ${Gateway} 上述的模版就会略微复杂了，我们下面结合上一章节中的代码逐步进行讲解，如果能完全理解这个模版，那么就算是基本入门TextFSM了。 12345678910111213141516171819202122import restdout = &quot;output of show ip route&quot;regexp = r'(\\w) (\\w+) (\\S+)\\s+via (\\S+)\\s+(\\d+)/(\\d+)\\s+(\\S+)'gateway_regexp = r'\\s+via (\\S+)'result = []tmp = []for line in stdout.split(&quot;\\n&quot;): res = re.findall(regexp, line) if res: if tmp and len(tmp[-1]) &gt; 5: result.append(tmp) tmp = [] # clear tmp route = list(res[0]) route[3] = [route[3]] tmp.append(route) continue col = re.findall(gateway_regexp, line) if not col: continue if len(tmp) &gt; 0: tmp[-1][3].append(col[0])print(result) 变量定义用到了“选项”中的Required和List： Required表示匹配的时候必须存在路由前缀字段的值，如果该值没有，那么Record的时候就不会进行记录； List表示网关字段在Record的时候要处理成一个列表，而不是字符串，这个就等同于上述代码中高亮的一段逻辑route[3] = [route[3]]。 状态转移这里其实没必要使用状态转移，但一个是可以提高准确性，另外是可以让大家熟悉一下状态转移怎么用 12Start ^.*----- -&gt; Routes 默认匹配的时候直接进入Start状态下进行匹配，当匹配到有^.*-----的时候，说明下面的输出都是路由表的内容了，这时候就通过状态转移，进入到“Routes”中进行匹配。 操作Routes下的规则如下： 1234Routes ^\\s\\s\\S\\s\\S\\S -&gt; Continue.Record ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s+via ${Gateway} 大家可以思考一下为什么要这么写？为什么把Record加在最上面，还有一个Continue操作？ 这里最开始的第一想法肯定是如下： 12345678Routes ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} -&gt; Record 或者Routes ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s+via ${Gateway} -&gt; Record 上面第一种的结果会只有全部值都匹配到才会记录，因为我们把“Prefix”字段声明成了Required，所以当某一行只有一个Gateway字段的时候，Record就无效了，最终的结果肯定匹配不到全部的Gateway。 第二种写法呢，虽然有一条单独匹配Gateway的规则，但同样受限于Prefix的Required字段，导致结果不完整； 那如果取消掉Required是不是可以匹配完整呢？答案是同样不可以。 如果去掉了Required，那么最终的结果就会出现只有Gateway有值，而其他字段都是“”，类似于这样 123456[ ['B', 'EX', '0.0.0.0/0', ['192.0.2.73'], '20', '100', '4w0d'], ['', '', '', ['192.0.2.201'], '', '', ''], ['', '', '', ['192.0.2.202'], '', '', ''], ...] 所以我们需要重新考虑一下执行Record的时机，以及Record的定义，Record可以将上一次记录之后匹配到的值进行记录，那么把执行Record操作的时机改到匹配到B EX 0.0.0.0/0 ...的时候是不是也可以。 相当于把Record后移了一下，把原先的在匹配到路由条目之后记录，改成在匹配下一个完整的路由条目之前记录，如下： 1234Routes ^\\s\\s\\S\\s\\S\\S -&gt; Record ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s\\s\\S\\s\\S\\S这个规则就相当于匹配到了B EX 0.0.0.0/0 ...内容，但命中某个规则后如果没有指定行操作，那么默认会执行Next，相当于-&gt; Next.Record；那命中的这行完整路由条目岂不是提取不到字段了？ 别忘了，行操作里还有一个Continue，Continue的定义是“命中该规则后仍然使用这一行，继续进行下一个规则的匹配”，所以在^\\s\\s\\S\\s\\S\\S后衔接-&gt; Continue.Record就可以把之前匹配的结果记录下来，并且保留当前行，去做下一个匹配。 那么匹配路由条目的完整规则就演变成如下所示： 1234Routes ^\\s\\s\\S\\s\\S\\S -&gt; Continue.Record ^\\s\\s${Protocol} ${Type} ${Prefix}\\s+via ${Gateway}\\s+${Distance}/${Metric}\\s+${LastChange} ^\\s+via ${Gateway} 大家可以再回看一下上面的代码，是不是和这里的逻辑基本吻合。 总结通过这三个TextFSM模版的举例大家可以发现，TextFSM恰好解决了我们一开始遇到的三大痛点，而且模版本身的匹配机制和行操作于我们的代码逻辑几乎一致。 所以可以看到TextFSM是将匹配过程或者逻辑进行抽象封装，这样就可以让匹配的模版以文本的形式存在，定义一个模版其实相当于同时定义了解析的正则和匹配的逻辑，这样可以大大简化系统设计时的复杂度，并且对加强可扩展性起到了非常重要的作用。","link":"/posts/3a4924fc.html"},{"title":"2.13 自动化运维初级村-巡检-解析","text":"摘要经过前面几个章节的学习，总算是初步上手了文本的解析功能，那么这一章节中我们就来完善巡检框架，让它具备解析能力，这样才是一个完整巡检模块。 模版管理线上化Action 模型改造上一章节中提到，考虑到程序设计的可扩展性，我们希望把解析模板做线上管理，这里采用的方式是把模板的内容保存到数据库中，以供我们在需要的时候查询检索。 巡检框架已存在的实体中，有一个 Action 实体，这个实体用来表示执行的巡检动作，它所对应的表结构如下： 1234567891011class Action(db.Model): __tablename__ = &quot;action&quot; id = db.Column(db.Integer, primary_key=True, autoincrement=True) name = db.Column(db.String(64), nullable=False, comment=&quot;动作名称&quot;) description = db.Column(db.String(256), comment=&quot;动作描述&quot;) vendor = db.Column(db.String(64), comment=&quot;厂商&quot;) model = db.Column(db.String(64), comment=&quot;型号&quot;) cmd = db.Column(db.String(256), nullable=False, comment=&quot;命令行&quot;) type = db.Column(db.String(8), comment=&quot;命令类型[show|config]&quot;) parse_type = db.Column(db.String(8), comment=&quot;解析类型[regexp|textfsm]&quot;) parse_content = db.Column(db.String(1024), comment=&quot;解析内容&quot;) 上述 Action 的属性中有两个属性就是专门预留来做解析时用的，因为每一个巡检动作都会对应一条具体的命令，而解析功能必然是和命令挂钩的，所以解析的相关属性理所当然应该与 Action 存放在一起。 parse_type 属性是用来表示解析类型的，理想情况下我们希望每个命令都有 TextFSM 模板，但也不排除有其他特殊情况需要用到正则表达式来解析，所以专门设置了该属性来进行区分。 parse_content 属性就是用来保存模板内容（或者正则表达式）的，但之前的设计中，把这一列设置成了 Varchar(1024)，那么通过上一章节的学习，大家应该会发现在 ntc-template 库中提供的解析模板会存在较长的情况，所以我们需要将该列在数据库中改为 Text，使其可以保存更多的字符（65535个字符）。 修改数据库列字段的 DDL 如下： 1ALTER TABLE action MODIFY COLUMN parse_content TEXT COMMENT '解析内容'; 同时 Action Model 中的 parse_content 修改如下： 1parse_content = db.Column(db.Text, comment=&quot;解析内容&quot;) Action 录入之前的章节中已经实现巡检动作的增删改查，所以我们可以通过接口来进行 Action 的操作，其中就可以包含对解析模板的管理。 比如增加一个巡检动作的请求如下： 1234567891011121314POST http://127.0.0.1:5000/action/addContent-Type: application/json[ { &quot;name&quot;: &quot;version_check&quot;, &quot;description&quot;: &quot;版本检查&quot;, &quot;vendor&quot;: &quot;cisco&quot;, &quot;model&quot;: &quot;ios&quot;, &quot;cmd&quot;: &quot;show version&quot;, &quot;type&quot;: &quot;show&quot;, &quot;parse_type&quot;: &quot;textfsm&quot;, &quot;parse_content&quot;: &quot;{模版内容}&quot; } 或者对已存在的巡检动作来修改其解析模板，请求如下： 12345678910POST http://127.0.0.1:5000/action/updateContent-Type: application/json[ { &quot;id&quot;: &quot;1&quot; , &quot;parse_type&quot;: &quot;textfsm&quot;, &quot;parse_content&quot;: &quot;{模版内容}&quot; }] 通过以上的方式就可以将模板与巡检动作+执行的命令进行绑定，统一进行管理，而不是通过额外的文件形式来做。 执行+解析在之前章节中我们已经实现了基于SSH的执行器，并且可以在初始化执行器时，传入 ActionHandler 与 DeviceHandler 进行后续的创建连接和执行命令使用，其中用于执行命令的 execute() 方法代码如下： 12345678910111213141516class SSHExecutor: ... def execute(self, action: Optional[Action] = None, action_condition: Optional[Dict] = None, read_timeout: int = 10) -&gt; str: action_condition.update({&quot;vendor&quot;: self.device.vendor.lower(), &quot;model&quot;: self.device.model.lower()}) if action is None: action = self.fetch_action(action_condition) if action.type == CommandType.Config: output = self.conn.send_config_set(action.cmd, read_timeout=read_timeout) else: self.conn.enable() output = self.conn.send_command(action.cmd, read_timeout=read_timeout) # TODO parse_result = self.parse() parse_result = &quot;&quot; self.save(action.cmd, output, parse_result) return output 上述代码中我们预留了一个 TODO 部分，用来添加解析逻辑，现在的目标就是将解析功能添加在这里。 解析参数因为现在默认执行 execute 后是不进行解析的，所以需要在该方法中设置一个参数 parse，来表示是否在执行完命令后，对输出结果进行解析。 大家可以回想一下上一章节中介绍的 Netmiko 里自带的 send_command 函数也有解析的功能，并且还支持三种解析方式，分别是 TextFSM、TTP、Genie，只需传入对应的 use_textfsm、use_ttp、use_genie 即可；我们执行器中的 execute 也可以支持多种解析，目前支持 regexp 和 textfsm，如果后续想支持 ttp 或者其他也是可以的，但为什么只需要设置一个 parse 参数呢？ 原因是 execute 方法中传入的 action 参数是一个巡检项模型，这个对象里就已经包含了其所对应的执行命令和解析方式以及解析模板，所以就不需要像 send_command 一样将解析的参数与执行的命令拆开传递，这也是大型项目的设计与工具包的设计之间的差异，大家可以下来仔细体会一下。 解析逻辑ActionHandler 改造这里的 parse 参数是一个 bool 类型的变量，需要解析时该变量为 True，所以在执行完命令之后需要加一段逻辑如下： 12if parse: # parse 现在需要考虑的就是解析的逻辑应该属于哪个对象？ 既然解析类型和模板内容是属于 Action 对象的，那么解析的逻辑其实顺理成章的应该属于 ActionHandler 对象，因为这个对象就是用来处理 Action 相关的衍生逻辑的。 所以我们在 ActionHandler 类中增加解析的逻辑，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142from ntc-template import parse_outputfrom ..models.action import ParseTypeEnumclass ActionHandler(abc.ABC): &quot;&quot;&quot; 该类为 Handler 的抽象类 &quot;&quot;&quot; ... @abc.abstractmethod def parse(self, device_type: str, action: Any, output: str) -&gt; List[Dict]: &quot;&quot;&quot; 抽象方法，继承了 ActionHandler 的子类必须实现该方法 &quot;&quot;&quot; pass class ActionJSONHandler(ActionHandler): &quot;&quot;&quot; 基于 json 文件实现的 Handler 类 &quot;&quot;&quot; ... def parse(self, device_type: str, action: Dict, output: str) -&gt; List[Dict]: &quot;&quot;&quot; 使用 ntc-template 进行解析 &quot;&quot;&quot; try: if action[&quot;parse_type&quot;] == ParseTypeEnum.TextFSM.value: return parse_output(platform=device_type, command=action[&quot;cmd&quot;], data=output) except Exception: pass return [] class ActionORMHandler(ActionHandler): &quot;&quot;&quot; 基于 ORM 实现的 Handler 类 &quot;&quot;&quot; ... def parse(self, device_type: str, action: Action, output: str) -&gt; List[Dict]: &quot;&quot;&quot; 使用基于字符串的 TextFSM 进行解析 &quot;&quot;&quot; try: if action.parse_type == ParseTypeEnum.TextFSM.value: fsm = TextFSM(StringIO(action.parse_content)) return fsm.ParseTextToDicts(output) except Exception: pass return [] 大家可以回想之前我们实现了两种 ActionHandler，第一种是基于 JSON 文件的，可以处理 action 不存在数据库的情况，第二种是基于 ORM 的，处理在数据库中保存 action 的情况，同时还定义了一个抽象的 ActionHandler，利用 abc.ABC 来实现对其子类方法的约束。 当需要增加 parse 逻辑时，应该先在抽象类 ActionHandler 中增加该抽象方法，以此来约束 ActionJSONHandler 和 ActionORMHandler 必须实现 parse 逻辑。 对于 ActionJSONHandler 来说，适用于小范围的场景，action 和 device 都通过文件形式管理，所以 parse 逻辑就恰好适合使用通过文件管理模板的 ntc-template。 对于 ActionORMHandler 来说，所有的数据都通过数据库进行存储，所以 parse 逻辑中通过基于字符串的 TextFSM 模板进行解析。 有一个小细节就是在解析的时候判断了一下 parse_type，判断的时候引用了 model.py 中定义的枚举类 ParseTypeEnum，关于为什么使用枚举类而不直接用 if parse_type == “textfsm” 来判断，之前的文章中也提到过； 在程序中尽量不使用字面量，其一是因为字面量不易读，其二是因为字面量扩展性差，不易维护。 Execute 改造现在已经实现了 parse 逻辑，那么 execute 函数要如何补全逻辑呢？代码如下： 123456789101112class SSHExecutor: ... def execute( self, action: Optional[Action] = None, action_condition: Optional[Dict] = None, read_timeout: int = 10, parse: bool = False) -&gt; Union[List, str]: ... parse_result = &quot;&quot; if parse: parse_result = self.action_handler.parse(self.device_type, action, output) ... return parse_result if parse_result else output 上述代码省略了 execute 方法中的其他逻辑，解析的部分实际上只增加了一行；在初始化 SSHExecutor 的时候已经传入了 ActionHandler，所以这里只需要调用 self 本身的 action_handler 中的 parse 方法，传入 device_type、action、output 参数即可完成解析。 该函数的返回值也做了一下处理，在可以正确解析的情况下返回 parse_result，否则直接返回 output。 执行接口我们已经实现了通过接口来触发执行器，所以还需要改造一下路由函数，如下： 123456789101112131415161718192021222324252627@executor_blueprint.route(&quot;/execute&quot;, methods=[&quot;POST&quot;])def execute(): data = request.get_json() try: device_condition = data.get(&quot;device_condition&quot;) action_condition = data.get(&quot;action_condition&quot;) action = None if &quot;action&quot; in data: action = Action.to_model(**data.get(&quot;action&quot;)) device_handler = DeviceORMHandler(db.session()) action_handler = ActionORMHandler(db.session()) with SSHExecutor( username=current_app.config.get(&quot;SSH_USERNAME&quot;), password=current_app.config.get(&quot;SSH_PASSWORD&quot;), secret=current_app.config.get(&quot;SSH_SECRET&quot;), device_condition=device_condition, device_handler=device_handler, action_handler=action_handler, logger=current_app.logger) as ssh: output = ssh.execute( action=action, action_condition=action_condition, parse=data.get(&quot;parse&quot;, False) ) return Success(data=output) except Exception as e: raise ExecutorError(message=str(e)) 健壮性执行器作为提供底层支持能力的模块，在健壮性方面必须足够强，所以现在还需要做一些额外的处理以应对异常状况。 重试创建连接是最容易发生错误的地方，可能会由于网络抖动或者设备繁忙导致连接失败，所以重试是非常有必要的，代码如下： 12345678910111213141516171819class SSHExecutor: ... self.retry_times = 3 self.conn = self.connect(self.retry_times) def connect(self, retry: int) -&gt; BaseConnection: if retry == 0: raise Exception(&quot;Retry to connect over maximum&quot;) try: conn = ConnUnify(host=self.host, port=self.port, username=self.username, password=self.password, secret=self.secret, device_type=self.device_type, conn_timeout=self.conn_timeout, auth_timeout=self.auth_timeout, banner_timeout=self.banner_timeout, session_log=&quot;netmiko.log&quot;, session_log_file_mode=&quot;append&quot;) msg = f&quot;Netmiko connection successful to {self.host}:{self.port}&quot; self.logger.info(msg) return conn except Exception as e: self.logger.error(str(e)) return self.connect(retry - 1) 上述代码中，定义了一个最大重试次数，并且在调用 ConnUnify 失败后重试次数减一再次调用自身，直至重试次数为 0 后抛出连接异常。 这里抛出异常是非常有必要的，因为如果没有成功创建连接，那么 self.conn 属性将会是 None，但其他实例方法使用 self.conn 时是默认该属性有值的。 所以我们要在连接异常的情况下，及时抛出异常，避免将异常“吞掉”而导致下游方法发生不可预知的错误。 懒加载在 execute 方法中使用了创建好的连接，大家可以一种场景：初始化执行器的时候连接创建成功，但由于长时间未使用，或者网络质量不佳，导致连接丢失或被关闭，那此时 execute 执行命令就会出现异常。 所以最佳的做法是在 execute 中使用连接前获取连接即可，而不需要在初始化执行器的时候就去创建连接，代码如下： 12345678910111213141516171819202122232425262728class SSHExecutor: ... self.conn = None @property def connection(self) -&gt; BaseConnection: if not self.conn or not self.conn.is_alive(): self.conn = self.connect(self.retry_times) return self.conn def execute( self, action: Optional[Action] = None, action_condition: Optional[Dict] = None, read_timeout: int = 10, parse: bool = False) -&gt; Union[List, str]: action_condition.update({&quot;vendor&quot;: self.device.vendor.lower(), &quot;model&quot;: self.device.model.lower()}) if action is None: action = self.fetch_action(action_condition) if action.type == CommandType.Config: output = self.connection.send_config_set(action.cmd, read_timeout=read_timeout) else: self.connection.enable() output = self.connection.send_command(action.cmd, read_timeout=read_timeout) parse_result = &quot;&quot; if parse: parse_result = self.action_handler.parse(self.device_type, action, output) self.save(action.cmd, output, parse_result) return parse_result if parse_result else output 上述代码中用到了 Python 中的属性装饰器 @property，被该装饰器装饰的实例方法可以直接作为实例属性使用，因此在 execute 方法中将 self.conn 均改为 self.connection。 而 connection() 的逻辑是，判断此刻如果不存在 self.conn 或者 self.conn 不是可用状态，则发起连接获取 self.conn，最后将 self.conn 返回。 这种模式是在大型项目中非常常见的懒加载模式，有两点好处，一是可以避免在初始化对象时耗时过多或者产生当下的资源；二是在需要时获取可以保证资源的实时性。 演示执行器到目前为止已经改造完成，我们发送请求测试一下： 12345678910111213POST http://127.0.0.1:5000/executor/executeContent-Type: application/json{ &quot;device_condition&quot;: { &quot;ip&quot;: &quot;192.168.31.149&quot; }, &quot;action_condition&quot;: { &quot;name&quot;: &quot;version_check&quot;, &quot;model&quot;: &quot;cisco&quot; }, &quot;parse&quot;: true} 在 execute 请求中传入 parse 参数，代表在执行完命令后进行解析，返回结构化数据。 这里我们将模拟器关闭，测试连接失败的情况，返回内容如下： 123456{ &quot;data&quot;: null, &quot;status_code&quot;: 520, &quot;message&quot;: &quot;Retry to connect over maximum&quot;, &quot;request&quot;: &quot;POST /executor/execute&quot;} 模拟器正常运行时，再次发起请求，返回如下： 123456789101112131415161718192021222324252627{ &quot;data&quot;: [ { &quot;VERSION&quot;: &quot;&quot;, &quot;ROMMON&quot;: &quot;Bootstrap&quot;, &quot;HOSTNAME&quot;: &quot;r1&quot;, &quot;UPTIME&quot;: &quot;0 minutes&quot;, &quot;UPTIME_YEARS&quot;: &quot;&quot;, &quot;UPTIME_WEEKS&quot;: &quot;&quot;, &quot;UPTIME_DAYS&quot;: &quot;&quot;, &quot;UPTIME_HOURS&quot;: &quot;&quot;, &quot;UPTIME_MINUTES&quot;: &quot;0&quot;, &quot;RELOAD_REASON&quot;: &quot;Unknown reason&quot;, &quot;RUNNING_IMAGE&quot;: &quot;/opt/unetlab/addons/iol/bin/i86bi-linux-l3-adventerprisek9-15.4&quot;, &quot;HARDWARE&quot;: [], &quot;SERIAL&quot;: [ &quot;67108896&quot; ], &quot;CONFIG_REGISTER&quot;: &quot;0x0&quot;, &quot;MAC&quot;: [], &quot;RESTARTED&quot;: &quot;&quot; } ], &quot;status_code&quot;: 200, &quot;message&quot;: &quot;success&quot;, &quot;request&quot;: &quot;POST /executor/execute&quot;} 总结到目前为止，我们已经完成了执行器的全部设计，但是对于巡检模块来说，还缺少一个重要的功能，那就是巡检项的结果校验以及导出，在后面的章节，我会带大家逐步进行这一功能的设计和实现。 附录github分支：https://github.com/EthanYue/PythonAutoOps/tree/junior-parse","link":"/posts/380a5944.html"}],"tags":[{"name":"CMDB","slug":"CMDB","link":"/tags/CMDB/"},{"name":"Python基础","slug":"Python基础","link":"/tags/Python%E5%9F%BA%E7%A1%80/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Flask","slug":"Flask","link":"/tags/Flask/"},{"name":"ORM","slug":"ORM","link":"/tags/ORM/"},{"name":"Gunicorn","slug":"Gunicorn","link":"/tags/Gunicorn/"},{"name":"Paramiko","slug":"Paramiko","link":"/tags/Paramiko/"},{"name":"Netmiko","slug":"Netmiko","link":"/tags/Netmiko/"},{"name":"SSH","slug":"SSH","link":"/tags/SSH/"},{"name":"巡检","slug":"巡检","link":"/tags/%E5%B7%A1%E6%A3%80/"},{"name":"正则","slug":"正则","link":"/tags/%E6%AD%A3%E5%88%99/"},{"name":"文本解析","slug":"文本解析","link":"/tags/%E6%96%87%E6%9C%AC%E8%A7%A3%E6%9E%90/"},{"name":"TextFSM","slug":"TextFSM","link":"/tags/TextFSM/"}],"categories":[{"name":"新手村","slug":"新手村","link":"/categories/%E6%96%B0%E6%89%8B%E6%9D%91/"},{"name":"初级村","slug":"初级村","link":"/categories/%E5%88%9D%E7%BA%A7%E6%9D%91/"},{"name":"中级村","slug":"中级村","link":"/categories/%E4%B8%AD%E7%BA%A7%E6%9D%91/"},{"name":"高级村","slug":"高级村","link":"/categories/%E9%AB%98%E7%BA%A7%E6%9D%91/"}],"pages":[{"title":"","text":"","link":"/test/index.html"}]}